<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="Hexo Theme Keep"><meta name="description" content="Hexo Theme Keep"><meta name="author" content="Carpe Tu"><title>01_Introduction to Convolutional Neural Networks for Visual Recognition | Carpe&#39;s Blog</title><link rel="stylesheet" href="/css/style.css"><link rel="shortcut icon" href="https://pic.imgdb.cn/item/622b2b2e5baa1a80abf6fcff.png"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/fontawesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/regular.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/solid.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/brands.min.css"><script id="hexo-configurations">let KEEP=window.KEEP||{};KEEP.hexo_config={hostname:"overduse.github.io",root:"/",language:"en",path:"search.json"},KEEP.theme_config={toc:{enable:!0,number:!0,expand_all:!0,init_open:!0},style:{primary_color:"#0066cc",logo:"https://pic.imgdb.cn/item/622b2b2e5baa1a80abf6fcff.png",favicon:"https://pic.imgdb.cn/item/622b2b2e5baa1a80abf6fcff.png",avatar:"https://pic.imgdb.cn/item/639ece3eb1fccdcd365542dd.jpg",font_size:"15.6px",font_family:null,hover:{shadow:!0,scale:!0},first_screen:{enable:!0,header_transparent:!0,background_img:"/images/bg.svg",description:"Keep writing and Keep loving.",font_color:null,hitokoto:!0},scroll:{progress_bar:!0,percent:!0}},local_search:{enable:!0,preload:!0},code_copy:{},code_block:{tools:{enable:!0,style:"mac"},highlight_theme:"default"},side_tools:{},pjax:{enable:!0},lazyload:{enable:!0},comment:{enable:!1,use:"valine",valine:{appid:null,appkey:null,placeholder:null},gitalk:{github_id:null,github_admins:null,repository:null,client_id:null,client_secret:null},twikoo:{env_id:null,region:null,version:"1.6.7"},waline:{server_url:null,reaction:!1,version:2}},post:{author_label:{enable:!0,auto:!1,custom_label_list:["Bell","Red Whistle","Blue Whistle","Moon Whistle","Black Whistle","YouWorth"]},word_count:{enable:!0,wordcount:!0,min2read:!0},img_align:"center",copyright_info:!0},version:"3.6.1"},KEEP.language_ago={second:"%s seconds ago",minute:"%s minutes ago",hour:"%s hours ago",day:"%s days ago",week:"%s weeks ago",month:"%s months ago",year:"%s years ago"},KEEP.language_code_block={copy:"Copy code",copied:"Copied",fold:"Fold code block",folded:"Folded"},KEEP.language_copy_copyright={copy:"Copy copyright info",copied:"Copied",title:"Original article title",author:"Original article author",link:"Original article link"}</script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Keep Theme" type="application/atom+xml">
</head><body><div class="progress-bar-container"><span class="scroll-progress-bar"></span> <span class="pjax-progress-bar"></span> <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i></div><main class="page-container"><div class="page-main-content"><div class="page-main-content-top"><header class="header-wrapper"><div class="header-content"><div class="left"><a class="logo-image" href="/"><img src="https://pic.imgdb.cn/item/622b2b2e5baa1a80abf6fcff.png"> </a><a class="logo-title" href="/">Carpe&#39;s Blog</a></div><div class="right"><div class="pc"><ul class="menu-list"><li class="menu-item"><a href="/">HOME</a></li><li class="menu-item"><a href="/archives">ARCHIVES</a></li><li class="menu-item"><a href="/categories">CATEGORIES</a></li><li class="menu-item"><a href="/tags">TAGS</a></li><li class="menu-item"><a href="/links">LINKS</a></li><li class="menu-item"><a href="/about">ABOUT</a></li><li class="menu-item"><a href="/changelog">CHANGELOG</a></li><li class="menu-item search search-popup-trigger"><i class="fas fa-search"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div><div class="icon-item menu-bar"><div class="menu-bar-middle"></div></div></div></div></div><div class="header-drawer"><ul class="drawer-menu-list"><li class="drawer-menu-item flex-center"><a href="/">HOME</a></li><li class="drawer-menu-item flex-center"><a href="/archives">ARCHIVES</a></li><li class="drawer-menu-item flex-center"><a href="/categories">CATEGORIES</a></li><li class="drawer-menu-item flex-center"><a href="/tags">TAGS</a></li><li class="drawer-menu-item flex-center"><a href="/links">LINKS</a></li><li class="drawer-menu-item flex-center"><a href="/about">ABOUT</a></li><li class="drawer-menu-item flex-center"><a href="/changelog">CHANGELOG</a></li></ul></div><div class="window-mask"></div></header></div><div class="page-main-content-middle"><div class="main-content"><div class="fade-in-down-animation"><div class="post-page-container"><div class="article-content-container"><div class="article-title"><span class="title-hover-animation">01_Introduction to Convolutional Neural Networks for Visual Recognition</span></div><div class="article-header"><div class="avatar"><img src="https://pic.imgdb.cn/item/639ece3eb1fccdcd365542dd.jpg"></div><div class="info"><div class="author"><span class="name">Carpe Tu</span> <span class="author-label">Black Whistle</span></div><div class="meta-info"><div class="article-meta-info"><span class="article-date article-meta-item"><i class="fa-regular fa-calendar-plus"></i>&nbsp; <span class="pc">2022-03-01 18:50:55</span> <span class="mobile">2022-03-01 18:50</span> </span><span class="article-update-date article-meta-item"><i class="fas fa-file-pen"></i>&nbsp; <span class="pc">2022-12-12 11:06:57</span> </span><span class="article-categories article-meta-item"><i class="fas fa-folder"></i>&nbsp;<ul><li><a href="/categories/Computer-Vision/">Computer Vision</a>&nbsp;</li></ul></span><span class="article-tags article-meta-item"><i class="fas fa-tags"></i>&nbsp;<ul><li><a href="/tags/CS231n-Feifei-Li/">CS231n (Feifei Li)</a>&nbsp;</li><li>| <a href="/tags/Courses-Notes/">Courses Notes</a>&nbsp;</li></ul></span><span class="article-wordcount article-meta-item"><i class="fas fa-file-word"></i>&nbsp;<span>935 Words</span> </span><span class="article-min2read article-meta-item"><i class="fas fa-clock"></i>&nbsp;<span>5 Mins</span> </span><span class="article-pv article-meta-item"><i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span></span></div></div></div></div><div class="article-content keep-markdown-body"><h1 id="Welcome-to-CS231n"><a href="#Welcome-to-CS231n" class="headerlink" title="Welcome to CS231n"></a>Welcome to CS231n</h1><p>the class <strong>CS231n</strong> is really about computer vision which is really study of visual data.</p><ul><li>the majority of bits flying around the Internet are visual data.</li><li>this field of computer vision is truly interdisciplinary field, and it touches on many different areas of science and engineering and technology.</li></ul><h2 id="Related-Courses-Stanford"><a href="#Related-Courses-Stanford" class="headerlink" title="Related Courses @ Stanford"></a>Related Courses @ Stanford</h2><ul><li><p>CS131(Fall 2016, Profs. Fei-Fei Li &amp; Juan Carlos Niebles)<br>– Undergraduate introductory class</p></li><li><p>CS231a(Spring 2017, Prof. Silvio Savarese)<br>– Core computer vision class for seniors, matters, and PhDs<br>– Topics include image processing, cameras, 3D reconstruction, segmentation, object recognition, scene understanding</p></li><li><p>CS231n(Prof. Fei-Fei Li &amp; Justin Johnson &amp; Serena Yeung)<br>– Neural network (aka “deep learning”) class on image classification</p></li></ul><p>And an assortment of CS331 and CS431 for advanced topics in computer vision.</p><h1 id="History-of-the-computer-vision"><a href="#History-of-the-computer-vision" class="headerlink" title="History of the computer vision"></a>History of the computer vision</h1><h2 id="70s-80s’-attempts-amp-toy-examples"><a href="#70s-80s’-attempts-amp-toy-examples" class="headerlink" title="70s 80s’ attempts & toy examples"></a>70s 80s’ attempts &amp; toy examples</h2><h3 id="object-recognition"><a href="#object-recognition" class="headerlink" title="object recognition"></a>object recognition</h3><ul><li><p><strong>camera obscura</strong></p><ul><li>which is a camera based on <strong>pinhole camera theories</strong>.</li><li>1600s, the Renaissance period of time</li><li>it’s very similiar early eyes that animals developed with a hole that collects lights。</li></ul></li><li><p>In the mean time biologists started <strong>mechanism of vision</strong>.</p><ul><li>The work done by Hubel an Wiesel in the 50s and 60s using electrophysiology<br><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/621f5ada5baa1a80abf8b820.jpg"></li></ul></li><li><p><strong>Larry Roberts</strong> published a set of work called <strong>Block World</strong>.</p><ul><li>The history of computer vision also starts around early 60s.<blockquote><p>That’s the first PhD thesis of computer vision where the visual world was simplified into simple geometric shapes and the goal is to be able to reconginze them and reconstruct what these shapes are.</p></blockquote></li></ul></li><li><p>“The Summer Vision Project” (famous MIT summer project)</p><ul><li>an attempt to use summer workers effectively in a construction of a significant part of a visual system</li><li>the field of computer vision has <strong>blossomed from one summer project</strong> into field of thousands of researchers still working on some of the most fundamental problems of vision</li></ul></li><li><p>Vision, David Marr, in the late 70s</p><ul><li>in order to take a image and arrive at a final holistic full 3d representation of the visual world</li></ul></li></ul><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/621f9e5c5baa1a80ab2f250b.jpg"></p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/621f9e825baa1a80ab2f3f0e.jpg"></p><ul><li><p>generalized cylinder &amp; pictorial structure<br><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/622030ec5baa1a80ab6fe231.jpg"><br>the basic idea is that every object is composed of simple <strong>geometric primitives</strong><br>either representation is a way to reduce the complex structure of the object into a collection of simpler shapes</p></li><li><p><strong>David Lowe</strong></p><ul><li>try to recognize razors by constructing lines and edges and mostly straight lines and their combination.</li></ul></li></ul><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/621fa0af5baa1a80ab30aae0.jpg"></p><h3 id="object-segmentation"><a href="#object-segmentation" class="headerlink" title="object segmentation"></a>object segmentation</h3><p><strong>Normalized Cut(Shi &amp; Malik, 1997)</strong><br><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/621fa17b5baa1a80ab314ed5.jpg"></p><p>The task is taking an image and group the pixels into meaningful areas.<br>here’s one very early seminal work by <strong>Jitendra Malik</strong> and his student <strong>Jianbo Shi</strong> from Berkeley.<br>Using <strong>a graph theory algorithm</strong> for the problem of image segmentation.</p><h2 id="After-2000"><a href="#After-2000" class="headerlink" title="After 2000"></a>After 2000</h2><h3 id="face-detection"><a href="#face-detection" class="headerlink" title="face detection"></a>face detection</h3><p>around 1999 to 2000 machine learning techniques, especially statistical machine learning techniques start to gain momentum. E.g. SVM, boosting, graphical models, <strong>the first wave of the neural network</strong>.</p><p>One Particular work that made lots of contribution —- <strong>AdaBoost algorithm</strong> to do real-time face detection by <strong>Paul Viola</strong> and <strong>Michael Jones</strong></p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/621fa3905baa1a80ab32b3a7.jpg"></p><h3 id="SIFT-feature"><a href="#SIFT-feature" class="headerlink" title="SIFT feature"></a>SIFT feature</h3><p>by <strong>David Lowe</strong>, 1999<br>The idea is that to match and the entire object to another one.</p><h3 id="Spatial-Pyramid-Matching"><a href="#Spatial-Pyramid-Matching" class="headerlink" title="Spatial Pyramid Matching"></a>Spatial Pyramid Matching</h3><p><strong>Lazeblink, Schmid &amp; Ponce, 2006</strong></p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/622070905baa1a80ab9ff995.jpg"></p><blockquote><p>The idea is that there are features in the images that can give us clues about which type of scene it is, whether it’s a landscape or a kitchen or a highway and so on and this particular work takes these features from different parts of image and in different resolutions and put them together in a feature discriptor and than we do support vector machine algorithm on top of that.</p></blockquote><h3 id="Before-ImageNet"><a href="#Before-ImageNet" class="headerlink" title="Before ImageNet"></a>Before ImageNet</h3><p>In the early 2000s, we began to have benchmark data set that can enable us to measure the progress of recognition.</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/622071e15baa1a80aba11670.jpg"><br>PASCAL Visual Object Challenge（20 object categories)</p><h3 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h3><p><a class="link" target="_blank" rel="noopener" href="http://www.image-net.org/">www.image-net.org<i class="fas fa-external-link-alt"></i></a>.</p><ol><li>just want to recognize the world of all the objects</li><li>to come back the machine learning overcome the machine learning bottleneck of overfitting.<blockquote><p>The part of the problem is the visual data is very complex, because it’s complex, our model tend to have a high dimension. High dimension of inputs and have a lot of parameter to fit and when we don’t have enough training data overfitting happens very fast and then we cannot generalize very well.</p></blockquote></li></ol><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/6220794a5baa1a80aba74678.jpg"></p><p>Scientists group from Princeton to Stanford put together the largest possible dataset.</p><h4 id="ImageNet-Large-Scale-Visual-Recognition-Challenge"><a href="#ImageNet-Large-Scale-Visual-Recognition-Challenge" class="headerlink" title="ImageNet Large-Scale Visual Recognition Challenge"></a>ImageNet Large-Scale Visual Recognition Challenge</h4><p>Begin from 2009<br><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62207a585baa1a80aba81a2a.jpg"></p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62207b465baa1a80aba8c209.jpg"></p><p>There is a huge gap between 2011 and 2012</p><h1 id="CS231n-Overview"><a href="#CS231n-Overview" class="headerlink" title="CS231n Overview"></a>CS231n Overview</h1><p>CS231n focuses on one of the most important problems of visual recognition – image classification.</p><blockquote><p>There is a number of visual recognition problems that are related to image classification, such as <em>object detection, image captioning</em></p></blockquote><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/622097335baa1a80abbe9c34.jpg"></p><p><em>Convolutional Neural Network(CNN)</em> have become an important tool for object recognition. CNN somtimes called convnets.</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/622080875baa1a80abacf800.jpg"></p><p>The main <strong>takeaway</strong> here is that <u>convolutional neural networks</u> really had this breakthrough moment in 2012, and since then there’s been a lot of effort focused in <em>tuning and tweeking</em> these algorithms to make them perform better and better on this problem of image classification.</p><p>Even though CNNs perform well in the ImageNet challenges, it’s not invented overnight.</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62209abe5baa1a80abc103de.jpg"></p><h2 id="Philosophy"><a href="#Philosophy" class="headerlink" title="Philosophy"></a>Philosophy</h2><p>u should really understand the deep mechanics of all of these algorithm</p><ul><li>Thorough and Detailed/<br>– Understand how to write from scratch, debug and train CNNs.</li><li>Practical.<br>– Focus on practical techniques for training these networks at scale, and on GPUs(e.g. will touch on distributed optimization, differences between CPU vs. GPU, etc.) Also look at state of the art <strong>software tools</strong> such as Caffe, TensorFlow, and (Py)Torch</li><li>State of the art.<br>– Most materials are new from research world in the past 1-3 years(2014-&gt;2016). Very exciting stuff!</li><li>Fun.<br>– Some fun topics sunch as Image Captioning(using RNN)<br>– Also DeepDream, NeuralStyle, etc.</li></ul></div><div class="post-copyright-info"><div class="article-copyright-info-container"><ul class="copyright-info-content"><li class="post-title"><span class="type">Post title</span>: <span class="content">01_Introduction to Convolutional Neural Networks for Visual Recognition</span></li><li class="post-author"><span class="type">Post author</span>: <span class="content">Carpe Tu</span></li><li class="post-time"><span class="type">Create time</span>: <span class="content">2022-03-01 18:50:55</span></li><li class="post-link"><span class="type">Post link</span>: <span class="content">Computer-Vision/01-introduction-to-convolutional-neural-networks-for-visual-recognition/</span></li><li class="post-license"><span class="type">Copyright notice</span>: <span class="content">All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.</span></li></ul><div class="copy-copyright-info flex-center tooltip" data-content="Copy copyright info" data-offset-y="-2px"><i class="fa-solid fa-copy"></i></div></div></div><ul class="post-tags-box"><li class="tag-item"><a href="/tags/CS231n-Feifei-Li/">#CS231n (Feifei Li)</a>&nbsp;</li><li class="tag-item"><a href="/tags/Courses-Notes/">#Courses Notes</a>&nbsp;</li></ul><div class="article-nav"><div class="article-prev"><a class="prev" rel="prev" href="/WalkThroughs/gumdam-driving-license-test/"><span class="left arrow-icon flex-center"><i class="fas fa-chevron-left"></i> </span><span class="title flex-center"><span class="post-nav-title-item">Gumdam(driving) license Test</span> <span class="post-nav-item">Prev posts</span></span></a></div><div class="article-next"><a class="next" rel="next" href="/broadcast/cs229-complete/"><span class="title flex-center"><span class="post-nav-title-item">CS229 complete</span> <span class="post-nav-item">Next posts</span> </span><span class="right arrow-icon flex-center"><i class="fas fa-chevron-right"></i></span></a></div></div></div><div class="toc-content-container"><div class="post-toc-wrap"><div class="post-toc"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Welcome-to-CS231n"><span class="nav-number">1.</span> <span class="nav-text">Welcome to CS231n</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Courses-Stanford"><span class="nav-number">1.1.</span> <span class="nav-text">Related Courses @ Stanford</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#History-of-the-computer-vision"><span class="nav-number">2.</span> <span class="nav-text">History of the computer vision</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#70s-80s%E2%80%99-attempts-amp-toy-examples"><span class="nav-number">2.1.</span> <span class="nav-text">70s 80s’ attempts &amp; toy examples</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#object-recognition"><span class="nav-number">2.1.1.</span> <span class="nav-text">object recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#object-segmentation"><span class="nav-number">2.1.2.</span> <span class="nav-text">object segmentation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#After-2000"><span class="nav-number">2.2.</span> <span class="nav-text">After 2000</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#face-detection"><span class="nav-number">2.2.1.</span> <span class="nav-text">face detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SIFT-feature"><span class="nav-number">2.2.2.</span> <span class="nav-text">SIFT feature</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spatial-Pyramid-Matching"><span class="nav-number">2.2.3.</span> <span class="nav-text">Spatial Pyramid Matching</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Before-ImageNet"><span class="nav-number">2.2.4.</span> <span class="nav-text">Before ImageNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ImageNet"><span class="nav-number">2.2.5.</span> <span class="nav-text">ImageNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ImageNet-Large-Scale-Visual-Recognition-Challenge"><span class="nav-number">2.2.5.1.</span> <span class="nav-text">ImageNet Large-Scale Visual Recognition Challenge</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CS231n-Overview"><span class="nav-number">3.</span> <span class="nav-text">CS231n Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Philosophy"><span class="nav-number">3.1.</span> <span class="nav-text">Philosophy</span></a></li></ol></li></ol></div></div></div></div></div></div></div><div class="page-main-content-bottom"><footer class="footer"><div class="info-container"><div class="copyright-info info-item">&copy; <span>2021</span> - 2023 &nbsp;<i class="fas fa-heart icon-animate"></i> &nbsp;<a href="/">Carpe Tu</a></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="website-count info-item">Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp; Totalview&nbsp;<span id="busuanzi_value_site_pv"></span></div><div class="theme-info info-item">Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.6.1</a></div></div></footer></div></div><div class="post-tools"><div class="post-tools-container"><ul class="tools-list"><li class="tools-item flex-center toggle-show-toc"><i class="fas fa-list"></i></li></ul></div></div><div class="right-bottom-side-tools"><div class="side-tools-container"><ul class="side-tools-list"><li class="tools-item tool-font-adjust-plus flex-center"><i class="fas fa-search-plus"></i></li><li class="tools-item tool-font-adjust-minus flex-center"><i class="fas fa-search-minus"></i></li><li class="tools-item tool-dark-light-toggle flex-center"><i class="fas fa-moon"></i></li><li class="tools-item rss flex-center"><a class="flex-center" href="/atom.xml" target="_blank"><i class="fas fa-rss"></i></a></li><li class="tools-item tool-scroll-to-bottom flex-center"><i class="fas fa-arrow-down"></i></li></ul><ul class="exposed-tools-list"><li class="tools-item tool-toggle-show flex-center"><i class="fas fa-cog fa-spin"></i></li><li class="tools-item tool-scroll-to-top flex-center"><i class="arrow-up fas fa-arrow-up"></i> <span class="percent"></span></li></ul></div></div><div class="zoom-in-image-mask"><img class="zoom-in-image"></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fas fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="Search..." spellcheck="false" type="search" class="search-input"></div><span class="close-popup-btn"><i class="fas fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/dark-light-toggle.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/local-search.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/code-block.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/lazyload.js"></script><div class="post-scripts pjax"><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/post-helper.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/toc.js"></script></div><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/pjax.min.js"></script><script>window.addEventListener("DOMContentLoaded",()=>{window.pjax=new Pjax({selectors:["head title",".page-container",".pjax"],history:!0,debug:!1,cacheBust:!1,timeout:0,analytics:!1,currentUrlFullReload:!1,scrollRestoration:!1}),document.addEventListener("pjax:send",()=>{KEEP.utils.pjaxProgressBarStart()}),document.addEventListener("pjax:complete",()=>{KEEP.utils.pjaxProgressBarEnd(),window.pjax.executeScripts(document.querySelectorAll("script[data-pjax], .pjax script")),KEEP.refresh()})})</script></body></html>