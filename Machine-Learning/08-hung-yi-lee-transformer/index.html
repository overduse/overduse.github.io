<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="Hexo Theme Keep"><meta name="description" content="Hexo Theme Keep"><meta name="author" content="Carpe Tu"><title>08_Hung-yi Lee_Transformer | Carpe&#39;s Blog</title><link rel="stylesheet" href="/css/style.css"><link rel="shortcut icon" href="https://pic.imgdb.cn/item/622b2b2e5baa1a80abf6fcff.png"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/fontawesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/regular.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/solid.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/brands.min.css"><script id="hexo-configurations">let KEEP=window.KEEP||{};KEEP.hexo_config={hostname:"overduse.github.io",root:"/",language:"en",path:"search.json"},KEEP.theme_config={toc:{enable:!0,number:!0,expand_all:!0,init_open:!0},style:{primary_color:"#0066cc",logo:"https://pic.imgdb.cn/item/622b2b2e5baa1a80abf6fcff.png",favicon:"https://pic.imgdb.cn/item/622b2b2e5baa1a80abf6fcff.png",avatar:"https://pic.imgdb.cn/item/639ece3eb1fccdcd365542dd.jpg",font_size:"15.6px",font_family:null,hover:{shadow:!0,scale:!0},first_screen:{enable:!0,header_transparent:!0,background_img:"/images/bg.svg",description:"Keep writing and Keep loving.",font_color:null,hitokoto:!0},scroll:{progress_bar:!0,percent:!0}},local_search:{enable:!0,preload:!0},code_copy:{},code_block:{tools:{enable:!0,style:"mac"},highlight_theme:"default"},side_tools:{},pjax:{enable:!0},lazyload:{enable:!0},comment:{enable:!1,use:"valine",valine:{appid:null,appkey:null,placeholder:null},gitalk:{github_id:null,github_admins:null,repository:null,client_id:null,client_secret:null},twikoo:{env_id:null,region:null,version:"1.6.7"},waline:{server_url:null,reaction:!1,version:2}},post:{author_label:{enable:!0,auto:!1,custom_label_list:["Bell","Red Whistle","Blue Whistle","Moon Whistle","Black Whistle","YouWorth"]},word_count:{enable:!0,wordcount:!0,min2read:!0},img_align:"center",copyright_info:!0},version:"3.6.1"},KEEP.language_ago={second:"%s seconds ago",minute:"%s minutes ago",hour:"%s hours ago",day:"%s days ago",week:"%s weeks ago",month:"%s months ago",year:"%s years ago"},KEEP.language_code_block={copy:"Copy code",copied:"Copied",fold:"Fold code block",folded:"Folded"},KEEP.language_copy_copyright={copy:"Copy copyright info",copied:"Copied",title:"Original article title",author:"Original article author",link:"Original article link"}</script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style><link rel="alternate" href="/atom.xml" title="Keep Theme" type="application/atom+xml">
</head><body><div class="progress-bar-container"><span class="scroll-progress-bar"></span> <span class="pjax-progress-bar"></span> <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i></div><main class="page-container"><div class="page-main-content"><div class="page-main-content-top"><header class="header-wrapper"><div class="header-content"><div class="left"><a class="logo-image" href="/"><img src="https://pic.imgdb.cn/item/622b2b2e5baa1a80abf6fcff.png"> </a><a class="logo-title" href="/">Carpe&#39;s Blog</a></div><div class="right"><div class="pc"><ul class="menu-list"><li class="menu-item"><a href="/">HOME</a></li><li class="menu-item"><a href="/archives">ARCHIVES</a></li><li class="menu-item"><a href="/categories">CATEGORIES</a></li><li class="menu-item"><a href="/tags">TAGS</a></li><li class="menu-item"><a href="/links">LINKS</a></li><li class="menu-item"><a href="/about">ABOUT</a></li><li class="menu-item"><a href="/changelog">CHANGELOG</a></li><li class="menu-item search search-popup-trigger"><i class="fas fa-search"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div><div class="icon-item menu-bar"><div class="menu-bar-middle"></div></div></div></div></div><div class="header-drawer"><ul class="drawer-menu-list"><li class="drawer-menu-item flex-center"><a href="/">HOME</a></li><li class="drawer-menu-item flex-center"><a href="/archives">ARCHIVES</a></li><li class="drawer-menu-item flex-center"><a href="/categories">CATEGORIES</a></li><li class="drawer-menu-item flex-center"><a href="/tags">TAGS</a></li><li class="drawer-menu-item flex-center"><a href="/links">LINKS</a></li><li class="drawer-menu-item flex-center"><a href="/about">ABOUT</a></li><li class="drawer-menu-item flex-center"><a href="/changelog">CHANGELOG</a></li></ul></div><div class="window-mask"></div></header></div><div class="page-main-content-middle"><div class="main-content"><div class="fade-in-down-animation"><div class="post-page-container"><div class="article-content-container"><div class="article-title"><span class="title-hover-animation">08_Hung-yi Lee_Transformer</span></div><div class="article-header"><div class="avatar"><img src="https://pic.imgdb.cn/item/639ece3eb1fccdcd365542dd.jpg"></div><div class="info"><div class="author"><span class="name">Carpe Tu</span> <span class="author-label">Black Whistle</span></div><div class="meta-info"><div class="article-meta-info"><span class="article-date article-meta-item"><i class="fa-regular fa-calendar-plus"></i>&nbsp; <span class="pc">2022-04-07 15:52:50</span> <span class="mobile">2022-04-07 15:52</span> </span><span class="article-update-date article-meta-item"><i class="fas fa-file-pen"></i>&nbsp; <span class="pc">2022-12-12 11:06:57</span> </span><span class="article-categories article-meta-item"><i class="fas fa-folder"></i>&nbsp;<ul><li><a href="/categories/Machine-Learning/">Machine Learning</a>&nbsp;</li></ul></span><span class="article-tags article-meta-item"><i class="fas fa-tags"></i>&nbsp;<ul><li><a href="/tags/Courses-Notes/">Courses Notes</a>&nbsp;</li><li>| <a href="/tags/Hung-yi-Lee/">Hung-yi Lee</a>&nbsp;</li><li>| <a href="/tags/transformer/">transformer</a>&nbsp;</li><li>| <a href="/tags/Seq2seq/">Seq2seq</a>&nbsp;</li></ul></span><span class="article-wordcount article-meta-item"><i class="fas fa-file-word"></i>&nbsp;<span>1.7k Words</span> </span><span class="article-min2read article-meta-item"><i class="fas fa-clock"></i>&nbsp;<span>7 Mins</span> </span><span class="article-pv article-meta-item"><i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span></span></div></div></div></div><div class="article-content keep-markdown-body"><h1 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1><p>一个 Sequence to Sequence 的model</p><h2 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h2><p>Input a sequence, output a sequence<br>The output length is determined by model</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624eadba239250f7c5dbf584.jpg"></p><p>在一些翻译问题中，我们一般<strong>直接翻译</strong>（不会有语音转文字转另一种语言的中间过程</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624edf31239250f7c53ed9a2.jpg"></p><ol><li>Seq2seq 的架构，由 Encoder 和 Decoder组成</li><li>最早的 Seq2seq 架构在2014年被发表在 arxiv 上，<a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">Seq2seq learning with Neural Networks<i class="fas fa-external-link-alt"></i></a></li><li>发扬光大的paper（右） <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is all you need<i class="fas fa-external-link-alt"></i></a>(Transformer)</li></ol><h3 id="语言翻译"><a href="#语言翻译" class="headerlink" title="语言翻译"></a>语言翻译</h3><p><strong>硬train一发：</strong> 最粗暴的训练</p><ul><li>做了一个，直接将 <strong>语音信号</strong>，转换为 <strong>繁体中文字母</strong> 的 Seq2seq模型</li><li>利用youtube上的 <em>乡土剧</em> （闽南语语言，繁体中文字幕）的资料为训练数据</li><li>Using 1500 hours of data for training</li></ul><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624eae4a239250f7c5dd0857.jpg"></p><ul><li>Background music &amp; noises? —— Don’t care</li><li>Noisy transcriptions —— Don’t care</li><li>Phonemes of Hokkien？ —— No…</li></ul><p>直接台语声音信号转中文，<strong>不是不可能实现</strong>，但是一些 <strong>倒装</strong> 等语言习惯难以学会。</p><h3 id="Text-to-Speech-Synthesis"><a href="#Text-to-Speech-Synthesis" class="headerlink" title="Text-to-Speech Synthesis"></a>Text-to-Speech Synthesis</h3><p>文本转合成语音,Text-to-Speech 文本转语音。</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624eb68d239250f7c5ecee23.jpg"></p><h3 id="Seq2seq-for-Chatbot"><a href="#Seq2seq-for-Chatbot" class="headerlink" title="Seq2seq for Chatbot"></a>Seq2seq for Chatbot</h3><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624eb6cd239250f7c5ed7083.jpg"></p><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><h3 id="Seq2seq-NLP应用"><a href="#Seq2seq-NLP应用" class="headerlink" title="Seq2seq NLP应用"></a>Seq2seq NLP应用</h3><blockquote><p>Seq2seq model 在 NLP 的领域的应用非常的多。<br>大多数 NLP 的问题，都可以被视为 <strong>Question Answering（QA）</strong> 的问题</p></blockquote><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624eb7e4239250f7c5efa4bf.jpg"></p><p><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.08730">https://arxiv.org/abs/1806.08730<i class="fas fa-external-link-alt"></i></a><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.03329">https://arxiv.org/abs/1909.03329<i class="fas fa-external-link-alt"></i></a></p><h3 id="客制化"><a href="#客制化" class="headerlink" title="客制化"></a>客制化</h3><p>对于NLP任务而言，一般<strong>客制化</strong>设计问题的解决方案，可以获得更好的结果。<br>但是 Seq2seq model 一般被认为是一种通用模型的解决方案。</p><h3 id="一些问题转换"><a href="#一些问题转换" class="headerlink" title="一些问题转换"></a>一些问题转换</h3><p><strong>文法剖析：</strong></p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624eb919239250f7c5f1f0e2.jpg"></p><p>一些并非是 Sequence 结构的东西，也可以用一些技巧转换，然后用 Seq2seq model去完成。</p><p>一篇14年的paper <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.7449">Grammar as a Foreign Language<i class="fas fa-external-link-alt"></i></a></p><h3 id="Multi-label-Classification"><a href="#Multi-label-Classification" class="headerlink" title="Multi-label Classification"></a>Multi-label Classification</h3><p><strong>Multi-class</strong>: 单纯的多分类问题<br><strong>Multi-label</strong>: An object can belong to multiple classes.</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624edcb2239250f7c5399379.jpg"><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.03434">https://arxiv.org/abs/1909.03434<i class="fas fa-external-link-alt"></i></a><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.05495">https://arxiv.org/abs/1707.05495<i class="fas fa-external-link-alt"></i></a></p><h3 id="Seq2seq-for-Object-Detection"><a href="#Seq2seq-for-Object-Detection" class="headerlink" title="Seq2seq for Object Detection"></a>Seq2seq for Object Detection</h3><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624eddfa239250f7c53c3c28.jpg"></p><h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><p>encoder: 输入一同样长的, Self-attention, <strong>rnn</strong>, <strong>cnn</strong> 都可以实现同样的</p><p>transformer的encoder就是 <strong>Self-attention</strong> 的结构</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624efb50239250f7c5821374.jpg"></p><ol><li><strong>Residual</strong>： Resnet中提出的结构设计，输入与前馈给系统的输出</li><li><strong>norm</strong>: 这里的 norm，为 <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Layer Norm<i class="fas fa-external-link-alt"></i></a> ，一个样本的所有dimension进行标准化<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align:-1.577ex" xmlns="http://www.w3.org/2000/svg" width="12.832ex" height="4.425ex" role="img" focusable="false" viewBox="0 -1259 5671.9 1956"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(605,413) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g><g data-mml-node="mi" transform="translate(605,-247) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1176.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2232.5,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1121.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2121.4,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mi" transform="translate(1434.2,-686)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><rect width="3199.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>全联接层被写为 FC standfor: fully-connected</li></ol><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624efda4239250f7c587e100.jpg"></p><ul><li>输入后会加入 Positional Encoding</li><li>Add &amp; Norm = Residual + Layer norm</li><li>Feed Forward 是 FC 层</li><li>encoder 就是 若干个这样的block叠加的结构</li></ul><p><strong>Bert</strong> 使用了与 <strong>transformer encoder</strong> 一样的结构</p><h2 id="To-learn-more"><a href="#To-learn-more" class="headerlink" title="To learn more"></a>To learn more</h2><ul><li><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">On Layer Normalization in the Transformer Architecture. <i class="fas fa-external-link-alt"></i></a><ul><li>原始论文的设计并不一定是最好的论文设计，里面发现了，右下的结构有更好的表现。</li></ul></li><li><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2003/07845">PowerNorm: Rethinking Batch Normalization in Transformers<i class="fas fa-external-link-alt"></i></a><ul><li>这篇证明了 BatchNorm 为什么在 Transformer 中，不如 Layer Norm<br><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624efff8239250f7c58d723f.jpg"></li></ul></li></ul><h1 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h1><h2 id="AutoRegressive-AT"><a href="#AutoRegressive-AT" class="headerlink" title="AutoRegressive(AT)"></a>AutoRegressive(AT)</h2><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624f1601239250f7c5be90e0.jpg"></p><ol><li>Decoder 会先把 Encoder 的输出读入。以处理语音辨识为例。</li><li>输入一个 special token (Begin Of Sentence, BOS)</li><li>完成以上两步后，在 softmax 之后，会输出一个 vector</li><li>vector 的长度很长，跟 vocabulary size 是一样的（加上 END</li><li>输出最大字后，作为输入再次进入Decoder获得下一个字节<br><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624f19c8239250f7c5c59b92.jpg"></li></ol><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624f1ae5239250f7c5c77a65.jpg"></p><ul><li><p>除开中间的 Multi-Head Attention(从外面输入) 与 Add &amp; Norm， 结构与 Encoder 相近</p></li><li><p><strong>Masked</strong> Self-attention 比起原版本，被车轮地依次纳入计算，就像Decoder的循环运行</p></li></ul><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624f1e13239250f7c5cd2216.jpg"></p><ul><li><p><strong>Decoder</strong> 的论文中，图上，特别强调了是一个 Masked Self-attention, decoder 的 token 是一个个产生的（<strong>非并行</strong>）。</p></li><li><p><strong>Decoder</strong> 自己决定，输出 Sequence 的长度，在某次输出为END后，Decoder的输出就结束了。这样Decoder就自己决定了Sequence的长度。</p></li></ul><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624f8d54239250f7c5436320.jpg"></p><h2 id="Non-autoregressive-NAT"><a href="#Non-autoregressive-NAT" class="headerlink" title="Non-autoregressive(NAT)"></a>Non-autoregressive(NAT)</h2><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624f8d8c239250f7c543cfce.jpg"></p><ul><li>How to decide the ouput length for NAT decoder?<ul><li>Another predictor for output length</li><li>Output a very long sequence, ignore tokens after END</li></ul></li><li>Advantage: parallel, more stable generation(e.g., TTS)</li><li>NAT is usually worse than AT(why? <strong>Multi-modality</strong>)<br>现在如果要让 NAT 的 Decoder 达到 AT 但表现效果，需要非常多的trick让这件事情办到。</li></ul><p>在 Self-attention 出现以后，<strong>NAT decoder</strong> 已经成为了一个热门的研究领域（大坑）</p><p>在语音合成的应用上，NAT的架构，甚至能很方便的 <strong>控制输出的长度</strong>，讲话变快、变慢</p><h3 id="To-learn-more-1"><a href="#To-learn-more-1" class="headerlink" title="To learn more"></a>To learn more</h3><p><a class="link" target="_blank" rel="noopener" href="https://youtu.be/jvyKmU4OM3c">https://youtu.be/jvyKmU4OM3c<i class="fas fa-external-link-alt"></i></a></p><h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624fa0ad239250f7c5715e8a.jpg"></p><ul><li>encoder 的输出会被在中间的 Multi-Head Attention 模块输入，计算出 Cross attention</li></ul><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624fa110239250f7c5726703.jpg"></p><p>经过 Self-attention(Mask) 生成的 query 与 encoder 生成<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="1.918ex" height="1.904ex" role="img" focusable="false" viewBox="0 -830.4 848 841.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mi" transform="translate(554,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>做相关性计算，生成attention score.</p><p>最后 weighted sum 最后生成 v 进入 fully-connected network.</p><h2 id="Cross-Attention"><a href="#Cross-Attention" class="headerlink" title="Cross Attention"></a>Cross Attention</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7472621"><strong>Listen, attend and spell</strong>: A neural network for large vocabulary conversational speech recognition</a></p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624fa7b4239250f7c582d732.jpg"><br>跟 state of art 的结果只差一点点。</p><p><em>那个时候的paper，只要是 Seq2seq 的model，paper名就要有3个动词</em></p><p>上面图表：</p><ol><li>上面的是声音信号，是机器的输入，横轴是时间，纵轴是一排排的vector</li><li>左边那排是，encoder的输出（<space>当作特殊的词汇来处理了）</space></li><li>灰度方块（自己造的）反应了不同时间节点的 attention score</li></ol><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/624fab46239250f7c58c256e.jpg"><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08081">https://arxiv.org/abs/2005.08081<i class="fas fa-external-link-alt"></i></a></p><p><em>decoder 接受的输入不一定要是 encoder最后一层的输出，有各式各样的组合</em></p><h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62507811239250f7c5a2f66b.jpg"></p><ul><li>训练时的 <strong>loss-function</strong> 为 <strong>cross entropy</strong></li><li>训练中，每次出现一个输出，解决一次分类</li></ul><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/625078e9239250f7c5a44aa1.jpg"></p><ul><li>在训练的输入，依次输入为 <strong>Ground Truth</strong>， 这个机制叫作 <u><strong>Teacher Forcing</strong></u></li><li>最后的<eos>字段，也是被要求在内</eos></li></ul><h1 id="Tip"><a href="#Tip" class="headerlink" title="Tip"></a>Tip</h1><h2 id="Copy-Mechanism"><a href="#Copy-Mechanism" class="headerlink" title="Copy Mechanism"></a>Copy Mechanism</h2><blockquote><p>对于很多任务而言，不需要 encoder 产生输出</p></blockquote><h3 id="Chat-bot"><a href="#Chat-bot" class="headerlink" title="Chat-bot"></a>Chat-bot</h3><p>User: 你好，我是<strong>库洛洛</strong><br>Machine：<strong>库洛洛</strong>你好，很高兴认识你</p><p>User: 小杰<strong>不能使用念能力</strong>了！<br>Machine： 你所谓的“<strong>不使用念能力</strong>”是什么意思？</p><h3 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h3><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62507a1a239250f7c5a68ca3.jpg"><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.04368">https://arxiv.org/abs/1704.04368<i class="fas fa-external-link-alt"></i></a></p><p>要让机器能说合理的句子，至少要 <strong>百万篇文章</strong></p><p><em>对于摘要而言，Copy Mechanism 尤为重要</em></p><p>Pointer Network: <a class="link" target="_blank" rel="noopener" href="https://youtu.be/VdOyqNQ9aww">video from Youtube<i class="fas fa-external-link-alt"></i></a><br>Incorporating Copying Mechanism in Seq2seq Learning: <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06393">paper<i class="fas fa-external-link-alt"></i></a></p><h2 id="Guided-Attention"><a href="#Guided-Attention" class="headerlink" title="Guided Attention"></a>Guided Attention</h2><ul><li>Montonic Attention</li><li>Location-aware attention</li></ul><blockquote><p>In some tasks, input and output are monotonically aligned.<br>For example, speech recognition, TTS, etc.</p></blockquote><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62507b3f239250f7c5a9171a.jpg"></p><h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><p>The <em>red</em> path is <em><strong>Greedy Decoding</strong></em>.<br>The <strong>green</strong> path is the best one.<br>Not possible to chech all the paths…<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="2.262ex" height="1.181ex" role="img" focusable="false" viewBox="0 -511 1000 522"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g></g></g></svg></mjx-container>Beam Search</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62507b93239250f7c5a9babe.jpg"></p><h3 id="不一定有用"><a href="#不一定有用" class="headerlink" title="不一定有用"></a>不一定有用</h3><p>paper: <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09751">The Curious Case of Neural Text Degeneration<i class="fas fa-external-link-alt"></i></a></p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62507c06239250f7c5aaa72d.jpg"></p><p>这篇paper处理的任务叫作 content completion, Beam search 加入后的训练结果会是，不断重复。</p><blockquote><p>Randomness is needed for decoder when generating sequence in some tasks.</p></blockquote><p>Beam Search 是否有用，基于要处理的 task</p><ol><li>任务结果非常明确，有唯一答案，通常比较有用</li><li>creative task, 很重要。<strong>语音合成TTS</strong> 要加入 Noise</li></ol><h2 id="Optimizing-Evaluation-Metrics"><a href="#Optimizing-Evaluation-Metrics" class="headerlink" title="Optimizing Evaluation Metrics"></a>Optimizing Evaluation Metrics</h2><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62507d20239250f7c5ad02b4.jpg"><br><strong>BLEU score</strong>: 依次比对，输出与 ground trues<br>最大化 BLUE score 不等效于 最小化 Cross-entropy</p><ul><li>训练的时候 minimize <strong>Cross entropy</strong></li><li>Validation: Maximize <strong>BLEU score</strong></li><li>BLUE score 无法微分，所以不方便作为train时，用的loss func</li><li>但是 test score 要用 <strong>BLUE score</strong>， 所以validation的时候，可以取BLUE score</li></ul><h3 id="training-with-BLUE-score"><a href="#training-with-BLUE-score" class="headerlink" title="training with BLUE score"></a>training with BLUE score</h3><p>只用当作 <strong>强化学习</strong> 问题，直接训练。<br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.06/32">https://arxiv.org/abs/1511.06/32<i class="fas fa-external-link-alt"></i></a></p><h2 id="Scheduled-Sampling"><a href="#Scheduled-Sampling" class="headerlink" title="Scheduled Sampling"></a>Scheduled Sampling</h2><p>mismatch: exposure bias<br>如果test的时候，一环的output错误，之后的输出，倾向于全部输出错误。</p><p>可以考虑的技巧叫作 Scheduled Sampling</p><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62507ee4239250f7c5b04c03.jpg"></p><ul><li>Original Scheduled Sampling: <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.03099">https://arxiv.org/abs/1506.03099<i class="fas fa-external-link-alt"></i></a></li><li>Scheduled Sampling for Transformer: <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.07651">https://arxiv.org/abs/1906.07651<i class="fas fa-external-link-alt"></i></a></li><li>Paralled Scheduled Sampling: <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.04331">https://arxiv.org/abs/1906.04331<i class="fas fa-external-link-alt"></i></a></li></ul><p><img lazyload="" alt="image" data-src="https://pic.imgdb.cn/item/62507ee4239250f7c5b04c03.jpg"></p></div><div class="post-copyright-info"><div class="article-copyright-info-container"><ul class="copyright-info-content"><li class="post-title"><span class="type">Post title</span>: <span class="content">08_Hung-yi Lee_Transformer</span></li><li class="post-author"><span class="type">Post author</span>: <span class="content">Carpe Tu</span></li><li class="post-time"><span class="type">Create time</span>: <span class="content">2022-04-07 15:52:50</span></li><li class="post-link"><span class="type">Post link</span>: <span class="content">Machine-Learning/08-hung-yi-lee-transformer/</span></li><li class="post-license"><span class="type">Copyright notice</span>: <span class="content">All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.</span></li></ul><div class="copy-copyright-info flex-center tooltip" data-content="Copy copyright info" data-offset-y="-2px"><i class="fa-solid fa-copy"></i></div></div></div><ul class="post-tags-box"><li class="tag-item"><a href="/tags/Courses-Notes/">#Courses Notes</a>&nbsp;</li><li class="tag-item"><a href="/tags/Hung-yi-Lee/">#Hung-yi Lee</a>&nbsp;</li><li class="tag-item"><a href="/tags/transformer/">#transformer</a>&nbsp;</li><li class="tag-item"><a href="/tags/Seq2seq/">#Seq2seq</a>&nbsp;</li></ul><div class="article-nav"><div class="article-prev"><a class="prev" rel="prev" href="/Tutorial/pytorch-tut-offcial/"><span class="left arrow-icon flex-center"><i class="fas fa-chevron-left"></i> </span><span class="title flex-center"><span class="post-nav-title-item">pytorch_tut_offcial</span> <span class="post-nav-item">Prev posts</span></span></a></div><div class="article-next"><a class="next" rel="next" href="/Machine-Learning/07-hung-yi-lee-self-attention/"><span class="title flex-center"><span class="post-nav-title-item">07_Hung-yi Lee_Self-attention</span> <span class="post-nav-item">Next posts</span> </span><span class="right arrow-icon flex-center"><i class="fas fa-chevron-right"></i></span></a></div></div></div><div class="toc-content-container"><div class="post-toc-wrap"><div class="post-toc"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#transformer"><span class="nav-number">1.</span> <span class="nav-text">transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Seq2seq"><span class="nav-number">1.1.</span> <span class="nav-text">Seq2seq</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91"><span class="nav-number">1.1.1.</span> <span class="nav-text">语言翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Text-to-Speech-Synthesis"><span class="nav-number">1.1.2.</span> <span class="nav-text">Text-to-Speech Synthesis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Seq2seq-for-Chatbot"><span class="nav-number">1.1.3.</span> <span class="nav-text">Seq2seq for Chatbot</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Notes"><span class="nav-number">1.2.</span> <span class="nav-text">Notes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Seq2seq-NLP%E5%BA%94%E7%94%A8"><span class="nav-number">1.2.1.</span> <span class="nav-text">Seq2seq NLP应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%A2%E5%88%B6%E5%8C%96"><span class="nav-number">1.2.2.</span> <span class="nav-text">客制化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E8%BD%AC%E6%8D%A2"><span class="nav-number">1.2.3.</span> <span class="nav-text">一些问题转换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-label-Classification"><span class="nav-number">1.2.4.</span> <span class="nav-text">Multi-label Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Seq2seq-for-Object-Detection"><span class="nav-number">1.2.5.</span> <span class="nav-text">Seq2seq for Object Detection</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Encoder"><span class="nav-number">2.</span> <span class="nav-text">Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#To-learn-more"><span class="nav-number">2.1.</span> <span class="nav-text">To learn more</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Decoder"><span class="nav-number">3.</span> <span class="nav-text">Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AutoRegressive-AT"><span class="nav-number">3.1.</span> <span class="nav-text">AutoRegressive(AT)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Non-autoregressive-NAT"><span class="nav-number">3.2.</span> <span class="nav-text">Non-autoregressive(NAT)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#To-learn-more-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">To learn more</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Encoder-Decoder"><span class="nav-number">4.</span> <span class="nav-text">Encoder-Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cross-Attention"><span class="nav-number">4.1.</span> <span class="nav-text">Cross Attention</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Train"><span class="nav-number">5.</span> <span class="nav-text">Train</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tip"><span class="nav-number">6.</span> <span class="nav-text">Tip</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Copy-Mechanism"><span class="nav-number">6.1.</span> <span class="nav-text">Copy Mechanism</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Chat-bot"><span class="nav-number">6.1.1.</span> <span class="nav-text">Chat-bot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarization"><span class="nav-number">6.1.2.</span> <span class="nav-text">Summarization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Guided-Attention"><span class="nav-number">6.2.</span> <span class="nav-text">Guided Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Beam-Search"><span class="nav-number">6.3.</span> <span class="nav-text">Beam Search</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E4%B8%80%E5%AE%9A%E6%9C%89%E7%94%A8"><span class="nav-number">6.3.1.</span> <span class="nav-text">不一定有用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizing-Evaluation-Metrics"><span class="nav-number">6.4.</span> <span class="nav-text">Optimizing Evaluation Metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#training-with-BLUE-score"><span class="nav-number">6.4.1.</span> <span class="nav-text">training with BLUE score</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scheduled-Sampling"><span class="nav-number">6.5.</span> <span class="nav-text">Scheduled Sampling</span></a></li></ol></li></ol></div></div></div></div></div></div></div><div class="page-main-content-bottom"><footer class="footer"><div class="info-container"><div class="copyright-info info-item">&copy; <span>2021</span> - 2023 &nbsp;<i class="fas fa-heart icon-animate"></i> &nbsp;<a href="/">Carpe Tu</a></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="website-count info-item">Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp; Totalview&nbsp;<span id="busuanzi_value_site_pv"></span></div><div class="theme-info info-item">Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.6.1</a></div></div></footer></div></div><div class="post-tools"><div class="post-tools-container"><ul class="tools-list"><li class="tools-item flex-center toggle-show-toc"><i class="fas fa-list"></i></li></ul></div></div><div class="right-bottom-side-tools"><div class="side-tools-container"><ul class="side-tools-list"><li class="tools-item tool-font-adjust-plus flex-center"><i class="fas fa-search-plus"></i></li><li class="tools-item tool-font-adjust-minus flex-center"><i class="fas fa-search-minus"></i></li><li class="tools-item tool-dark-light-toggle flex-center"><i class="fas fa-moon"></i></li><li class="tools-item rss flex-center"><a class="flex-center" href="/atom.xml" target="_blank"><i class="fas fa-rss"></i></a></li><li class="tools-item tool-scroll-to-bottom flex-center"><i class="fas fa-arrow-down"></i></li></ul><ul class="exposed-tools-list"><li class="tools-item tool-toggle-show flex-center"><i class="fas fa-cog fa-spin"></i></li><li class="tools-item tool-scroll-to-top flex-center"><i class="arrow-up fas fa-arrow-up"></i> <span class="percent"></span></li></ul></div></div><div class="zoom-in-image-mask"><img class="zoom-in-image"></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fas fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="Search..." spellcheck="false" type="search" class="search-input"></div><span class="close-popup-btn"><i class="fas fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/dark-light-toggle.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/local-search.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/code-block.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/lazyload.js"></script><div class="post-scripts pjax"><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/post-helper.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/toc.js"></script></div><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/pjax.min.js"></script><script>window.addEventListener("DOMContentLoaded",()=>{window.pjax=new Pjax({selectors:["head title",".page-container",".pjax"],history:!0,debug:!1,cacheBust:!1,timeout:0,analytics:!1,currentUrlFullReload:!1,scrollRestoration:!1}),document.addEventListener("pjax:send",()=>{KEEP.utils.pjaxProgressBarStart()}),document.addEventListener("pjax:complete",()=>{KEEP.utils.pjaxProgressBarEnd(),window.pjax.executeScripts(document.querySelectorAll("script[data-pjax], .pjax script")),KEEP.refresh()})})</script></body></html>