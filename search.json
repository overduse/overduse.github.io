[{"title":"Prepare for program contest","url":"/WalkThroughs/prepare-for-program-contest/","content":"\n\n\n学习路线感谢创佬分享的攻略\n\n\n\n\n\n\n\n数学和数论看看这里就行，主要是为了配合其他算法的应用\n\n字符串内容差不多就这么多，其中KMP可以多花时间\n\n木子喵neko\n木子喵neko – 教算法打apex的管人算法辅导\n\n","categories":["WalkThroughs"],"tags":["Learning Route","program contest","Algorithm"]},{"title":"CS229_Assignments","url":"/Courses-Assignments/cs229-assignments/","content":"\n收录了CS229_Coursera Machine Learning (Andrew Ng) 的编程作业可以到github到链接里，找对应到答案\n\nEx1: Linear Regression with Multiple Variablesgithub： https://github.com/Carp2i/AssignmentCs229/tree/master/ex1\nEx2: Logitic Regressiongithub：https://github.com/Carp2i/AssignmentCs229/tree/master/ex2\nEx3: Multi-class Classification and Neural Networksgithub：https://github.com/Carp2i/AssignmentCs229/tree/master/ex3\n更新了！ 【date 2022-01-14】\nEx4: Neural Network Learninggithub： https://github.com/Carp2i/AssignmentCs229/tree/master/ex4\nEx5: Regularized Linear Regression and Bias/Variancegithub: https://github.com/Carp2i/AssignmentCs229/tree/master/ex5\nEx6: Support Vector Machines\n还没做～【date: 2022-1-17】EX6做完了～ 【date:2022-2-18]github: https://github.com/Carp2i/AssignmentCs229/tree/master/ex6\nEX7: 做完了K-Means Clustering and Principal Component Analysisgithub: https://github.com/Carp2i/AssignmentCs229/tree/master/ex7\nEX8: 异常检测与推荐系统，完成Anomaly Detection and Recommender Systemsgithub: https://github.com/Carp2i/AssignmentCs229/tree/master/ex8\n","categories":["Courses Assignments"],"tags":["CS229 (Andrew Ng)"]},{"title":"01_Introduction to Convolutional Neural Networks for Visual Recognition","url":"/Computer-Vision/01-introduction-to-convolutional-neural-networks-for-visual-recognition/","content":"Welcome to CS231nthe class CS231n is really about computer vision which is really study of visual data.\n\nthe majority of bits flying around the Internet are visual data.\nthis field of computer vision is truly interdisciplinary field, and it touches on many different areas of science and engineering and technology.\n\nRelated Courses @ Stanford\nCS131(Fall 2016, Profs. Fei-Fei Li &amp; Juan Carlos Niebles)– Undergraduate introductory class\n\nCS231a(Spring 2017, Prof. Silvio Savarese)– Core computer vision class for seniors, matters, and PhDs– Topics include image processing, cameras, 3D reconstruction, segmentation, object recognition, scene understanding\n\nCS231n(Prof. Fei-Fei Li &amp; Justin Johnson &amp; Serena Yeung)– Neural network (aka “deep learning”) class on image classification\n\n\nAnd an assortment of CS331 and CS431 for advanced topics in computer vision.\nHistory of the computer vision70s 80s’ attempts &amp; toy examplesobject recognition\ncamera obscura\n\nwhich is a camera based on pinhole camera theories.\n1600s, the Renaissance period of time\nit’s very similiar early eyes that animals developed with a hole that collects lights。\n\n\nIn the mean time biologists started mechanism of vision. \n\nThe work done by Hubel an Wiesel in the 50s and 60s using electrophysiology\n\n\nLarry Roberts published a set of work called Block World.\n\nThe history of computer vision also starts around early 60s. \nThat’s the first PhD thesis of computer vision where the visual world was simplified into simple geometric shapes and the goal is to be able to reconginze them and reconstruct what these shapes are.\n\n\n\n\n“The Summer Vision Project” (famous MIT summer project)\n\nan attempt to use summer workers effectively in a construction of a significant part of a visual system\nthe field of computer vision has blossomed from one summer project into field of thousands of researchers still working on some of the most fundamental problems of vision\n\n\nVision, David Marr, in the late 70s\n\nin order to take a image and arrive at a final holistic full 3d representation of the visual world\n\n\n\n\n\n\ngeneralized cylinder &amp; pictorial structurethe basic idea is that every object is composed of simple geometric primitiveseither representation is a way to reduce the complex structure of the object into a collection of simpler shapes\n\nDavid Lowe\n\ntry to recognize razors by constructing lines and edges and mostly straight lines and their combination.\n\n\n\n\nobject segmentationNormalized Cut(Shi &amp; Malik, 1997)\nThe task is taking an image and group the pixels into meaningful areas.here’s one very early seminal work by Jitendra Malik and his student Jianbo Shi from Berkeley.Using a graph theory algorithm for the problem of image segmentation.\nAfter 2000face detectionaround 1999 to 2000 machine learning techniques, especially statistical machine learning techniques start to gain momentum. E.g. SVM, boosting, graphical models, the first wave of the neural network. \nOne Particular work that made lots of contribution —- AdaBoost algorithm to do real-time face detection by Paul Viola and Michael Jones\n\nSIFT featureby David Lowe, 1999The idea is that to match and the entire object to another one.\nSpatial Pyramid MatchingLazeblink, Schmid &amp; Ponce, 2006\n\n\nThe idea is that there are features in the images that can give us clues about which type of scene it is, whether it’s a landscape or a kitchen or a highway and so on and this particular work takes these features from different parts of image and in different resolutions and put them together in a feature discriptor and than we do support vector machine algorithm on top of that.\n\nBefore ImageNetIn the early 2000s, we began to have benchmark data set that can enable us to measure the progress of recognition. \nPASCAL Visual Object Challenge（20 object categories)\nImageNetwww.image-net.org.\n\njust want to recognize the world of all the objects\nto come back the machine learning overcome the machine learning bottleneck of overfitting.\nThe part of the problem is the visual data is very complex, because it’s complex, our model tend to have a high dimension. High dimension of inputs and have a lot of parameter to fit and when we don’t have enough training data overfitting happens very fast and then we cannot generalize very well.\n\n\n\n\nScientists group from Princeton to Stanford put together the largest possible dataset.\nImageNet Large-Scale Visual Recognition ChallengeBegin from 2009\n\nThere is a huge gap between 2011 and 2012\nCS231n OverviewCS231n focuses on one of the most important problems of visual recognition – image classification.\n\nThere is a number of visual recognition problems that are related to image classification, such as object detection, image captioning\n\n\nConvolutional Neural Network(CNN) have become an important tool for object recognition. CNN somtimes called convnets.\n\nThe main takeaway here is that convolutional neural networks really had this breakthrough moment in 2012, and since then there’s been a lot of effort focused in tuning and tweeking these algorithms to make them perform better and better on this problem of image classification.\nEven though CNNs perform well in the ImageNet challenges, it’s not invented overnight.\n\nPhilosophyu should really understand the deep mechanics of all of these algorithm\n\nThorough and Detailed/– Understand how to write from scratch, debug and train CNNs.\nPractical.– Focus on practical techniques for training these networks at scale, and on GPUs(e.g. will touch on distributed optimization, differences between CPU vs. GPU, etc.) Also look at state of the art software tools such as Caffe, TensorFlow, and (Py)Torch\nState of the art.– Most materials are new from research world in the past 1-3 years(2014-&gt;2016). Very exciting stuff!\nFun.– Some fun topics sunch as Image Captioning(using RNN)– Also DeepDream, NeuralStyle, etc.\n\n","categories":["Computer Vision"],"tags":["CS231n (Feifei Li)","Courses Notes"]},{"title":"02_Image Classification Pipeline","url":"/Computer-Vision/02-image-classification-pipeline/","content":"AdministrativePython &amp; NumPyall of the assignments are using Python and NumPy,If u are not familiar with Python &amp; NumPy, there is a tutorial https://cs231n.github.io/python-numpy-tutorial/\nNumPy let u write very efficient vectorized operations that do a lot of computation in just a couple lines of code.\nGoogle CloudGoogle Cloud offers the virtual machines with GPUs which can be used to accelerate the Machine Learning\nGoogle Cloud Tutorial offered by Stanford https://github.com/cs231n/gcloud\nImage Classification– A core task in Computer VisionOffical Notes: Image Classification\nSemantic Gap\n\nThe computer really is representing the image as gigantic grid of numbers, e.g. the image might be something like 800 by 600 pixel and each pixel is represented by 3 numbers, giving the red, green, blue values for that pixel.\n\nSemantic Gap:  The idea of the cat, or this label of cat, is a semantic label that we’re assigning to the image, and there’s this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing.\nChallenges\nViewpoint variation\nIllumination\nDeform\nOcclusion\n\nFormation of Classiferdef classify_image(image):    # some magic here?    return class_label    \nUnlike e.g. sorting a list of numbers.no obvious way to hard-core the algorithm for recognizing a cat, or other classes.\nData-Driven Approach\nCollect a dataset of images and labels\nUse Machine Learning to train a classifier\nEvaluate the classifier on new images\n\ndef train(images, labels):    # Machine Learning!    return modeldef predict(model, test_images):    # Use model to predict labels    return test_labels\n\nKNNExample Dataset: CIFAR10\n10 classes\n50,000 training images\n10,000 testing images\n\nThe picture in the right side are test image and nearest neighbor\nNearest Neighbor\n\ntrain function: memorize training data\npredict functions: For each test image\nFind closest train image\nPredict label of nearest image\n\n\n\nimport numpy as npclass NearestNeighbor:    def __init__(self):        pass    def train(self, X, y):        \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"        # the nearest neighbor classifier simply remenbers all the training data        self.Xtr = X        self.ytr = y    def predict(self, X):        \"\"\" X is N xD where each row is an example we wish to predict label for \"\"\"        num_text = X.shape[0]        # lets make sure that the output type matches the input type        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)        # loop over all test rows        for i in xrange(num_test):            # find the nearest training image to the i'th test image            # using the L1 distance (sum of absolute value differences)            distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)            min_index = np.argmin(distances) # get the index with smallest distance            Ypred[i] = self.ytr[min_index] # predict the label of the nearest example                return Ypred    \n\nComplexityQ: With N examples, how fast are training and prediction?\nA: Train , Predict \nThis is bad, the algorithm is expected fast at prediction; slow for training is acceptable.\nK-Nearest NeighborsThe link below here, is the visualization website designed by the TA(Justin Johnson) of CS231nhttp://vision.stanford.edu/teaching/cs231n-demos/knn/\nInstead of copying label from nearest neighbor, take majority vote from K closest points.\n\na higher value of k have a smoothing effect that makes the classifier more resistant to outliers.\nNote that: the blank regions in the 3-NN or 5-NN images are caused by ties in the votes among the nearest neighbors\nDistance Metric\nL1(Manhattan) distance (left)\n\nL2(Euclidean) distance (right)The Manhatten Distance is coordinate axis relatived\n\n\nKNN’s shortcomingk-Nearest Neighborr on images never used\n– Very slow at test time– Distance metrics on pixels are not informative– Curse of dimensionality\n\nAs an aside, the computational complexity of the Nearest Neighbor classifier is an active area of research, and several Approximate Nearest Neighbor (ANN) algorithms and libraries exist that can accelerate the nearest neighbor lookup in a dataset (e.g. FLANN).\n\nKNN in practiceIf you wish to apply kNN in practice (hopefully not on images, or perhaps as only a baseline) proceed as follow:\n\n\nPreprocess your data: Normalize the features in your data (e.g. one pixel in images) to have zero mean and unit variance. We will cover this in more detail in later sections, and chose not to cover data normalization in this section because pixels in images are usually homogeneous and do not exhibit widely different distributions, alleviating the need for data normalization.\nIf your data is very high-dimensional, consider using a dimensionality reduction technique such as PCA, NCA, or even Random Projections.\nSplit your training data randomly into train/val splits. As a rule of thumb, between 70-90% of your data usually goes to the train split. This setting depends on how many hyperparameters you have and how much of an influence you expect them to have. If there are many hyperparameters to estimate, you should err on the side of having larger validation set to estimate them effectively. If you are concerned about the size of your validation data, it is best to split the training data into folds and perform cross-validation. If you can afford the computational budget it is always safer to go with cross-validation (the more folds the better, but more expensive).\nTrain and evaluate the kNN classifier on the validation data (for all folds, if doing cross-validation) for many choices of k (e.g. the more the better) and across different distance types (L1 and L2 are good candidates)\nIf your kNN classifier is running too long, consider using an Approximate Nearest Neighbor library (e.g. FLANN) to accelerate the retrieval (at cost of some accuracy).\nTake note of the hyperparameters that gave the best results. There is a question of whether you should use the full training set with the best hyperparameters, since the optimal hyperparameters might change if you were to fold the validation data into your training set (since the size of the data would be larger). In practice it is cleaner to not use the validation data in the final classifier and consider it to be burned on estimating the hyperparameters. Evaluate the best model on the test set. Report the test set accuracy and declare the result to be the performance of the kNN classifier on your data.\n\n\nHyperparameter and ValidationWhat is the best value of k to use ?What is the best distance metrics to use ?\nThese are hyperparameters: choices about the algorithm that we set rather than learn\n\nVery problem-dependent.Must try them all out and see what works best.\n\nSetting Hyperparameters\nK-Cross ValidationCross-Validation:   Split data into folds, try each fold as validation and average the results\n\nUseful for small datasets, but not used too frequently in deep learning\n\nThe accuracy on validation set is always lower than in test dataset, that’s because validation can be thought using to fit the hyperparameter(or select the best combination of hyperparameters)\n\nLinear RegressionNeural Network likes a building, and Linear Regression likes the building blocks.\nParameter Approach： Linear Classifier\nExample with an image with 4 pixels, and 3 classes(cat/dog/ship)\nHard Examples for a Linear Classifier\nComing up– Loss function (quantfilying what it means to have a “good” )– Optimization (start with random  and find a  that minimizes the loss)– ConvNets! (tweak the functional form of f)\n","categories":["Computer Vision"],"tags":["CS231n (Feifei Li)","Courses Notes"]},{"title":"03_Loss Function and Optimization","url":"/Computer-Vision/03-loss-function-and-optimization/","content":"","categories":["Computer Vision"],"tags":["CS231n (Feifei Li)","Courses Notes"]},{"title":"CS231n_Assignments","url":"/Courses-Assignments/cs231n-assignments/","content":"Administrative:Assigment 1– K-Nearest Neigbour– Linear classifiers: SVM, Softmax– Two-layer neural network– Image features\nPython &amp; NumPy Tutorial\nGoogle Cloud Tutorial\n","categories":["Courses Assignments"],"tags":["CS231n (Feifei Li)"]},{"title":"01_C\\C++编程方法","url":"/Code-like-tourist/01-c-c-%E7%BC%96%E7%A8%8B%E6%96%B9%E6%B3%95/","content":"这一节主要摘录一些 C&#x2F;C++ 的语言特性（我时常会忘记的那些，比较偏的\nmain()\n可以理解为程序运行时就会执行 main() 中的代码。实际上，main 函数是由系统或外部程序调用的。最后的 return 0; 表示程序运行成功。\n\n\nmain 函数也可以有参数，通过 main 函数的参数，我们可以获得外界传给这个程序的指令（也就是“命令行参数”），以便做出不同的反应。\n\nCpp的文件管理Namespacenamespace(命名空间)其实是Cpp用来区分文件（避免混淆）的文件管理方式\nUsing两种Using指令的形式：\n\nusing 命名空间::成员名；调用成员的时候，可以直接使用成员名。 有点像在环境变量中引入PATH。代码示例1:#include &lt;iostream&gt;using std::cin;using std::cout;using std::endl;int main() &#123;  int x, y;  cin &gt;&gt; x &gt;&gt; y;  cout &lt;&lt; y &lt;&lt; endl &lt;&lt; x;  return 0;&#125;\nusing namespace 命名空间；可以直接通过成员名访问命名空间中的 任何 成员。\n\n#include &lt;iostream&gt;using std::cin;using std::cout;using std::endl;int main() &#123;  int x, y;  cin &gt;&gt; x &gt;&gt; y;  cout &lt;&lt; y &lt;&lt; endl &lt;&lt; x;  return 0;&#125;// 会将 std 中的 所有名字 引入// 因此，如果声明了与 std 重名的函数or变量，会导致命名冲突而导致编译错误。\n\n工程上不推荐使用， using namespace 命名空间； 的指令。会将命名空间内的所有变量和函数全部导入，容易引起命名冲突\n\n在C&#x2F;C++中，程序返回值不为零，会导致运行时错误(Runtime Error, RE)\n\n\n预处理命令\n预处理命令就是 预处理器 所接受的命令，用于对代码进行初步的文本变换，比如 文件包含操作 #include 和 处理宏 #define 等，对GCC而言，默认不会保留预处理阶段的输出 .i 文件。可以用 -E 选项保留输出文件。\n\n#include命令\n#include 其实是一个预处理命令，意思为将一个文件“放”在这条语句处，被“放”的文件被称为头文件。也就是说，在编译时，编译器会“复制”头文件 iostream 中的内容，“粘贴”到 #include  这条语句处。这样，你就可以使用 iostream 中提供的 std::cin、std::cout、std::endl 等对象了。\n\n#define命令#define 是一种预处理命令，用于定义宏，本质上是文本替换。\n#include &lt;iostream&gt;#define n 233// n 不是变量，而是编译器会将代码中所有 n 文本替换为 233，但是作为标识符一部分的// n 的就不会被替换，如 fn 不会被替换成 f233，同样，字符串内的也不会被替换int main() &#123;  std::cout &lt;&lt; n;  // 输出 233  return 0;&#125;\n（可以使用 #undef 取消宏定义）#define 作用域为整个程序，容易造成文本意外替换更鼓励的做法： 使用 const 限定符声明常量\n#ifdef#ifdef 命令的灵活使用，可以在程序在不同操作系统（环境）下，得到方便\nE.g.\n#ifdef LINUX// code for linux#else// code for other OS#endif\n\n可以在编译的时候通过 -DLINUX 来控制编译出的代码，而无需修改源文件。这还有一个优点：通过 -DLINUX 编译出的可执行文件里并没有其他操作系统的代码，那些代码在预处理的时候就已经被删除了。#define 还能使用 #、## 运算符，极大地方便调试。\n\n运算位运算\n\n\n运算符\n功能\n\n\n\n~\n位非\n\n\n&amp;（双目）\n逐位与\n\n\n|\n逐位或\n\n\n^\n逐位异或\n\n\n&lt;&lt;\n逐位左移\n\n\n&gt;&gt;\n逐位右移\n\n\n位运算的优先级低于普通的算数运算符\n逻辑运算符\n\n\n运算符\n功能\n\n\n\n&amp;&amp;\n逻辑与\n\n\n||\n逻辑或\n\n\n！\n逻辑非\n\n\n成员访问运算符\n\n\n运算符\n功能\n\n\n\n[]\n数组下标\n\n\n.\n对象成员\n\n\n&amp;(单目)\n取地址&#x2F;获取引用\n\n\n*(单目)\n间接寻址&#x2F;解引用\n\n\n-&gt;\n指针成员\n\n\n控制流分支选择控制流\nif语句if (条件1) &#123;  主体1;&#125; else if (条件2) &#123;  主体2;&#125; else if (条件3) &#123;  主体3;&#125; else &#123;  主体4;&#125;\n\nswitch 语句switch (选择句) &#123;  case 标签1:    主体1;  case 标签2:    主体2;  default:    主体3;&#125;\n\n循环for loopfor (初始化; 判断条件; 更新) &#123;  循环体;&#125;\n\nwhile loopwhile (判断条件) &#123;  循环体;&#125;\n\ndo-while loopdo {  循环体;} while (判断条件);\n","categories":["Code like tourist"],"tags":["C\\C++"]},{"title":"02_C\\C++文件操作","url":"/Code-like-tourist/02-c-c-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/","content":"文件的概念文件，是根据特定的目的而收集在一起的有关数据的集合。\n万物皆文件C&#x2F;C++ 把每个文件都看成是一个有序的字节流，每个文件都以 文件结束标志（EOF）结束。\n文件的操作步骤\nopen the file. 将文件指针指向文件，决定打开文件类型；\n对文件进行读、写操作；\n使用完，close file.\n\nfreopen 函数将输入输出流以 指定方式 重定向到文件，包含在头文件 stdio.h(cstdio)函数主要有三种方式： 读、写和附加\n命名格式FILE* freopen(const char* filename, const char* mode, FILE* stream);\n\n参数说明\nfilename\nmode： 设置文件打开模式\nstream: 文件指针，标准文件流（stdin&#x2F;stdout)活标准错误输入流（stderr）\n返回值：文件指针，指向被打开文件\n\n文件打开模式\n使用方法读入文件内容freopen(&quot;data.in&quot;,&quot;r&quot;, stdin);// data.in 就是读取的文件名，要和可执行文件放在同一目录下\n\n输出到文件freopen(&quot;data.out&quot;,&quot;w&quot;,stdout);// data.out 就是输出文件到文件名，和可执行文件在统一目录下\n\n关闭标准输入&#x2F;输出流fclose(stdin);fclose(stdout);\n\n模板#include &lt;cstdio&gt;#include &lt;iostream&gt;int main(void) &#123;  freopen(&quot;data.in&quot;, &quot;r&quot;, stdin);  freopen(&quot;data.out&quot;, &quot;w&quot;, stdout);  /*  中间的代码不需要改变，直接使用 cin 和 cout 即可  */  fclose(stdin);  fclose(stdout);  return 0;&#125;\n\nfopen函数（选读）函数大致与 freopen 相同，函数打开指定文件并返回打开文件的指针\n函数原型FILE* fopen(const char* path, const char* mode)\n可用读写函数（基本）\nfread&#x2F;fwrite\nfgetc&#x2F;fputc\nfscanf&#x2F;fprintf\nfgets&#x2F;fputs\n\n使用方式FILE *in, *out;  // 定义文件指针in = fopen(&quot;data.in&quot;, &quot;r&quot;);out = fopen(&quot;data.out&quot;, &quot;w&quot;);/*do what you want to do*/fclose(stdin);fclose(stdout);\n\nC++ 的 ifstream&#x2F;ofstream 文件输入输出流使用方法读入文件内容：ifstream fin(&quot;data.in&quot;);// data.in 就是读取文件的相对位置或绝对位置\n输出到文件：ofstream fout(&quot;data.out&quot;);// data.out 就是输出文件的相对位置或绝对位置\n\n关闭标准输入&#x2F;输出流fin.close()fout.close()\n\n模板#include &lt;cstdio&gt;#include &lt;fstream&gt;ifstream fin(&quot;data.in&quot;);ofstream fout(&quot;data.out&quot;);int main(void) &#123;  /*  中间的代码改变 cin 为 fin ，cout 为 fout 即可  */  fin.close();  fout.close();  return 0;&#125;\n\n结构体的构造函数一个普通定义的结构体，内部会生成默认的构造函数\n构造函数struct studentInfo&#123;  int id;  char gender;  // 默认生成的构造函数  studentInfo()&#123;&#125;&#125;\n\n有两种相似的表达\n正常模式struct studentInfo&#123;  int id;  char gender;  // 下面的参数用以对结构体内部变量进行赋值  studentInfo(int _id, char _gender)&#123;    // 赋值    id = _id;    gender = _gender;  &#125;&#125;\n单行实现构造函数单行定义的写法有点像python\nstruct studentInfo&#123;  int id;  char gender;  studentInfo(int _id, char _gender): id(_id), gender(_gender)&#123;&#125;&#125;;\n\n\n覆盖机制构造函数被重新定义后，会遇到默认构造函数被覆盖\nstruct studentInfo&#123;  int id;  char gender;  // 用以不初始化就定义结构体变量  studentInfo()&#123;&#125;  // 只初始化gender  studentInfo(char _gender)&#123;    gender = _gender;  &#125;  // 同时初始化id和gender  studentInfo(int _id, char _gender)&#123;    id = _id;    gender = _gender;  &#125;&#125;;","categories":["Code like tourist"],"tags":["C\\C++"]},{"title":"03_C++的一些语法特性","url":"/Code-like-tourist/03-c-%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%AD%E6%B3%95%E7%89%B9%E6%80%A7/","content":"C++中的string类以前用 char[] 的方式处理字符串很繁琐。string 类，定义、拼接、输出、处理都很简单string 只能用 cin 和 cout 处理，无法用 scanf 和 printf 处理：\nstring s = &quot;hello world&quot;;   // 赋值字符串string s2 = s;string s3 = s + s2; // 字符串拼接直接用 +string s4;cin &gt;&gt; s4;  // 读入cout &lt;&lt; 输出\n\n\ncin读入字符串的时候，是以空格为分隔符的\n如果想读入一整行的字符串，就需要使用 getline\n\n\ns 的长度可以用 s.length()获取\n字符数量，不包含\\0\n\n\n\nstring s;   // 定义一个空字符串sgetline(cin, s);    // 读取一行的字符串，包括空格cout &lt;&lt; s.length(); // 输出字符串s的长度\n\nstring 中还有个很常用的函数叫做 substr，作用时街区某个字符串中的淄川，用法有两种形式:\nstring s2 = s.substr(4);    // 表示从下标4开始一直到结束string s3 = s.substr(5,3);  // 下标5开始3个字符\n\nC 与 Cpp 结构体的区别struct stu&#123;    int grade;    float score;&#125;;struct stu arr1[10];    // Cy远里面需要写成 struct stustu arr2[10];   // C++ 里面不用写 struct,直接写 stu 就行\n\nSTLvector头文件 #include &lt; vector &gt;命名空间 using namespace std;\n\nvector默认置零\n可以在设定后动态地resize#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;int main()&#123;    vector&lt;int&gt; v1; // 定义⼀个vector v1，定义的时候没有分配⼤小     cout &lt;&lt; v1.size(); // 输出vector v1的⼤大⼩小，此处应该为0 return 0;&#125;\nvector&lt;int&gt; v(10); // 直接定义⻓度为10的int数组，默认这10个元素值都为0// 或者vector&lt;int&gt; v1;v1.resize(8); //先定义一个vector变量v1，然后将⻓度resize为8，默认这8个元素都是0// 在定义的时候就可以对vector变量进行初始化vector&lt;int&gt; v3(100, 9);// 把100⻓度的数组中所有的值都初始化为9// 直接⽤用[]下标访问即可(也可以⽤用迭代器器访问，下⾯会讲)v[1] = 2;cout &lt;&lt; v[0];\n\n几个vector的常用方法\n#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;int main()&#123;    vector&lt;int&gt; a; // 定义的时候不指定vector的⼤小     cout &lt;&lt; a.size() &lt;&lt; endl; // 这个时候size是0     for(int i = 0; i &lt; 10; i++)&#123;        a.push_back(i); // 在vector a的末尾添加⼀个元素i         &#125;    cout &lt;&lt; a.size() &lt;&lt; endl; // 此时会发现a的size变成了10     vector&lt;int&gt; b(15); // 定义的时候指定vector的大小，默认b里面元素都是0    cout &lt;&lt; b.size() &lt;&lt;endl;    for(int i=0;i&lt;b.size;i++)&#123;        b[i]=15;    &#125;    for(int i=0; i&lt;b.size(); i++)&#123;        cout &lt;&lt; b[i] &lt;&lt;&quot; &quot;;    &#125;    cout&lt;&lt;endl;    vector&lt;int&gt; c(20, 2);   // 定义的时候制定vector的大小并把所有的元素赋到一个指定值    for(int i=0;i&lt;c.size();i++)&#123;        cout &lt;&lt; c[i] &lt;&lt; &quot; &quot;l    &#125;    cout &lt;&lt; endl;    for(auto it=c.begin(); it!=c.end();it++)&#123;   // 使用迭代器的方式访问vector        cout &lt;&lt; *it &lt;&lt;&quot; &quot;;    &#125;    return 0;\n\n容器 vector\\set\\map 都可以用迭代器访问遍历。c.begin() 是一个指针，只想容器的第一个元素， c.end() 指向容器的最后一个元素的后一个位置\n","categories":["Code like tourist"],"tags":["C\\C++"]},{"title":"DataStructure_xdu","url":"/Data-Structure/datastructure-xdu/","content":"README现在是 2022.3.16 离西电空间院数据结构考试还有38小时不到，我开始复习了。感觉这篇文章，没有太大的学习价值(当作正经的笔记来学习），只是帮助我在明天晚上差不多这个时候，理解和记忆考试考点而写的。现在开始吧\n数据结构数据结构 主要研究： 非数值计算的程序设计中，计算机操作对象以及他们之间的 关系和操作\n绪论基本概念和术语基本概念\n典型结构\n集合\n线形\n树形\n图或网络\n\n逻辑与存储逻辑结构： 具体问题抽象出的数学模型。体现逻辑关系。物理结构（存储结构）： \b数据元素（Data Element）及关于在计算机中的表示\n\nDE 存储成为结点\n关系存储： a. 顺序存储    b.链式存储  c. 索引存储   d. 散列存储\n\n数据结构广义定义数据元素的 逻辑结构、物理结构、抽象运算\n基本操作插入 删除 更新 排序 查找\n\n算法的定义与特征算法定义对特定问题求解的一种描述，是指令的有限序列。\n算法特征\n有穷性\n确定性\n可行性\n0或多个输入\n1或多个输出\n\n算法的表述类C语言（C/C++）语言的基本结构： 顺序、分支、循环\n算法的基本要求\n算法分析两大标准a. 时间复杂度：算法中基本操作重复执行的次数（频度）分为 平均时间复杂度 和 最坏时间复杂度。\nb. 空间复杂度： 算法所需存储空间称此算法原地工作\n\n复杂度的值取规模函数的最高阶，都是关于 开销（时间或者空间） 与 规模 的函数。常用规则（西电的考试不会用到）加法规则\n乘法规则\n可以结合 洛必达法则 与 函数图像 来证明，口令“常对幂指阶\"\n\n\n线性表定义与基本操作定义略过（考试不会太要求这个\n 可以是基本数据类型，也可以是 struct 类型\n\n元素同构，且不能出现缺项\n元素个数 —— 表长度， 空表\n\n线性结构特点\n“第一个”唯一\n“最后一个”唯一\n除首个外，每个有且仅有一个直接前驱\n除最后外，每个有且仅有一个直接后继\n\n基本运算typedef int ElemType;void initList(sq);  // 建立线性表的架构int ListLen(sq);    // 返回线性表sq的长度ElemType GetElem(sq, i)；  // 返回第i个数据元素；否则返回NULLElemType LocateElem(sq, x);  // 按值查找，找到第一个相应元素的序号List* LinstInsert(sq, x, i);    // 在线性表sq的第i个位置上增加一个以x为值的新元素List* ListDelete(sq, i);        // 删除线性表sq的第i个元素aiElemType PriorElem(sq, e);  // 返回前驱，否则NULLElemType NextElem(sq, e);   // 返回后继，否则NULL\n\n\n顺序表这门课内，不考察 动态数组 的实现。\n特点\n实现逻辑上相邻——物理地址相邻\n实现随机存取（ 一维数组\n\n优缺点\n缺点：\n插入、删除操作需要移动大量的元素\n预先分配空间需按最大空间分配，利用不充分\n\n\n有点：\n逻辑相邻，物理相邻\n可随机存取任一元素\n存储空间使用紧凑\n表容量难以扩充\n\n\n\n单链表利用指针实现了用不相邻的存储单元存放逻辑上相邻的元素。\n结点 = 数据域 + 指针域数据域： 元素本身信息指针域： 只是直接后继的存储位置\n顺序vs链式\n存储空间存储密度结点数据本身占用存储量结点数据占用存储量\n运算时间\n程序设计语言\n\n栈与队列栈通常插入、删除的一端（表尾）为 栈顶top，另一端（表头）为 栈底bottom，表中无元素时，称为 空栈特点： 先进后出（FILO）或后进先出（LIFO）\n#define MAXSIZE 6typedef struct{    elemtype elements[MAXSIZE];    int top;}SqStack;\n\n\n基本操作\n判空判满\n栈空， top = -1, 无法出栈（underflow\n栈满， top = MAXSIZE-1，无法入栈（overflow\n\n队列只能在表的一端进插入，另一段删除\n\n队尾（rear）——允许插入的端\n队头（front）——允许删除的端队列特点： 先进先出（FIFO）#define MAXSIZE 6typedef struct{    elemtype elements[MAXSIZE];    int front, rear;}SqQueue;\n初始化时，均置为-1\n\n基本操作\n循环队列\n存储队列的数组被当作首尾相接的表处理。\n队头、队尾指针加1时从MAXSIZE-1直接进到0，可用语言的取模（余数）运算实现。\n\n\n王道考研中，rear是最后的空指针，课本里面，rear是最后一个结点。\n判空判满\n队空 rear = front\n队满 (rear + 1) % maxSize = front;\n\n串串就是字符串，n个字符的有限序列，是一种特殊的线性表。\n\n空串： 长度为0的串，没有任何字符\n空白串： 包含一个或者多个空格字符的串。\n\n\n\n空串 是任意串的子串\n任意串是自身子串\n\n基本操作\n感觉ppt上内容很少，也没怎么看明白，kmp应该大概率也不会考，应该只要知道c语言的字符串定义就差不多了。\n数组与广义表这一章的内容比较简单多画图，通过可视化来研究各种矩阵，展开后的位置。\n数组\n元素数目固定\n元素类型相同\n下标有界有序\n\n定义：由值和下标构成的有序对，结构中的每一个元素都与一对下标有关。\n运算：\n\n给定一组下标，存取对应DE\n给定下标，修改对应DE\n\n数组可以是多维的，多维数组可以以一维的方式存储。\n数组内存地址计算\n\n行优先： \n列优先： \n\n压缩存储\n多个值相同的元素，分配一个空间\n零元素不分配空间。\n\n特殊矩阵\n对角矩阵\n\n单对角矩阵（左） 三对角矩阵（右）\n用一维数组进行压缩存储，\n\n上（下）三角矩阵\n\n\n\n\n对称矩阵同三角矩阵\n\n这里的做法就是，按照 行优先 或者 列优先，求出任意(i,j)元素前面的元素数量\n稀疏矩阵 中非零原属个数远小于零元素个数组成： 所有非零元素（行，列，值） + （行数，列数，非零元素个数） \n\n\n稀疏矩阵转置算法思想\n\n时间复杂度： \n广义表广义表（lists, 又称列表）是线性表的推广（表中有表）。\n性质：\n\n元素可以是子表，子表的元素也可以是子表\n具有递归和共享的性质\n\n长度: 最外层的长度深度: 最大嵌套次数求表头: Head(Ls)求表尾: Tail(Ls) 求表的最后一个子表，用小括号（）包起来\n\n树定义tree是个结点的有限集\n\n有且仅有一个特定的结点，称为树的**根(root)**，只有直接后继，没有直接前驱\n当时，其余结点可以分为个互不相交的有限集，T1,T2,,Tm，其中每个集合本身又是一棵树，称为根的子树(subtree)if n = 0, 称为空树；\n\n特点： a. 树中至少有一个结点——根    b. 树中各子树是互不相交的集合\n基本运算&amp;术语\n\n二叉树二叉树是n（n&gt;=0)个结点的有限集，或为空树（n=0），或由一个结点和两棵分别称为左子树和右子树的互不相交的二叉树构成。\n\n结点至多两棵子树（不存在度大于2的结点）\n二叉树的子树有左、右之分，且其次序不能任意颠倒\n\n二叉树的性质\n二叉树在第层上，最多有个结点\n深度为k的二叉树，最多有个结点\n对 二叉树，若叶结点树为，度为2的非叶子结点个数为，则有\n完全二叉树，具有n个结点的完全二叉树的高度为   \n对于完全二叉树，结点序列按照层序编号，则有\ni=1， 这个结点是根，无双亲；i&gt;1， 双亲是 \n2i&gt;n, 则无左孩子，否则2i是左孩子\n2i+1&gt;n，则无右孩子\n\n\n\n特殊二叉树\n满二叉树\n完全二叉树\n\n存储结构\n顺序存储结构——适合满二叉树和完全二叉树\n链式存储\n\n二叉树遍历\n先序遍历\n中序遍历\n后序遍历\n层序遍历这几个非常重要，层序遍历需要用上队列。\n\n线索二叉树，不看，考试补考\n树的存储结构一、双亲表示法\n孩子不好找，寻找结点的孩子，需要遍历全部结点\n二、孩子表示法每个结点的孩子，用单链表存储\n\n三、孩子双亲表示法以上三种，都不是很重要\n四、孩子兄弟表示法（二叉树法，这个直接记住可以把所有都结点转化为，孩子和兄弟，左孩子，右兄弟，所有树都变成二叉树\n\n缺点，找双亲比较麻烦\n孩子兄弟转换法，森林转换\n\n森林变二叉树\n各树转成二叉树\n每棵树结点用线链接\n第一棵树的跟结点作为第二棵树的跟结点，按顺时针方向旋转（其他的树，当第一棵树的兄弟\n\n\n\n\n\n二叉树转变成森林\n\nHuffman树一种构造出有最小路径和的算法\n\n\n图定义： —— 顶点的，有穷、非空集合； —— 边的集合\n\n有向图： 每条边都有方向\n无向图： 每条边都没方向\n\n不考虑指向自身的边，与重复出现的边\n\n完全有向图： 任一顶点到其余个顶点都有弧\n\n完全无向图： 任两个顶点都有边\n\n稀疏图： \n\n稠密图： \n\n\n基本概念\n表示方法邻接矩阵\n邻接表\n\n顶点的出度： 该顶点在对应连标结点数；\n顶点的入度： 结点序号在所有连标中出现次数\n\n十字链表、邻接多重表（不列举\n邻接矩阵表示唯一； 邻接表形态于边的输入次序有关;\n遍历深度优先搜索DFS先序遍历，碰到就访问时间复杂度：        空间复杂度： \n广度优先搜索BFS\n连通性DFS和BFS只能遍历一个连通分量，遇到多个连通分量，需要多次调用\n生成树树： 无回路的连通图生成树： 连通图的绩效连通子图（n个顶点，n-1条边）\n\n\n连通网：各边带权的连通图； 其生成树全值为各边全值之和。\n\nMiniCost Spanning TreePrim\n\n找最短路径\n找连接目前回路的最短路径（引入新结点不断的加入新结点时间复杂度：    与边数无关，适合边稠密网\n\nKruskal\n\n不停的找最短边，直到所有连通分量连通时间复杂度：    与顶点数无关，适合边稀疏网\n\n拓扑排序AOV网问题发生在，无环有向图（DAG图)\n\n基本步骤：\n\n选入度为零的顶点输出\n删除改点与相关点\n重复步骤1，2，若无结点，完成；还有结点，说明有回路\n\n最短路径dijkstra\n\n从一个点出发，找到其他点，最近点距离\n然后查找，经过上一个结点，初始位置能到达点最短距离只是一个结点的运算 时间复杂度：     遍历所有结点 \n\nfloyd初始状态 -&gt; z0为中转点 -&gt; z1为中转点 。。。 -&gt;\n查找静态查找表： 顺序查找、二分查找和顺序索引   (查询、检索)动态查找表： 二叉排序树、平衡二叉树       （查询、检索、插入、删除哈希表\n关键字： 数据元素（记录）某个数据项点值\n\n主关键字：可以唯一的识别一个记录的关键字（学号\n次/辅关键字： 可以识别若干记录的关键字\n\n平均查找长度  \n查找算法顺序查找int Searth_Seq(Stable R[],keytype key){  int i=n;R[0].key = key;  while(R[i].key != key)    i--;  return (i);  // 添加了R[0] 充当监视哨，优化了反复盘踞}\n成功简单、适用范围广，ASL大\n折半查找又称二分查找条件： 有序表查找采用分治思想\nint BinSearch(Stable R[], keytype key){  low = 1; high = n;  // 置上下界值  while(low &lt;= high){    mid = (low + high)/2;    if(key == R[mid].key) return mid; // 成功    if(key &lt; R[mid].key)  high = mid - 1; // 查找左子表    else  low = mid + 1;  // 查找右子表  }  return 0; // 失败}\n\n二叉排序树（BST或是空树，或者有以下性质\n\n左子树不空，左子树上所有结点小于root\n右子树不空，右子树上所有结点大于root\n左右都是BST\n\n中序遍历 可以得到一个递增序列\n二叉树查找BiTree SearchBST(BiTree T, keytype key){  while(!T){    if(T-&gt;key == key) return T;    if(T-&gt;key &gt; key)  T=T-&gt;lchild;    else T = T -&gt; rchild;  }  return NULL;}\n\n二叉树删除\n删除叶子，双亲指向它的指针置空，释放叶子\n删除结点，有一棵非空子树，删除，接上\n左右都不空，左子树最大，右子树最小替代\n\n平衡二叉树所有平衡因子的绝对值小于1的二叉树平衡因子 = 左子树深度 - 右子树深度\n失去平衡的调整方法： LL RR LR RL\n命名是，自顶向下编排的。 调整方式，从下到上。\nHash表哈希表： 以元素的key作为自变量，通过函数f，计算地址f(key)来建立的表\n\n函数f——哈希（散列）函数地址f(key)——哈希（散列）地址\n\n\n冲突：多个元素征用一个地址key1  key2, f(key1) = f(key2)\n\n除留余数法取关键字被某个不大于 哈希表表长m 的数p除后所得余数为哈希地址其中为小于等于表长的最大质数\n解决冲突两个方法\n线性探测法（开放地址若 , d上不空，依次探测 d+1， d+2， d+3 …即 \n\n易产生堆积\n\n链地址法\n\n排序定义： 略稳定性： 同样大小的关键字，排序前后，他们的为序没有发生颠倒\n八大排序稳定性 冒泡插入基数归并\n插入排序基本思想：依次将无序记录插入到一个有序子文件中的适当位置  稳定在序列本身就有序的情况下，直接插入排序最快\n希尔排序不稳定 分组插入排序，再一起插入排序\n冒泡排序交换n轮，两两比较，互换  稳定\n快速排序\n时间复杂度  不稳定\n选择排序选择每次选择最小的元素的元素跟无序区交换时间复杂度  不稳定\n堆排序归并排序基数排序题目库顺序表章\n\n链表\n栈的递归\n栈的括号匹配见代码\n遍历二叉树求二叉树深度\n","categories":["Data Structure"],"tags":["Algorithm","Courses Notes","Program Contest"]},{"title":"GAMES101_1","url":"/computer-graphics/games101-1/","content":"最近要做一个关于mesh变形的任务，为此开始学 CG. GAMES101 Lingyu Yan 老师真的很厉害（apex champion）前两节 Lecture 没有太重要的知识点。从本章从 Lecture 3 的 Transform开始。\nLinear Algebra和所有课一样，leture2 做了线性代数的复习课这里只摘录了一些补充\nCross product\nLater in this lecture\n左边 3x3 的矩阵叫做 dual matrix of vector \nHomogenous Coordinates and TransformationHomogenous Coordinates: 齐次坐标\nAffine map = linear map + translation\n\n\nTranslation cannot be represented in matrix form\nBut we don’t want translation to be a special case\nIs there a unified way to represent all transformations?(and what’s the cost?)\n\n用齐次坐标，能够统一所有的图形变换\nMore DefinitionAdd a third coordinate(w-coordinate)\n\n2D point \n2D vector \n\nMatrix representation of translations\n\n\n\n\n增加第三维度，1为点，0为向量.做以上矩阵乘法运算，向量vector不会受影响，但是 Point 会出现平移\n\n\n\nValid operation if w-coordinate of result is 1 or 0坐标系是纵坐标系（没有\n\nvec + vec = vec\npoint - point = vector\npoint + vector = point \npoint + point = ？？\n\n？？ 是扩充定义\n引入齐次坐标，可以直接将平移变换融入矩阵乘法\n\nrotate around any point\nTranslate center to origin.\nRotate.\nTranslate back.\n\n\n3D transformationsUse 4 by 4 matrices for affine transformations\n\n先应用了线性变换再加上了平移量。\n\n旋转的变换就是前3维度，基变换\nRotation around x-, y-, or z-axis\n\n\n\nCompose any 3D rotation from \n\nSo-called Euler angles\nOften used in flight simulators: roll, pitch, yaw\n\n\nRodrigues’s Rotation FormulaRotation by angle  around axis \n\n参考资料：https://blog.csdn.net/qq_37680545/article/details/123415191https://sites.cs.ucsb.edu/~lingqi/teaching/resources/GAMES101_Lecture_04_supp.pdf闫老师这个真的看不懂。。。\n\n\n缩放scale是单位矩阵的对应维度的数乘\n\nScale\n\n\n位移是最后一列的加减\n\nTranslation\n\n\n\nViewing transformationaka Camera Transformation\n\nmodel transformation\nview tranformation\nprojection transformation\n\nDefine the camera\nPosition \nLook-at /geze direction \nUp direction \n\n\n\n\nIf the camera and all objects move together, the “photo” will be the same\n\n\n\nSO we always transform the camera to \n\nThe origin, up at Y, look at -Z\ntransform the objects along with the camera\n\n\nAlso  known as ModelView Transformation-for projection transformation\nprojection\nProjection in Computer Graphics\n3D to 2D\nOrthographic projection\nPerspective projection\n\n\n\n\n\nOrthographic Projection\nA simple way of undersatanding\nCam located at origin, looking at -Z, up at Y\nDrop Z coordinate\nTranslate and scale the resulting rectangle to \n\n\n\n\n\n\n在空间里定义一个立方体然后将立方体 [l, r] x [b, t] x [f, n] to the “canonical” cube \n\n\n\n\nTransformation matrix\ntransalte(center to origin) first, then scale(length/width/height to 2)\n\n\n\n\n\nCaveat\nLooking at/along -Z is making near and far not intuitive(n&gt;f)\nFYI:that’s why OpenGL(a Graphics API) uses left hand coords\n\n\n\nPerspective Projection\nMost common in Computer Graphics, art, visual system.\nFurther objects are smaller.\nParallel lines not parallel; converge to single point.\n\n\n\n\nHow to do perspective projection\nFirst “squish” the frustum into a cuboid(n-&gt;n, f-&gt;f)()\nDo orthographic projection(, already known!)\n\n\n\n\n\n\n\n先做挤压操作，把frustum挤压成长方体远平面挤压，近平面不变\n\n\n\n\nIn order to find a transformation\nFind the relationship between transformed points(x’, y’, z’) and the original points(x, y, z)\n\n\n\n\n根据远近平面的设定 In homogeneous coordinates\n根据近平面和远平面的两个设定（利用了近平面不变，远平面中心不变）作为边值条件，求得变换矩阵\n\n\n\n\n对于任意距离的平面，经过以上的形变，距离会被压缩的更远可以参考 知乎文章注意：n和f的设定一定是等符号的，这由透视投影的物理场景决定。\n\n\n\n","categories":["computer graphics"],"tags":["Courses Notes","GAMES101"]},{"title":"GAMES102_1","url":"/graphics/games102-1/","content":"课程简介(刘老师说) 计算机图形学主要分为：\n\n建模（设计） Modeling\n动画（仿真） Animation\n渲染（绘制） Redering\n\n\n分别对应 GAMES 的基础课 102，103，101因为，最近在折腾mesh deformation相关的项目，所以开始学习 GAMES10。\n课程信息授课老师： 刘利刚课程主页： http://staff.ustc.edu.cn/~lgliu/Courses/GAMES102_2020/default.html课程视频： https://www.bilibili.com/video/BV1NA411E7Yr/?spm_id_from=444.41.top_right_bar_window_history.content.click&amp;vd_source=c5b23ffe50af2ac743c8f9bfd9e24efe\n一些常识？图形表示\n离散表达： 光栅化\n\n函数表达： 矢量图\n\n\n\n\n\n矢量图的光栅化会有锯齿效应\n\n\n\n\n\n图像成像原理\n三维数据的获取流程\n制作三维数据:\n几何数据\nUV展开\n贴图（纹理\n材质\n灯光\n动画\n…\n\n\n\n\n函数拟合 DataFitting从一点点数学开始纯粹（理论）数学逻辑原则和推理演绎体系\n\n欧几里得《几何原本》\n总结规律进行抽象： 公理体系从五个公理出发\n\n集合\n集合： 一堆具有相同性质的对象（称为元素）\n基数（个数）： 集合中元素的个数\n有限集，例如 A={1, 2, 3, 4}\n无限集\n可数集：自然数集N、有理数集Q\n不可数集：实数集R、无理数集R\\N\n\n\n\n\n运算： 交、并、差\n\n\n线性空间\n元素之间有运算： 加法、数乘\n线性结构：对加法和数乘封闭\n加法交换律、结合律， 数乘分配律，…\n\n\n基/维数： L = span{} = \n每个元素表达为n个实数，即一个向量 \n\n\n例子：\n欧氏空间： 1D实数、2D平面、3D空间、…\nn次多项式\n\n\n\n映射\n函数\n函数空间\n完备性： 这个函数空间是否可以表示（逼近）任意函数\n赋范空间\n内积诱导函数、距离\n度量空间： 可度量函数之间的距离\nLp范数\n\n\n赋范空间+完备性=巴拿赫空间\n内积空间（无限维）+完备性=希尔伯特空间\n\n万能逼近定理： Weierstrass逼近\n定理1： 闭区间上的连续函数可用多项式级数一致逼近\n定理2： 闭区间上周期为  的连续函数可用三角函数级数一致逼近\n\n\n\n\n如何求满足要求的函数\n大部分的实际应用问题\n可建模为： 找一个映射/变换/函数\n输入不一样、变量不一样、维数不一样\n\n\n如何找函数的三部曲\n到哪儿？\n确定某个函数集合/几何\n\n\n找哪个？\n度量哪儿个函数是好的/“最好”的\n\n\n怎么找？\n求解或优化：不同的优化方法与技巧，既要快、又要好…\n\n\n[注]这里先暂时限定为单变量的函数形式\n\n\n\n曲线/曲面拟合问题\n拟合问题 Fitting\n输入： 一些观察的数据点\n输出： 反映这些数据规律的函数 \n\n\n到哪儿找\n选择一个函数空间\n线性函数空间 A=span{(x),\\cdots,B_n(x)}\n多项式函数span{}\nRBF函数  (径向基函数) Guassian Function\n三角函数\n\n\n函数表达式\n\n求n+1个系数\n\n\n\n\n\n找哪儿个\n目标1: 函数经过每个数据点（插值）\n\n\n怎么找\n目标1： 每个数据点都要插值（零误差）\n联立，求解线性方程组：\n求解  线性方程组\nn 次 Langrange 插值多项式\n\n\n病态问题： 系数矩阵条件数高时，求解不稳定\n\n插值与拟合Lagrange 插值函数\n插值n+1个点、次数不超过n的多项式是存在而且是唯一的\n(n+1个变量，n+1个方程)\n\n\n\n\n\n\n插值函数的自由度 = 未知数个数 - 已知量个数\n最小二乘\n函数尽量靠近数据点（逼近\n\n\n\n\n对各系数求导，得到方程（线性方程组）\n\nAX=b\n\n\n\n最小二乘法\n\n问题\n点多，系数少？\n点少，系数多？\n\n\n\n\n过拟合和欠拟合\n避免过拟合的方法\n数据去躁\n剔除训练样本中噪声\n\n\n数据增广 data augment\n增加样本数，或者增加样本的代表性和多样性\n\n\n模型简化\n预测模型过于复杂，拟合了训练样本中的噪声\n选用更简单的模型，或者对模型进行裁剪\n\n\n正则约束\n适当的正则项,比如方差正则项、稀疏正则项\n\n\n\n岭回归 Ridge Regression\n稀疏学习：稀疏正则化\n冗余基函数（过完备\n通过优化来选择合适的基函数\n系数向量  模（非0元素个数）尽量小\n挑选（“学习“） 出合适的基函数\n\n\n\n\n压缩感知\n","categories":["graphics"],"tags":["Courses Notes","Ligang Liu"]},{"title":"01_The philosophy behind imformation theory","url":"/Information-Theory/01-the-philosophy-behind-imformation-theory/","content":"写在前面\n看的网课是：台湾交通大学-陈伯宁（Po-ning chen）\n主要证明方法： 大数定理\n课本：An Introduction to Single-User Information Theory, Fady Alajaji and Po-Ning Chen.\n\nShannon’s theorem\nShannon first theorem -&gt; source coding theorem\nShannon second theorem -&gt; channel coding theorem\nShannon third theorem -&gt; rate-distortion therem\nInformation transmission theorem (heavy-core) == joint source-channel coding theorem\n\nOverviewChannel Coding Entropy这个模块是 Channel Coding Theorem（second theorem)\nClaude Shannon give a conclusive result.\nChannel Capacity: 通信管道的传输是有上限的。不可能比超越 Capacity 即使是技术进步。\n\nSource Entropy\n随机信号源无损压缩的极限是source entropy\n这里shannon的论文上写的是 Theorem2 但是是 Shannon First Theorem\nRate-Distortion\n信息压缩要有点损失，传输的Entropy小于Channel Capacity的时候，损失（Distortion）和传送速率（Rate）的关系： Rate-Disortion\nRandom Coding Augment\nShannon 提出的一种 “船新” 的证明模式\n只证明了 最优传输方法的存在性\n数学期望逼近0，那么必定存在方法让错误率-&gt;0\n\nIntroductionInformation\nWhat’s the information\n\nUncertainty\ninformation is a message that is previously uncertain to receivers\n\n\n\n\nRepresentation of Information\n\nAfter obtaining the information, one may wish to store it or convey it; this raises the question that:\n\n\n\n\n\n\nhow to represent information for ease of storing it or for ease of conveying it?\n\n\n\nRepresentation of information\nCode: 和 language 一样是有限符号集的排列。不同的编码，就好似不同的人类语言\nDictionary and Codebook有时候会把 book 省略\n\nAssumption made by both transmitter and receiver about symbolized information\nAll “possible symbols” of the conveyed information are a priori known.\nThe receiver is only uncertain about which symbol is going to be received.\n\n\nExample. In a conversation using English,\nit si priori known that one of the vocabularies in an English dictionary is going to be spoken.\nJust cannot tell which before its reception/\n\n\nExample. In code digital communications,\nthe codebook(or simply code)–the collection of all possible concatenations of pre-defined symbols–is always a priori known (to the receiver).\nOnly uncertain about which is going to be receiced.\n\n\n\nCompactnessCompactness是一个很古典的用词，等同于现代的 Compression\n\nHow to find the most compact code?\n\n推导最小的平均码长在所有可能的编码，再构建一个满足最小要求的编码\n通过measure将要传输的信息的多少\n\nHow to measure inforrmation信息是纪律的函数\n\n公设\n几率相关: 事件越不可能发生，所携带的信息越多。\n可加性：独立事件的信息量，等于两者相加。\n连续性：事件发生几率的微小改变会引起信息量的微小改变。\n\n\n\n\nThe only “measure” satisfying these axioms is:\n\n\n\n \n\nIf Shannon’s statement is true, then the below definitions on information content are equivalent:\n(Engineering view) The average cdeword length of the most compact code representing the information\n(Probabilistic view) Entropy of the information\n\n\nIn 1948 Shannon proved that the above two views are actually equivalent(under some constraints). I.e., the minimum average code length for a source descriptive code is indeed equal to the entropy of the source.\n\nOne can compute the entropy of a source, and assures that if the average codeword length of a code equals the source entropy, the code is optimal.\nContribution of Shannon\nShannon’s work laid the foudation for the field of information theory.\nHis work indicates that the mathematical results of information theory can serve as a guide for the development of information manipulation systems.\n\n\n\nThe measure of information is defined based on the definition of compactness\nthe definition of measure of code compactness may be application-depandent.\n\n\n虽然Shannon给出了一个conclusive result。仍然有许多基于应用的例子。\n\n\n为了对抗噪声，引入了编码冗余\n3比特传输一个信息，传输的效率也就降低为1/3 information symbol per channel usage\n\nInformation transmission上面的coder其实都是encoder\n\nSource Encoder是对冗余信息的压缩系-&gt;（压缩到最好）Entropy\nChannel Encoder 跟Source Encoder和在一起设计，称为 joint source-channel encoder.\n\nShannon’s Separate Design两个Encoder独立且最优的时候，整个joint Encoder最优。\n\nSource encoder\nFind the most compact representation of the informative message.\n\n\nChannel encoder\nAccording to the noise pattern, add the redundancy so that the source code bits can be reliably transmitted.\n\n\n\n\nSource encoder 资料压缩，挤出冗余信息。Channel encoder 添加结构性冗余，以对抗 noise\ncompression rate\n\n\nFor source encoder, the system designer wishes to minimize the number of U’s required to represent one Z’s, i.e,\n\n\n\n\nCompression Rate 越低越好\n\nShannon tells us that(for i.i.d. Z’s)\n\n上式中，log的底数是信号u的事件集合大小 {0，1}的底就是2\n\n\n\n最好的压缩方法，讯息的 source entropy=1\n最好的压缩码就是uniformly distributed.\n\n\n\n反过来，如果输出的编码分布不是 uniformed那么一定不是最好的压缩码（Entropy&lt;1）\n\ntransmission rateChannel Encoder的输出序列为 \n\nChannel code rate每个X表示的U的量。\n","categories":["Information Theory"],"tags":["Courses Notes","information therory","Po-ning chen"]},{"title":"MIT18.06_1_Course Introduction","url":"/Linear-Algebra/mit18-06-1-course-introduction/","content":"Course Introduction\n and the Four Subspaces\nLeast Squares, Determinants and Eigenvalues\nPositive Definite Matrices and Applications\n\nWebsitehttps://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/\nPrerequisitesMultivariable Calculus\nCourse Goal\nSystems of linear equations\nRow reduction and echelon forms\nMatrix operations, including inverses\nBlock matrices\nLinear dependence and dimensions\nOrthogonal bases and orthogonal projections\nGram-Schmidt process\nLinear models and least-squares problems\nDeterminants and their properties\nCramer’s Rule\nEigenvalues and eigenvectors\nDiagonalization of a matrix\nSymmetric matrices\nPositive definite matrices\nSimilar matrices\nLinear transformations\nSingular Value Decomposition\n\n","categories":["Linear Algebra"],"tags":["Courses Notes","Gilbert Strang"]},{"title":"MIT18.06_2_Unit 1 AX=B and the four subspaces","url":"/Linear-Algebra/mit18-06-2-unit-1-ax-b-and-the-four-subspaces/","content":"\nNote:\n\n\n\nthe properties of A and b determine the solutions x(if any exist) and pay particular attention to the solutions to Ax=0;\n\n\n\nthe geometry of linear equationsthere are 3 ways of thinking about these systems.\n\n“row method“: individual equations\n“column method“: combining the columns\n“matrix meth“: more compact and powerful way of describing systems of linear equations.\n\nAx=bn linear equations, n unknowns.\nn equal 2, for example:\n\nrow method被认为是 n 个 （n-1）维度空间的交集\n\n\n\n比如说，二维矩阵方程（二维空间为平面）的解是俩一维空间（直线）的交集。三维矩阵方程（rank3，其实这里说是 秩更合适）的解，是三个二维空间（平面）的交集\n\n\n\n\ncolomn method可以看作是 n 个 n 维向量与未知量的线性组合\nMatrix MultiplicationHow do we multiply a matrix A by a vector x\n\nthere are 2 methods to get the entries of x as the coefficients of a linear combination of the column vectors of the matrix:\nthis technique shows that Ax is a linear combination of the columns of A.\nyou may also calculate the product Ax by taking the dot product of each row of A with the vector x:\n\nLinear Independencefor Ax=b:\n\nfor every possible vector b?\ndo the linear combinations of the column vectors fill the xy-plane(or space, in the three dimensional case)?\n\nNO, if A is a singular matrix. In this singualr case its column vectors are linearly dependent\nAll linear combinations of those vectors lie on a point or line(in 2 dimensions) or on a point, line or plane(in three dimensions). The combinations don’t fill the whole space.\nOverview of key ideasThe pro. Strang give us 2 examples to understand a series of conceptions.\nThe first example:\nthe other example:\n\n主要用这两个例子，展示了 基、子空间、长成空间 的概念\nSubspaces\n从几何上看，矩阵C的各列所表征的向量都在一个平面上（这些向量组是线性相关的，来自矩阵A点向量组 线性不相关）\n\n三维向量空间  中包含许多向量，可以用向量组A表示，但是无法用C表示,这些无法用 矩阵C 中的向量组表示的点\n\n不在 C 所在平面上\n无法用 向量组C 的线性组合表示\n\n\n\nbasis\na basis for  is a collection of  independent vectors in .\na basis is a collection of  vectors whose combinations cover the whole space.\na collection of vectors forms a basis whenever a matrix which has those vectors as its columns is invertible.\n\nvector spacea vector space is a collection of vectors that is closed under linear combinations.\n\n\n\n向量空间 就是 线性组合闭合的向量集合\n\n\n\nsubspacesa subspace is a vector space inside another vector space\n\n\n\n子空间是 被一个向量空间内涵的向量空间\n\n\n\nThe subspaces of  are:\n\nthe origin,\na line through the origin,\na plane through the origin,\nall of \n\nElimination with matrices\n\n\n is the technique most commonly used by computer software to solve systems of linear equations.\n\n\n\nMethod of Elimination消去法(Elimination) 可找到  的解 ，在矩阵 可逆时。\n这里有个概念叫做 （也许叫作 主元），指着每个行列数最小的非零值。随着矩阵的不断消除，若矩阵出现某行不存在 pivot，那么原矩阵不可逆。\nBack Substitution当通过 消去法 或者上三角举证后，可以利用  (倒转代换) 的方法得到方程的解。\n得到矩阵  后可以通过方程，以  的顺序迭代，得到最后的结果。\nElimination Matrices消去矩阵，是一种基本矩阵，若要消除 row 2 column 1 的元素（通过行变换）可以左乘矩阵 。相应的，通过列变换，右乘举证。\n\nPermutation Matrices置换矩阵，是另一种基本矩阵，左乘则于行变换作比较，右乘以列变换做比较。For example,\n基本矩阵共同的重要性质： 可逆具体就是做基本变换的反变换。\nMultiplication and inverse matrice我们从四个角度去思考，两个矩阵的乘积 。\nStandard\n\n\nstandrad way:   equals the dot product of row  of matrix  and column  of the matrix .\n\n\n\n这就是大家在 线性代数 中学过的，最终矩阵的元素定义。\nColumns\n\n\nthe product of matrix  and column  of matrix B equals column  of matrix . This tells us that the columns of  are combinations of columns of .\n\n\n\n矩阵 和 矩阵 的第列 的乘积（其实就是我们之前提及的，），可以视为 向量组$A 的线性组合。\nRows\n\n\nThe product of row  of matrix  and matrix  equals row  of matrix . So the rows of  are combinations of rows of .\n\n\n\n矩阵的第列 和 矩阵 的乘积为 矩阵 , 这时  的行为  行的线性组合。\nColumn times rowA column of  is an  vector.A row of  is a  vector.Their product is a matrix:\n\n\n\n\nThe columns of this matrix are multiples of the column of  and the rows are multiples of the row of .The product of  and  is the sum of these “column times row” matrices:\n\n\n\n\n可以类比 standrad视角 的书写方法，每个元素的  被一并提出。\nBlock\n\n\nIf we subdivide  and  into blocks that match properly, we can write the product  in terms of products of the blocks:\n\n\n\n\nHere \n老师说，即使是高斯，也不能一眼看出这里的问题所在。\nInverse(Square matrices)\n\n\nIf  is a square matrix, the most important question you can ask about it has an inverse . If it does, then  and we say that  is invertible or nonsigular.\n\n\n\n\n\n\nIf  is singular -i.e.  does not have an inverse - its determinat is zero and we can find some non-zero vector  for which .\n\n\n\n\n方阵 如果存在 方阵 使得 ，那么称这个方阵为 矩阵的逆。\n如果矩阵可逆，则称原矩阵为 可逆 或者是 非奇异\n如果矩阵不可逆，则称原矩阵为 不可逆 或者是 奇异\n\n\n如果 矩阵 奇异 - 也就是说 没有逆阵 - 这个矩阵的行列式 不为零。\n\n\n\n\nFinding the inverse of a matrix is closely related to solving systems of linear equations.\n\n\n\n\nGauss-Jordan Elimination我们可以使用 \n\nGauss-Jordan Elimination(有一种翻译叫作 高斯-若而当消元法)\n可以同时解决 两个及以上 的线性方程。\n\n\n\n\n\n\nJust augment the matrix with the whole identity matrix :\n\n\n\n矩阵方程，可以很自然的表示多个线性方程组，并且通过 Gauss-Jordan Elimination 完成消除，这也就完成了 同时解多个方程组。\n使用 单位矩阵 进行矩阵增广，得到 增广矩阵。** augment 在某些语境下，有 强化 增强 的意思 **\n\n\n\n\n(Once we have used Gauss’ elimination method to convert the original matrix to upper triangular form, we go on to use Jordan’s idea of eliminating entries in the upper right portion of the matrix.)\n\n\n\n当我们完成使用 Gauss消元法 将上三角矩阵形式，紧接着运用 Jordan的想法 消除矩阵右上三角位置处的元素。\n\nGuass-Jordan 证明\n\n\nwe can write the results of the elimination method as the product of a number of elimination matrices  with the matrix .Letting  be the product of all the , we write the result of this Gauss-Joran elimination using block matrices:\n\n\n\n\nBut if , then .\n我们把 消去法 的效果等效于 左乘消去矩阵 , 对 增光矩阵 进行 块划分，等效于对 两个子矩阵 和 一起左乘 \n其中： ， 根据，矩阵的逆的定义，可以直接得出\nFactoration A=LU\nunderstand Gaussian elimination in terms of matrices.\nfind a matrix  such that .\n\nInverse&amp;Transpose of a productThe inverse of a matrix product  is .\n\n\n\ntranspose of a matrix by exchanging its rows and columns.the entry in row  column  of  is the entry in row  column  of \n\n\n\nThe transpose of a matrix product  is . For any invertible matrix , the inverse of  is \nA=LU\n\n\nWe’ve seem how to use elimination to convert a suitable matrix  into an upper triangular matrix .\n\n\n\n我们将会见证 如何将一个 合适 的矩阵 转换为一个 上三角矩阵。\n\n\n\nwhen there are no row exchanges, we can describe the elimination of the entries of matrix  in terms of multiplicaiton by a succession of elimination matrices , so that .\n\n\n\nIn the two by two case this looks like:\n\n从左到右，依次是矩阵 \n\n\n\nwe can convert this to a factorization  by “canceling” the matrix ;multiply by its inverse to get \n\n\n\n\n从左到右，依次是矩阵 \n\n\n\nThe matrix  is upper trangular with pivots on the diagonal.矩阵 是一个上三角矩阵（因为Gaussian Elimination只对 主对角线 以下的下三角区域元素进行消元）\n\n\n\n进一步可以进行列变换，生成 下三角矩阵x对焦矩阵x上三角矩阵 的形式:\n\n从左到右，依次为矩阵 \nhow expensive is elimination\n\n\nSome applicaitons require inverting very large matrices.\n\n\n\n许多应用，需要对 超大矩阵 求逆\n\n\n\nWhen using elimination to find the factorization  we just saw that we can build  as we go by keeping track of row subtractions. We have to remember  and (the matrix which will become) ;\n\n\n\n当使用 消去法 寻找  分解时，可以知道，利用消去法的track 来循迹。\n\n\n\nwe don’t have to store  or  in the computer’s memory.\n\n\n\n我们不得不，存储 矩阵 和 矩阵 于计算机内存。\nHow many operations does the computer perform during the elimination process for an  matrix ?\n\n\n\nA typical operation is to multiply one row and then subtract it from another, which requires on the order of  operations.  There are  rows, so the total number of operations used in eliminating entries in the first column is about . The second row and column are shorter; that product costs about  operations, and so on. The total number of operations needed to factor  into  is on the order of : \n\n\n\n\n\n\n\nWhile we’re factoring  we’re also operating on b. That costs about  operations, which is hardly worth counting compared to \n\n\n\nRow exchanges\n\n\n这一节就不翻译了，大概就是，行交换矩阵的表达形式。\n\n\n\nWhat if there are row exchanges? In other words, waht happens if there’s a zero in a pivot position?    To swap two rows, we multiply on the left by a permutation matrix. For example,\n\nswaps the first and second rows of a  matrix. The inverse of any permutation matrix  is .\nThere are  different ways to permute the rows of an  matrix(including the permutation that leaves all rows fixed)so there are  permutation matrices. These matrices form a multiplicative group.\nHomogeneous Linear System\n这一节是TA的课程，推荐听听 https://www.youtube.com/watch?v=3cMyj8EKFGo&amp;t=11s （需要科学上网比较直观的解释了，非齐次/齐次 方程的任意解的形式。\nThe set  of points \n\n集合 满足方程。\n集合中的所有点都满足方程\n方程的所有解都在集合之中。\n\n\n约束方程是 非齐次线性方程\n方程（组）的左侧，为线性的。\n方程（组）的右侧（常数项）不为零。\n\n\n\nThe set  of points \n\n约束方程是 齐次线性方程\n方程（组）的左侧，为线性的。\n方程（组）的右侧（常数项）为零。\n\n\n\n集合 对应的几何形态为 平面，此外 平面方程 和 平行。另外，齐次线性方程经过原点：即，平面经过原点坐标原点。\n非齐次线性方程解的一般形式\nMartina(TA) 把解的形式写成了上状\n理解：\n\n等式右边第一个矩阵，是 平面 上的一个特解\n等式右边的后两个矩阵\n线性无关\n\n\n的解释\n两个线性无关的向量的 线性组合 可以表示平面中的任意点\n\n\n再加上 原平面 上的一个特解，等效于将  原点平移到  的特解位置\n\n助教的图像解释：\n\nTransposes and PermutationsTransposesPermutations : execute row exchanges.\n\n\n\nMultiplication by a permutation matrix  swaps the rows of a matrix; when applying the method of elimination we use permutation matrices to move zeros out of pivot positions. Our factorization  then becomes , where  is a permutation matrix which reorders any number of rows of . Recall that , i.e. that .\n\n\n\n\n乘上转换 矩阵 可以交换对应的 矩阵行； \n在 消去法 的应用中，可以利用 矩阵 进行主元的变化；\n\nPermutation\n\n\nWhen we take the transpose of a matrix, its rows become columns and its columns become rows. If we denote the entry in row  column  of matrix  by , then we can describe  by: , then we can describe  by: . For example:\n\n\n\n\nsymmetric matrix: \nA matrix  is symmetric if . Given any matrix (not necessarily square) the product  is always symmetric, because .(Note that )\n\n\n\nMatlab not only checks whether that pivot is not zero. It checks for is that pivot big enough, because it doesn’t like very, very small pivots. Pivots close to zero are numerically bad.\n\n\n\n\nMatlab 不仅检查主元是否为零。 \n它还确认主元是否 够大。\n如果 主元 过小，从 数值计算 角度，计算误差会比较大。\n从 代数 角度看，较小的 主元 其实没啥事。\n\n\n\n\n对于阶行列式，仅仅因为 Permutation, 有  种不同的变换矩阵。\n\n超级重要的一个定理:\n","categories":["Linear Algebra"],"tags":["Courses Notes","Gilbert Strang"]},{"title":"MIT18.06_3_A Bigger Picture with vector spaces","url":"/Linear-Algebra/mit18-06-3-a-bigger-picture-with-vector-spaces/","content":"\nNot just vectors, but spaces of vectors and sub-spaces of those spaces.\n\n不只是向量，还包括向量空间和这些空间的子空间。\nVector spaces\nLet we say again what this word space is meaning.When we say that word space, that means that I’ve got a bunch of vectors, a space of vectors. But not just any bunch of vectors.It has to be a space of vectors – has to allow me to do the operations that vectors are for. It have to be able to add vectors and multiply by numbers.\n\n谈及 空间 时，意味着 一大堆的向量。但不是 一大堆向量 就能被称为 向量空间 ，必须满足，空间中的向量，能进行 线性操作。\n上面的内容是根据Gilbert老师的原话翻译的，有些不准确，准确翻译:\n\n\n线性空间 应该满足，向量的线性组合，闭合，即向量的 线性组合 依然满足空间的条件。\n\n\n\nOne such vector space is , the set of all vectors with exactly two real number components. We depict the vector  by drawing an arrow from the origin to the point  which is  units to the right of the origin and  units above it, and we call  the “x-y plane”.\n\n\nAnother example of a space is , the set of (column) vectors with  real number components.\n\nClosure闭合性\n\nThe collection of vectors with exactly two positive reall valued components is not a vector space.The sum of any two vectors in that collection is again in the collection, but multiplying any vector by, say, -5, gives a vector that’s not in the collection.We say that this collection of positive vectors is closed under addtion but not under multiplication.\n\n如果全部由 正实向量 组成空间，经过 加性运算 的结果也属于这个空间，但是考虑 数乘（multiplication） 不一定属于这个空间。\n那么对于这个 space 来说，只 加法闭合 但没有 乘法闭合。\n\nIf a collection of vectors is closed under linear cominations(i.e. under addtion and multiplication by any real numbers), then we call that collection a vector space.\n\nSubspaces子空间\n\nA vector space that is contained inside of another vector space is called a subspace of that space.For example, take any non-zero vector  in . Then the set of all vectors , where  is a real number, forms a subspace of . This collection of vectors describes a line through in  and is closed under addition.\n\n\n被一个向量空间包含的向量空间，叫作第一个空间的 子空间。\nThe subspaces of  are,\n\n\nall of ,\nany line through  and\nthe zero vector alone(Z).\n\n\nThe subspaces of \n\n\nall of ,\nany plane through the origin,\nany line through the origin, and\nthe zero vector alone(Z).\n\nColumn spaceGiven a matrix  with columns in , these columns and all their linear combinations form a subspace of . This is the column space . \nIf , the column space of  is the plane through the origin in  containing  and .\nColumn space and nullspaceIt’s really getting to the center of linear algebra.\nReview of subspaces\nA vector space is a collection of vectors which is closed under linear combinations. \nfor any 2 vectors v and w in the space and any 2 real numbers  and , the vector  is also in the vector space.\nA subspace is a vector space contained inside a vector space.\n\n存在两个过 原点 的子空间和，和的并集不一定是子空间，和的交集 一定是 子空间。\n\n第一个证明 举反例。对于 与 的交集，同时处于两个子空间中。那么他的线性组合，也 即在子空间中，也在子空间中。 那么  也满足 线性运算闭合 特性。\n\nColumn space of AThe column space of a matrix  is the vector space made up of all linear combinations of the columns of .\nSolving \nGiven a matrix , for what vectors  does  have a solution ?\n\nQUESTION: 给定矩阵，对于任意向量，都有解?\nLet\n\n方程  对于任意向量 不一定有解。\n解决方程  等效于求解一个有 3个未知数的4等式方程\n如果存在 解, 那么 常数必须是 矩阵 的 列向量 的线性组合。\n\n\n\n\nBig question: what ’s allow  to be solved ?\n\n一种视角： 由矩阵的列向量组构成的 张成空间，上面的点，都是 矩阵方程 有解的\n矩阵方程  可以视为，矩阵 的列向量的排列组合，系数为  的元素。\n\nA useful approach is to choose  and find the vector  corresponding to that solution.The components of  are just the coeffcients in a linear combination of columns of .The system of linear equations  is solvable exactly when  is a vector in the column space of .\n\n使用列向量生成 张成空间 时，张成空间 的维度跟最大线性无关组有关。\nNullspace of AThe nullspace of a matrix  is the collection of solutions  to equation .\n\nThe column space of the matrix in our example was a subspace of .The nullspace of  is subspace of . To see that it’s a vector space, check that any sum or multiple of solutions to  is also a solution:  and \n\nIn the example:\n\n\nthe nullspace  consists of all multiples of \n\ncolumn 1 plus column 2 minus column 3 equals the zero vector. This nullspace is a line in \nOther values of bThe solutions to the equation:\n\n以上的 解空间 并不是一个 子空间（被要求 线性闭合）。\n零向量 并不是等式的解。\n\n\n形成的 解集 是在 三维空间中的一条经过 (1, 0, 0)、(0, -1, 1) 但没有过(0, 0, 0) 的直线。\n\n\nPivot variables， Special solutions\n\n\nSo this is the, turning the idea, the definition, into an algorithm\n\n\n\nComputing the nullspace\nThe nullspace of a matrix  is made up of the vector  for which .\n\nSuppose:\n(Note that the columns of this matrix  are not independent.)\n\nOur algorithm for computing the nullspace of this matrix uses the method of elimination, despite the fact that  is not invertible.\n\n使用 消去法 来计算 零空间，无需考虑 矩阵 是否可逆。\n(We don’t need to use an augmented matrix because the right side(the vector ) is  in this computation.)不用写成 增广矩阵 的形式，因为 常数项 是 零向量。\n\nThe row operations used in the method of elimination don’t change the solution to  so they don’t change the nullspace. (They do affect the column space.)\n\n消去法 的行变换，不会改变 矩阵方程 的解。\nThe first step of elimination gives us:\n\n\n\nWe don’t find a pivot in the second column, so our next pivot is the 2 in the third column of the second row:\n\n\n我们没能在第二列找到一个主元，因此 第二主元 在第二行的第三列。\n\n\n\nThe matrix  is in echelon(staircase) form. The third row is zero because row 3 was a linear combination of rows 1 and 2; it was eliminated.\n\n\nrankThe rank of a matrix  equals the number of pivots it has.\nIn this example, the rank of (and of ) is 2.\n一个矩阵的秩 = 主元数量\nSpecial solutions\nOnce we’ve found  we can use back-substitution to find the solution  to the equation . In our example, columns 1 and 3 are pivot columns containing pivots, and columns 2 and 4 are free columns. We can assign any value to   and ; we call these free variables. Suppose  and . Then:\n\n一旦我们利用 消元法 获得 梯形矩阵，便可 回代 获得矩阵方程 的 解。\n\n存在 主元 的列，被称为 主元列\n其余列，被称为 自由列\n\n\n在 向下消元 获得矩阵\nand:\n\nSo one solution is  (because the second column is just twice the first column). Any multiple of this vector is in the nullspace.\n\n原矩阵的第二列刚好是，第一列元素的两倍。特解的线性组合，仍然属于该 零空间。\n\nLetting a different free variable equal 1 and setting the other free variables equal to zero gives us other vectors in the nullspace. For example:\n\n\n\nhas  and . The nullspace of  is the collection of all linear combinations of these “special solution” vectors.\n\n\n对 矩阵 自由列对应的，附上不同的值，会获得一个新的 特解 “special solution”\n\n\nThe rank  of  equals the number of pivot columns, so the number of free columns is : the number of columns(variables) minus the number of pivot columns.This() equals the number of special solution vectors and the dimension of the nullspace.\n\n\n矩阵的秩 = 主元列数\n自由列数\n 既是 nullspace 的维数，也是 最大线性无关组 的方程数。\n\nrref: Reduced row echelon form简化列阶形式\n\nBy continuing to use method of elimination we can convert  to a matrix  in reduced row echelon form (rref form), with pivots equal to 1 and zeros above and below the pivots.\n\n\n原矩阵 在使用 消元法 之后，获得的形式是 矩阵，为了获得 rref形式的矩阵 需要进行进一步的化简：\n\npivots 简化为零。\n利用向上消元，将 主元之上 的元素消除。\n\n\nBy exchanging some columns,  can be rewritten with a copy of the identity matrix in the upper left corner, possibly followed by some free columns on the right.If some rows of  are linearly dependent, the lower rows of the matrix  will be filled with zeros”\n\n\n(Here  is an  square matrix.)\n\n\n对于rref形态的矩阵，进行一系列的列变换，能够生成，左上子块为单位矩阵，又上子块为 自由矩阵 的矩阵。其中 自由矩阵 为  by  的形状。\n\n\n\nIf  is the nullspace matrix  then . (Here  is an  by  square matrix .) The columns of  are special solutions.\n\n矩阵 中单位矩阵 作为 的填充。矩阵 的各个列向量，为 特解。\nSolvability and Complete solution这一节，原标题为 Solving : row reduced form ，但是我感觉 可解性和解的结构 更到位。\n\nWhen does  have solutions , and how can we describe those solutions?\n\nSolvability conditions on b\nWe again use the example:\n\n\n\nThe 3rd row of  is the sum of its first and second rows, so we know that if  the 3rd components. If   does not satisfy  the system has no solution. If a combination of the rows of  gives the zero row, then the same combination of the entries of  must equal zero.\n\n因为 矩阵 的前两行之和 刚好为第三行。由此可以得知，必须满足  否则就不会有解。\n\nOne way to find out whether  is solvable is to use elimination on the augmented matrix. If a row of  is completely eliminated, so is the corresponding entry in . In our example, row 3 of  is completely eliminated:\n\n\n\nIf  has a solution, then b_3-b_2-b_1=0. For example, we could choose .\n\n\nFrom an earlier lecture, we know that  is solvable exactly when  is in the column space . We have these two conditions on ; In fact they are equivalent.\n\n之前的章节中，我们已经知道， 要有解，那么 向量 必定在 列空间 . 该条件与常数项约束条件等价。 \nComplete solution在这里 note 一下，接下来的内容涉及两种解，英译中是一样的，不好区别，直接在这里标明。\n\nparticular solution: 这个是  的特解。\nspecial solution: 这个是  的解，也是 nullspace 的一个向量。\n\n\nIn order to find all solutions to  we first check that the equation is solvable, then find a particular solution.We get the complete solution of the equation by adding the particular solution to all the vectors in the nullspace.\n\n为了寻求  中的所有解。\n\n检查等式的 可解性。常量是否在 列空间 。\n寻找一个 特解。 \n在 特解 基础上，加上对应 零空间 的所有解。\n\nparticular solution\nOne way to find a particular solution to the equation  is to set all free variables to zero, then solve for the pivot variables.\n\n一种寻找特解的方法： \n\n将所有 自由变量 设置为 0。\n然后求解 主变量。\n\n\nFor our example matrix , we let  to get the system of equations:\n\n\n\nwhich has the solution , . Our particular soution is:\n\n\nCombined with the nullspace\nThe general solution to  is given by , where  is a generic vector in the nullspace. To see this, we add  to  and get  for every vector  in the nullspace.\n\n\nLast lecture we learned that the nullspace of  is the collection of all combinations of the special solutions  and . So the complete solution to the equations  is:\n\n\nRank\nThe rank of a matrix equals the number of pivots of that matrix. If  is an  by  matrix of rank , we know  and \n\n\n矩阵的秩 等于该矩阵的 主元数。\n若矩阵 的形状是  秩为，我们可以知道， &amp;&amp; .\n\nFull column rankThat means \n\nIf , then from the previous lecture we know that the nullspace has dimension  and contains only the zero vector. There are no free variables or special solutions.If  has a solution, it is unique; there is either 0 or 1 solution. Examples like this, in which the columns are independent, are common in applications.We know , so if  the number of columns of the matrix is less than or equal to the number of rows. The row reduced echelon form of the matrix will look like . For any vector  in  that’s not a linear combination of the columns of , there is no solution to .\n\n矩阵阶数等于矩阵列时，可以知道 nullspace 的维度为0 只包含 零向量。没有 自由变量 和  特解。\n如果  有解，那么该解是唯一的。只有 0或1 个解。\nFull row rank\nIf , then the reduced matrix  has no rows of zeros and so there are no requirements for the entries of  to satisfy. The equation  is solvable for every . There are  free variables, so there are  special solutions to \n\n如果是行满秩矩阵 满足, 那么rref形态一定是 。有 个自由变量，有 个  的特解。\nFull row and column rank\nIf  is the number of pivots of , then  is an invertible square matrix and  is the identity matrix. The nullspace has dimension zero, and  has a unique solution for every  in .\n\n行列满秩矩阵，任意都有唯一的解。\nSummaryIf  is in row reduced form with pivot columns first(rref), the table below summarizes our results.\n\n","categories":["Linear Algebra"],"tags":["Courses Notes","Gilbert Strang"]},{"title":"MIT18.06_5_Orthogonal and Gramham-Schmidt","url":"/Linear-Algebra/mit18-06-5-orthogonal-and-gramham-schmidt/","content":"Orthogonal\nWe could say that this is part two of the fundamental theorem of linear algebra. Part one gives the dimensions of the four subspaces, part two says those subspaces come in orthogonal pairs, and part three will be about orthogonal bases for these subspaces.\n\n“我们认为，这是 线性代数基本定理 的 Part 2.“\n\nPart one 给出了四个子空间的维度。\nPart two 给出了子空间间的正交关系。\nPart three 将会是关于这些空间的正交基。\n\nOrthogonal vectors and subspaces\nIn this lecture we learn what it means for vectors, bases and subspaces to be orthogonal. The symbol for this is .  The “big picture” of this course is that the row space of a matrix’ is orthogonal to its nullspace, and its column space is orthogonal to its left nullspace.\n\n\n矩阵的shape是\n\n一个矩阵空间与一个相同维度的零空间正交(orthogonal).\nrow space 和 nullspace 属于.\ncolumn space 和 nullspace of  属于\n\nOrthogonal vectors\nOrthogonal is just another word for perpendicular. Two vectors are orthogonal if the angle between them is 90 degrees. If two vectors are orthogonal, they form a right triangle whose hypotenuse is the sum of the vectors. Thus, we can use the Pythagorean theorem to prove that the dot product  is zero exactly when  and  are orthogonal. (The length squared  equals  equals .)\n\n\n正交 的另一个名字叫做 垂直。\n两个向量正交 means 两个向量间的夹角为 90degrees。\nPythagorean theorem:\n相互垂直的两条边，可以构成一个直角三角形。\n斜边(hypotenuse)，是两个向量之和。\n可以证明点乘：。\n\n一个计算方式 .\n\nNote that all vectors are orthogonal to the zero vector.\n\n一个特例，零向量 对于所有向量全部正交。\nOrthogonal subspaces\nSubspace  is orthogonal to subspace  means: every vector in  is orthogonal to every vector in . \n\n子空间 正交于 子空间: 中的 任意向量 都与  中的 任意向量 垂直。\n\nThe blackboard is not orthognal to the floor; two vectors in the line where the blackboard meets the floor aren’t orthogonal to each other.\n\n特别注意，黑板和地面，并不两个正交的子空间。想象一下，两个平面内都取一个跟交界线夹角45度的射线。这两条射线并不是垂直的。\n\nIn the plane, the space containing only the zero vector and any line through the origin are orthogonal subspaces. A line through the origin and the whole plane are never orthogonal subspaces. Two lines through the origin are orthogonal subspaces if they meet at right angles.\n\n在平面空间中，中间的 正交子空间 仅有 原点 和 过原点的直线。\n\n过原点的直线 和 整个平面 从未正交。\n两条过原点的直线，仅在直线相互垂直的时候，相互正交。\n\nNullspace is perpendicular to row space\nThe row space of a matrix is orthogonal to the nullspace, because  means the dot product of  with each row of  is . But then the product of  with any combination of rows of  must be 0.\n\n结论：矩阵的行空间与矩阵的零空间正交。\n\n 反映了，矩阵的行向量 与  的点乘（内积）必须全部为0。\n 与行空间中的基全部正交。\n那么 与行空间中所有向量正交。\n\n\nThe column space is orthogonal to the left nullspace of  because the row space of  is perpendicular to the nullspace of .\n\n同理的结论： 矩阵的列空间与矩阵的左零空间正交。\n\nIn some sense, the row space and the nullspace of a matrix subdivide  into two perpendicular subspaces. For , the row space has dimension 1 and basis  and the nullspace has dimension 2 and is the plane through the orgin perpendicular to the vector .\n\n一定程度上，行空间 与 零空间 将矩阵分割为两个子空间。\n\n一个三维线性空间 rank=1。\n此时，rowspace的dim=1，子空间为直线。\nnullspace的dim=2，子空间为平面。\n两个子空间相互垂直。\n\n\nNot only is the nullspace orthogonal to the row space, their dimensions add up to the dimension of the whole space.\n\n行空间与零空间维度之和 = 整个空间维度\n\nWe say that the nullspace and the row space are orthogonal complements in 𝕟.\n\n\nThe nullspace contains all the vectors that are perpendicular to the row space, and vice versa.\n\n零空间中，包含所有垂直于行空间的向量，反之亦然。\n\n以下是关于最小二乘，投影相关的内容：\n\nDue to measurement error,  is often unsolvable if . Our next challenge is to find the best possible solution in this case. \n\n由于测量误差(比如在计算卫星轨道之类的事情)， 经常会遇到无法解的情况()。 那么下一个更富挑战性的问题就是，如何从中获得最好的可能解。\n\nThe matrix  plays a key role in this effort: the central equation is .We know that  is square () and symmetric. When is it invertible?Suppose . Then:is invertible.  is not always invertible. In fact:We conclude that  is invertible exactly when  has independent columns.\n\n\n对称方阵在最小二乘的算法中，扮演了核心角色。\n中心等式是 \n\n\n 并不是总 可逆。\n\n 仅当 矩阵的列都相互独立时，可逆。\n\nProjections onto subspacesProjections\nIf we have a vector  and a line determined by a vector , how do we find the point on the line that is closest to ?\n\n\n\n向量、、\n向量是向量在向量上的投影。\n\n\nWe can see from Figure 1 that this closest point  is at the intersection formed by a line through  that is orthogonal to . If we think of  as an approximation of , then the length of  is the error in that approximation.We could try to find  using trigonometry or calculus, but it’s easier to use linear algebra. Since  lies on the line through , we know  for some number . We also know that  is perpendicular to :and . Doubling  doubles . Doubling  does not affect .\n\n通过 投影误差 与 被投影向量 的垂直关系，来得到最后的放缩因子 。\n投影矩阵 \n\n倍增 向量 矩阵也会倍增。\n倍增 向量 矩阵不会倍增。\n\nProjection matrix\nWe’d like to write this projection in terms of a projection matrix .so the matrix is:Note that  is a three by three matrix, not a number; matrix multiplication is not commutative.\n\n\nThe column space of  is spanned by  because for any ,  lies on the line determined by . The rank of  is 1.  is symmetric.  because the projection of a vector already on the line through  is just that vector. In general, projection matrices have the properties:\n\n\n列空间由向量\n对于任意\n 所在的直线，由向量决定\n矩阵的秩为1\n 是 对称 的 \n\n\n\n** 对称性的证明 ** 分母是  是数字(两个向量的内积)。所以矩阵只是的放缩。\nWhy project?\nAs we know, the equation  may have no solution. The vector  is always in the column space of , and  is unlikely to be in the column space. So, we project  onto a vector  in the column space of  and solve .\n\n\n 并不是总有解。\n向量总是在column space当中。但向量不一定在column space中。\n投影是在column space上向量的投影。此时就有最近解 .\n\nProjection in higher dimensions在更高维度的线性空间的投影（在中）\n\nIn , how do we project a vector  onto the closest point  in a plane?If  and  form a basis for the plane, then that plane is the column space of the matrix .\n\n 和  是平面中的一组基，根据垂直条件，投影误差 应该与两组基都垂直。\n\nWe know that . We want to find  There are many ways to show that  is orthogonal to the plane we’re projecting onto, after which we can use the fact that  is perpendicular to  and : \n\n\n\nIn matrix form, . When we were projecting onto a line,  only had one column and so this equation looked like: .Note that  is in the nullspace of  and so is in the left nullspace of . We know that everything in the left nullspace of  is perpendicular to the column space of , so this is another confirmation that our calculations are correct.We can rewrite the equation  as:\n\n将上式与各个基向量垂直等式整合，得到矩阵乘法的表达式，最后变形为：上式是由：  演变而来的。** 比较重要的概念 **含义为：向量 在矩阵的列空间上的投影为 。 预测值为 \n\nWhen projecting onto a line,  was just a number; now it is a square matrix. So instead of dividing by  we now have to multiply by In  dimensions,\n\n当投影在直线(矩阵的列空间dim=1)是一个实数。更高维时，是一个方阵。\n\nIt’s tempting to try to simplify these expressions, but if  isn’t a square matrix we can’t say that  If  does happen to be a square, invertible matrix then its column space is the whole space and contains . In this case  is the identity, as we find when we simplify. It is still true that:\n\n\n若不是方针， 是不成立的。\n若恰好是方针，并且可逆，那么 向量 在矩阵的列空间中。\n\n\n\nLeast Squares\nSuppose we’re given a collection of data points :and we want to find the closest line  to that collection. If the line went through all three points, we’d have:which is equivalent to:\n\n以上的矩阵表达式，从左到右，分别为 、向量 和 向量为需要回归的直线方程，需要敲定、两个参数。\n\nIn our example the line does not go through all three points, so this equation is not solvable. Instead we’ll solve:\n\nProjection matrices and least squaresProjections\nLast lecture, we learned that  is the matrix that projects a vector  onto the space spanned by the columns of . If  is perpendicular to the column space, then it’s in the left nullspace  of  and . If  is in the column space then  for some , and .\n\n\n向量与列空间垂直情况下，垂直列空间于原点 （列空间过原点）。\n向量在列空间上时，投影矩阵。\n\n** 这里有一个非常重要的点 **\n, 并不代表是单位矩阵。比如，正好与特征向量垂直时。 仅仅与矩阵有关，与向量无关。\n\nA typical vector will have a component  in the column space and a component  perpendicular to the column space (in the left nullspace); its projection is just the component in the column space.The matrix projecting  onto  is :Naturally,  has all the properties of a projection matrix.\n\n** 可以理解为 误差矩阵 (我取的名字) 具备 投影矩阵 的全部属性（可以相互确定） **\nLeast squares\n\nWe want to find the closest line  to the points , and . The process we’re going to use is called linear regression; this technique is most useful if none of the data points are outliers.\n\n\n这个处理过程叫做： 线性回归；\n有效条件： 没有离群值；\n\n\nBy “closest” line we mean one that minimizes the error represented by the distance from the points to the line. We measure that error by adding up the squares of these distances. In other words, we want to minimize .\n\n为衡量最近采样点到直线的误差，采用了 距离平方： \n\nIf the line went through all three points, we’d have”but this system is unsolvable. It’s equivalent to , where:\n\n\nThere are two ways of viewing this. In the space of the line we’re trying to find,  and  are the vertical distances from the data points to the line. The components  and  are the values of  near each data point; .In the other view we have a vector  in , its projection  onto the column space of , and its projection  onto .\n\n对于以上的乘法矩阵，我们有两个视角来理解：\n\n尝试寻找 最小误差组, 其中估计值(投影点)组满足, .\n向量在矩阵的column space的投影在列空间中，投影在左零空间上。\n\n\n\nWe will now find  and . We know:\n\n\nFrom this we get the normal equations:We solve these to find  and .\n\n** 这里的 误差向量  是 回归值 与 ground true 之间的垂直距离。\n\nWe could also have used calculus to find the minimum of the following function of two variables”Either way, we end up solving a system of linear equations to find that the closest line to our points is .This gives us:or  and . Note that  and  are orthogonal, and also that  is perpendicular to the columns of .\n\nThe matrix \nWe’ve been assuming that the matrix  is invertible. Is this justified?\n\n在先前，我们都假设方程是可逆的，在这里要证明其合理性。\n\nIf  has independent columns, then  is invertible.\n\n结论： 若矩阵中的各个列独立，那么可逆。\n\nTo prove this we assume that , then show that it must be true that  :Since  has independent columns, .\n\n假设 ，当满足 矩阵 的各个列向量全部都 独立 时，能够唯一确定  那么就能证明  成立。\n\nAs long as the columns of  are independent, we can use linear regression to find approximate solutions to unsolvable systems of linear equations.The columns of  are guaranteed to be independent if they are orthonormal, i.e. if they are perpendicular unit vectors like ， and , or like  and .\n\nOrthogonal matrices and Gram-Schmidt\nIn this lecture we finish introducing orthogonality.Using an orthonormal basis or a matrix with orthonormal columns makes calculations much easier.Gram-Schmidt process starts with any basis and produces an orthonormal basis that spans the same space as the original basis.\n\n\n上一节我们完成了对 正交 的介绍。\n使用 正交基 或者 具有正交列的矩阵，能够让计算变得更容易。\n格拉姆-施密特 处理，从任意一组基出发，生成一组正交的向量基。\n\nOrthonormal vectors\nThe vectors  are orthonormal if:\n\n\n\nIn other words, they all have(normal) length 1 and are perpendicular(ortho) to each other. Orthonormal vectors are always independent.\n\n对于一组向量集：\n\n同一向量点乘为1：;\n不同向量点乘为0：;\n\nOrthonormal matrix\nIf the columns of  are orthonormal, then  is the identity.Matrices with orthonormal columns are a new class of important matrices to add to those on our list: triangular, diagonal, permutation, symmetric, reduced row echelon, and projection matrices. We’ll call them “orthonormal matrices“.A square orthonormal matrix  is called an orthogonal matrix. If  is square, the  tells us that .For example, if  then . Both  and  are orthogonal matrices, and their product is the identity.\n\n\n矩阵以向量集形式表示。\n 只有主对角线上的元素为1。\n由上式可以得到 。\n\n\nThe matrix  is orthogonal. The matrix  is not, but we can adjust that matrix to get the orthogonal matrix . We can use the same tactic to find some larger orthogonal matrices called Hadamard matrices:\n\n\nAn example of a tectangular matrix with orthonormal columns is:\n\n\nWe can extend this to a (square) orthogonal matrix:These examples are particularly nice because they don’t include complicated square roots.\n\n以上的例子都很不错，因为所有元素都是整数。\nOrthonormal columns are good\nSuppose  has orthonormal columns. The matrix that projects onto the column space of  is:If the columns of  are orthonrmal, then  and . If  is square, then  because the columns of  span the entire space.Many equations become trivial when using a matrix with orthonormal columns. If our basis is orthonormal, the projection component  is just  because  becomes .\n\n投影到矩阵的列空间的的投影矩阵为: 若具有 正交性，, 此时。又因为。最后得出结论。\n许多矩阵运算再考虑正交性后，都可以变得更加简单易解。\nGram-Schmidt\nWith elimination, our goal was “make the matrix triangular”. Now our goal is “make the matrix orthonormal”.\n\n在使用消元法时，我们的目的是： 使得矩阵 三角化。如今的目标是，使得矩阵 正交化。\n\nWe start with two independent vectors  and  and want to find orthonormal vectors  and  that span the same plane.We start by finding orthogonal vectors  and  that span the same space as  and .Then the unit vectors  and  form the desired orthonormal basis.\n\n\n我们从两个独立向量和出发。\n透过投影的处理，获得正交向量基和。\n一个比较重要的特性： 和 与 和 具有相同的张成空间。\n\n\nLet . We get a vector orthogonal to  in the space spanned by  and  by projecting  onto  and letting . ( is what we previously called .)If we multiply both sides of this equation by , we see that .\n\n\n假设，设为第一个基的向量。\n向量减去向量在上的投影。最终结果可以用 来验证。\n\n** 注意： 是一个实数 **代表：向量在向量/矩阵上投影的缩放因子\n\nWhat if we had started with three independent vectors, ,  and ? Then we’d find a vector  orthogonal to both  and  by subtracting from  its components in the  and  directions:\n\n如果由三个向量 获得第三个向量，仅需要减去两个正交向量的投影。\n\nFor example, suppose  and . Then  and:Normalizing, we get:The column space of  is the plane spanned by  and .\n\n获得向量后，想要获得单位矩阵的方式：\n\nWhen we studied elimination, we wrote the process in terms of matrices and found .  similar equation  relates our starting matrix  to the result  of the Gram-Schmidt process. Where  was lower triangular,  is upper triangular.Suppose . Then:\n\n从假设到  关系的推倒，利用了假设式子与其它向量的正交关系。** 感谢 Hengchuan zou 提供的证明（我看了好久 **\n我们像研究 消元法 时一样，将消元法用矩阵运算分析理解。像一样，我们把原独立列矩阵分解为 正交单位矩阵和关系矩阵。\n\nIf  is upper triangular, then it should be true that . This must be true because we chose  to be a unit vector in the direction of . All the later  were chosen to be perpendicular to the earlier ones.Notice that . This makes sense; .\n\n","categories":["Linear Algebra"],"tags":["Courses Notes","Gilbert Strang"]},{"title":"MIT18.06_4_Four Fundamental Subspaces","url":"/Linear-Algebra/mit18-06-4-four-fundamental-subspaces/","content":"Independence, basis, and dimension\nWhat does it mean for vectors to be independent? How does the idea of independence help us describe subspaces like the nullspace?\n\n这一节，涉及 独立性， 或者说 线性无关（独立） 的定义。 有助于帮我吗理解，像子空间这类的 零空间。\nLinear independence\nSuppose  is an  by  matrix with  (so  has more unknowns than equations).  has at least one free variable, so there are nozero solutions to . A combination of the columns is zero, so the columns of this  are dependent.\n\n假设 矩阵 的形状是  by ，且满足 (more unknowns than equations). 矩阵 至少有一个 自由变量。\n\nWe say vectors  are linearly independent (or just independent) if  only when  are all 0. When those vectors are the columns of , the only solution to  is .\n\n当一系列向量  线性组合为0 的唯一系数组为 ，则这一组向量组 线性无关。\n\nTwo vectors are independent if they do not lie on the same line. Three vectors are independent if they do not lie in the same plane. Thinking of  as a linear combination of the column vectors of , we see that the column vectors of  are independent exactly when the nullspace of  contains only the zero vector.\n\n\n两个向量 线性独立： 不在一条线上。\n三个向量 线性独立： 不在一个平面。\n 视为 矩阵 列向量的线性组合。\n矩阵的列向量组，独立，那么的零空间只包含 零向量。\n\n\nIf the columns of  are independent then all columns are pivot columns, the rank of  is , and there are no free variables. If the columns of  are dependent then the rank of  is less than  and there are free variables.\n\n\n若矩阵的列向量独立，那么所有列都为 主元列。\n矩阵的秩为，那么没有 自由变量。\n矩阵列向量 线性不独立：矩阵秩rank 小于，存在 自由向量。\n\nSpanning a space张成空间\n\nVectors  span a space when the space consists of all combinations of those vectors. For example, the column vectors of  span the column space of .\n\n向量组  张成(span) 一个由这些所有向量的排列组合组成的空间。 例如，矩阵 的列向量张成  的列空间。\n\nIf vectors  span a space , then  is the smallest space containing those vectors.\n\n如果向量组  张成空间为，那么应该就是包含这些向量组的最小空间。\nBasis and dimension基与维度\n\nA basis for a vector space is a sequence of vectors  with two properties:\n\n\n are independent\n span the vector space.\n\n线性空间的 基 是有以下两个中性质的一系列 向量：\n\n各个向量间相互独立。\n各个向量张成了这个空间。\n\n\nThe basis of a space tells us everything we need to know about that space.\n\n基蕴含了一个向量空间的全部信息(?)。大概是，唯一确定一组基，那么就能唯一确定一个向量空间。\nExample: \nOne basis for  is {}\n\nThese are independent becasuse:\n\nis only possible when . These vectors span .\n\nAs discussed at the start of Lecture 10, the vectors ,  and  do not form a basis for  because these are the column vectors of a matrix that has two identical rows. The three vectors are not linearly independent.\n\n上面三个向量并不是相互独立的（我像个傻子一样看了挺久的）。三个向量组成 3by3 的矩阵，然后计算矩阵的秩，就知道结果了。\n\nIn general,  vectors in  form a basis if they are the column vectors of an invertible matrix.\n\nBasis for a subspace\nThe vectors  and  span a plane in  but they cannot form a basis for . Given a space, every basis for that space has the same number of vectors; that number is the dimension of the space. So there are exactly  vectors in every basis for .\n\n\n上面的两个向量能够在 三维空间 生成一个平面，但是他们（两个向量）并不是  的基。\n给定空间，每个基（可以有很多基）都有相同数量的向量。这个数量 = dimension of the space.\n\nBase of a column space and nullspaceSuppose:\n\n\nBy definition, the four column vectors of  span the column space of . The third and fourth column vectors are dependent on the first and second, and the first two columns are independent. Therefore, the first two column vectors are the pivot columns. They form a basis for the column space . The matrix has rank 2. In fact, for any matrix  we can say:\n\n\n(Note that matrices have a rank but not a dimension. Subspaces have a dimension but not a rank.)\nNOTE: 矩阵有秩而无维度之说，子空间有维度而无秩一说。\n\nThe column vectors of this  are not independent, so the nullspace  contains more than just the zero vector. Because the third column is the sum of the first two, we know that the vector  is in the nullspace. Similarly,  is also in . These are the two solutions to . We’ll see that:so we know that the dimension of  is . These two special solutions form a basis for the nullspace.\n\nrecitation\n四个1by5的向量，寻找向量组的基。通过消元法，可以得到最终结果。\n\n最后消元结果的列向量，与原矩阵组的对应列向量，张成空间相同。\n\n但是，如果消元过程中，遇到 行列变换、非向量间(这个是我编的)，某某维度间消除.\n\n最后剩下的向量张成空间维度会与先前相同，但是不是同一个空间。\n\nFour Fundamental Subspaces\nIn this lecture we discusss the four fundamental spaces associated with a matrix and the relations between them.\n\n\nThis is really the heart of this approach to linear algebra. To see these four subspaces, how they’re related.\n\n四个基本矩阵的相互关系，是线性代数的核心。\n\nThe main question: how to produce a basis? What’s the dimension?\nFour subspacesAny  by  matrix  determines four subspaces(possibly containing only the zero vector):\nColumn space, consists of all combinations of the columns of  and is a vector space in .\nbasis:\nNullspace,This consists of all solutions  of the equation  and lies in .\nRow space,\nThe combinations of the row vectors of  form a subspace of . We equate this with , the column space of the transpose of .\n\n\nLeft nullspace, \nWe call the nullsapce of  the left nullspace of . This is a subspace of .\n\nBasis and DimensionColumn space\nThe  pivot columns form a basis for \n\n列空间的basis是 主列pivot cols.\nNullspace\nThe special solutions to  correspond to free variables and form a basis for . An  by  matrix has  free variables:\n\nNullspace 的基basis为 special solutions。\nRow space\nWe could perform row reduction on , but instead we make use of , the row reduced echelon form of .\n\n\n我们可以对  使用 row reduction\n也可以生成矩阵的rref矩阵\n\n\n\nAlthough the column spaces of  and  are different, the row space of  is the same as the row space of . The rows of  are combinations of the rows of , and because reduction is reversible the rows of  are combinations of the rows of R.\n\n\n尽管 与 对列空间有本质不同，的行空间和的行空间是相同的。\n的行向量是行向量的线性组合。\n\n\nThe first  rows of  are the “echelon” basis for the row space of :\n\nLeft nullspace\nThe matrix  has  columns. We just saw that  is the rank of , so the number of free columns of  must be :\n\nLeft nullspace 是的零矩阵，的rank为可以推断，左零的秩为。\n\nThe left nullspace is the collection of vectors  for which . Equivalently, ; here  and  are row vectors. We say “left nullspace” because  is on the left of  in this equation.\n\n左零空间的等式为  可以等效表达为 ; 等式的解就在矩阵的左侧了，因此称为left nullspace。\n\nTo find a basis for the left nullspace we reduce and augmented version of :\n\n\n为了寻求 left nullspace 的基，我们对 增广相同几何尺寸的，再求rref形式。\n\nFrom this we get the matrix  for which . (If  is a square, invertible matrix then .) In our example,\n\n\n\nThe bottom  rows of  describe linear dependencies of rows of , because the bottom  rows of  are zero. Here  (one zero row in ).\n\nrref矩阵  的上  是含有主元元素的列，下  行则是全为0的列。\n\nThe bottom  rows of  satisfy the equation  and form a basis for the left nullspace of .\n\n全为0的行对应的， 中的行，即为 的special solution.原因： 矩阵 中的某行，可以视为 矩阵 中的对应行 和矩阵 的乘积。\n补充一个点\n\nrow space 和 nullspace 都属于  \n行空间的向量维度为。\nnullspace 是的解空间，等于矩阵的行维。\n\n\ncolumn space 和 leftspace 同理属于 \n\nMatrix space\n讲稿上本来写的是 New vector space，但是私以为 Matrix space 更为合适\n这种观点是 “向量空间向矩阵的一种衍生（推广、外延）”\n理由：所有向量空间的运算性质，都能推广到矩阵空间\n\n\nThe collection of all  matrices forms a vector space; call it . We can add matrices and multiply them by scalars and there’s a zero matrix(additive identity). If we ignore the fact that we can multiply matrices by each other, they behave just like vectors.\n\n所有shape为 3by3 的矩阵（视为向量）张成空间，命名为。满足线性运算。如果不管（无视）矩阵乘法，就可以当作代数符号或者向量一样运算。\n\nSome subspaces of  include:\n\n\nall upper triangular matrices\nall symmetric matrices\nD, all diagonal matrices\n\n列举了一些  的子空间。\n\n is the intersection of the first two spaces. Its dimension is 3; one basis for  is:\n\n\nPro. Gilbert 举的一个例子：能够张成 对角矩阵 的一组 basis 基。\nMatrix spaces; rank 1; small world graphs\nWe’ve talked a lot about , but we can think about vector spaces made up of any sort of “vectors” that allow addition and scalar multiplication.\n\n书接上回，对于满足 线性运算 条件的运算对象，都可以引入 “线性空间” 概念。\nNew vector spaces3 by 3 matrices\nWe were looking at space  of all 3 by 3 matrices. We identified some subspaces; the symmetric 3 by 3 matrices , the upper triangular 3 by 3 matrices , and the intersection  of these two spaces - the space of diagonal 3 by 3 matrices.\n\n简单的三个例子： 对称矩阵、上三角矩阵、两者的交集 - 相同shape的主对角矩阵。\n\nThe dimension of  is 9; we must choose 9 numbers to specify an element of . The space  is very similar to . A good choice of basis is:\n\n**  和  的区别 **\n\n前者的元素为一个维度为9的向量。\n但是后者的元素为一个shape为的矩阵。\n\n\n以上为，生成 矩阵的最简单一组基。\n\nThe subspace of symmetric matrices  has dimension 6. When choosing an element of  we pick three numbers on the diagonal and three in the upper right, which tell us what must appear in the lower left of the matrix. One basis for  is the collection:\n\n以上为 空间 的子空间 3阶对称矩阵的 最简单的基。\n\nThe dimension of  is again 6; we have the same amout of freedom in selecting the entries of an upper right, which tell us what must appear in the lower left of the matrix. One basis for  is:\n\n上三角矩阵基。\n\nThis happens to be a subset of the basis we chose for , but there is no basis for  that is a subset of the basis we chose for .\n\n\n上三角矩阵空间恰好是 矩阵空间 的子空间。对称矩阵空间也是。\n但是 与 并不是属于关系。\n\n\nThe subspace  of diagonal 3 by 3 matrices has dimension 3. Because of the way we chose bases for  and , a good basis for  is the intersection of those bases.\n\n两个子空间的交集也是子空间。&gt; We only live so long, so skip the prove it.\n\nIs , the set of 3 by 3 matrices which are either symmetric or upper triangular, a subspace of ? No. This is like taking two lines in  and asking if together they form a subspace; we have to fill in between them. If we take all possible sums of elements of  and elements of  we get what we call the sum . This is a subsapce of . In fact, . For unions and sums, dimensions follow this rule:\n\n\n** 在探明，对称矩阵空间 与 上三角矩阵空间 两者的并集的时候，就跟研究两个直线集组成的线性空间一样，要填充两个集合各取一个元素进行线性运算的情形 **\n我们将所有可能 和的元素之和作集，记为 the sum . 这是的一个子空间（不严格属于，）。\nDifferential equations\nAnother example of a vector space that’s not  appears in differential equations.\n\n另一个 不能用 来表征，那就是 差分方程。\n我们思考差分方程  的 解空间 作为一个零空间的元素，解为：\n\nWe can think of the solutions  to  as the elements of a nullspace. Some solutions are:\n\n\nThe complete solution is:where  and  can be any complex numbers. This solution space is a two dimensional vector space with basis vectors  and . (Even though these don’t “look like” vectors, we can build a vector space from them because they can be added and multiplied by a constant.)\n\n最后得到的结果为：，最后的解空间为 由和像个基向量组成的。\n** 这里一个比较核心的观点 万物皆向量 **\nRank one matrices\nThe rank of a matrix is the dimension of its column(or row) space. The matrixhas rank 1 because each of its columns is a multiple of the first column.\n\n\n\nEvery rank 1 matrix  can be written , where  and  are column vectors. We’ll use rank 1 matrices as building blocks for more complex matrices.\n\nrank1矩阵 有一些比较特殊的性质：\n\n可以表示为一个列向量（也是basis）\n和scale放大因子构成的横向量\n最后就会写成  这里的 是basis，而  则为由scale的影响因子。\n\n\n此外，对于一个shape为的 空间 有一个的子空间。\n那么这时，只需要4个rank=1的矩阵，就能构造\n因此，rank1矩阵，在矩阵空间内，就像能够搭建起他矩阵的积木一样。\n\n\n\nRank 4 matrices\nNow let  be the space of  matrices. The subset of  containing all rank 4 matrices is not a subspace, even if we include the zero matrix, because the sum of two rank 4 matrices may not have rank 4.\n\n现将矩阵 设为  的矩阵。那么 的子集，包含所有 rank4矩阵 的空间，不是的子空间。\n因为，两个rank4的矩阵和 不一定rank4。\n\nIn , the set of all vectors  for which  is a subspace. It contains the zero vector and is closed under addition and scalar multiplication. It is the nullspace of the matrix . Because  has rank 1, the dimension of this nullspace is . The subspace has the basis of special solutions:\n\n\n\nThe column space of  is . The left nullspace contains only the zero vector, has dimension zero, and its basis is the empty set. The row space of  also has dimension 1.\n\n这里感觉没啥，好讲的，都是上一节的内容。\nSmall world graphs\nThis last topic of small world graphs, and leads into, a lecture about graphs and linear algebra.\n\n这里介绍一种特殊的图–“小世界图”，来引出，线性代数 与 图论 之间的关系。\ngraph\n** what’s a graph? **In this class, a graph  is a collection of nodes joined by edges:\n\n\n图是 节点 和 边 的集合，边连通各个节点。\nA typical graph appears in Figure 1. Another example of a graph is one in \n\n\nwhich each node is a person. Two nodes are connected by an edge if the people are friends. We can ask how close two people are to each other in the graph - what’s the smallest number of friend to friend connections joining them? The question “what’s the farthest distance between two people in the graph?” lies behind phrases like “six degrees of separation” and “it’s a small world”.\n\n\n”小世界” 模型中，每个节点代表一个社会人，边沿代表社会连结。\n最远的“社会距离”？ 6 degree.\n给人“这个世界真小啊”的感慨，所以叫“小世界”模型。\n\n\n\n\nAnother graph is the world wide web: its nodes are web sites and its edges are links.We’ll describe graphs in terms of matrices, which will make it easy to answer questions about distances between nodes.\n\n另一个典型的图是：万维网(www, world wide web)我们将会以矩阵的形式来表征 图，方便表征各个节点之间的距离。\nGraphs, networks, incidence matrices\nWhen we use linear algebra to understand physical systems, we often find more structure in the matrices and vectors than appears in the examples we make up in class. There are many applications of linear algebra; for example, chemists might use row reduction to get a clearer picture of what elements go into a complicatied reaction. In this lecture we explore the linear algebra associated with electrical networks.\n\n\n当我们使用 线性几何 来理解物理系统的时候，我们常常会发现比课堂例子中更复杂的矩阵/向量结构。\n此外还有很多应用的例子:\n化学家 使用 row reduction 来更简单地表述 化学元素在复杂反应中的 图。\n这一节中，我们结合 线性代数 与 电路网络 系统。\n\n\n\nGraphs and networksA graph is a collection of nodes joined by edges; Figure 1 shows one small graph.\n\nWe put an arrow on each edge to indicate the positive direction for currents running through the graph.\n\nIncidence matricesIncidence matrices: 关联矩阵\n\nThe incidence matrix of this directed graph has one column for each node of the graph and one row for each edge of the graph:\n\n有向图的 关联矩阵，每一列对应图的一个节点，每一行对应图的 边:\n\n\nIf an edge runs from node  to node , the row corresponding to that edge has -1 in column  and 1 in column ; all other entries in that row are 0. If we were studying a larger graph we would get a larger matrix but it would be sparse; most of the entries in that matrix would be 0. This is one of the ways matrices arising from applications might have extra structure.\n\n\n如果有一条边，从节点指向节点\n会先对各个边进行编码（这个顺序就是行数。\n对应边列元素值为-1，列元素值为1。\n\n\n当我们在研究更大的图时，矩阵会变得更稀疏。\n因为每行之有两个元素为非零元素。那么存在大量的零。\n每行对应入度和出度元素。\n所有行向量之和为0（全图所有节点，出入度中和）\n\n\n\n\nNote that nodes 1, 2 and 3 and edges1,2,3 form a loop. The matrix describing just those nodes and edges looks like:\n\n\n案例图：\n\n三个节点 a,b,c\n三条有向边 1,2,3以上 节点 与 边 组成的图形成一个loop,指向和，指向。\n\n\nNote that the third row is the sum of the first two rows; loops in the graph correspond to lienarly dependent rows of the matrix.\n\n\n矩阵的前两行之和等于第三行。\nloop 在图中与线性相关等效。\n\nTo find the nullspace of , we solve :\n\nIf the components  of the vector  describe the electrical potential at the nodes  of the graph, then  is a vector describing the different in potential at the nodes i of the graph, then  is a vector describing the difference in potential across each edge of the graph. \n\n\n用 向量来描述 节点 的电势，那么 就能描述图中各个 边 的电势差。\n\n\nWe see  when , so the nullspace has dimension 1. In terms of electricity through the network, but if one node of the network is grounded then its potential is zero.From that we can determine the potential of all other nodes of the graph.\n\nelectrical potential: 电势\n\nThe matrix has 4 columns and a 1 dimensional nullspace, so its rank is 3. The first, second and fourth columns are its pivot columns; these edges connect all the nodes of the graph without forming a loop = a graph with no loops is called a tree\n\ntree 树：边连结所有节点，但是没有形成loop的图。\n\nThe left nullspace of  consists of the solutions  to the equation: .Since  has 5 columns and rank 3 we know that the dimension of  is .Note that 2 is the number of loops in the graph and  is the number of edges. \n\n值得注意的是，left nullsapce的秩是2，同时也是loop的数量。\n\nThe rank  is , one less than the nubmer of nodes. This gives us #loops = #edges - (#nodes-1),or:This is Euler’s formula for connected graphs.\n\n\n矩阵 的秩 ，比结点数少1（刚好就是树的边数.\nnumber of loops =  = 边树 - （节点数-1）\n\n上式子是连结图的 欧拉方程。\nKirchhoff’s law\nIn our example of an electrical network, we started with the potentials  of the nodes. The matrix  then told us something about potential differences. \n\n上例解释了，矩阵可以反映电势差的相关信息。\n\nAn engineer could create a matrix  using Ohm’s law and information about the conductance of the edges and use the matrix to determine the current  on each edge. \n\n工程师可以利用 欧姆定律 构造矩阵.图的拓扑结构，能够决定各个边的电导。\n\nKirchhoff’s Current Law then says that , where  is the vector with components . Vectors in the nullspace of  correspond to collections of currents that satisfy Kirchhoff’s law.\n\n基尔霍夫电流定律 表示。在  零空间中的向量与满足基尔霍夫定律的 电流集 相关。\n\nWritten out,  looks like:\n\n\nMultipilying the first row by the column cector  we get . This tells us that the total current flowing out of node 1 is zero - it’s a balance equation, or a conservation law.\n\n\n\n第一行的乘法，可以得到 。 这可以反映出，所有流出 节点1 点电流为0。满足 平衡方程 or 守恒方程。\n\n\n\nMultiplying the second row by  tells us ; the current coming into node 2 is balanced with the current going out.\n\n\n\n第二行的乘法，可知 。 第二节点电流的 入度=出度。\n\n\n\nMultiplying the bottom rows, we get  and .\n\n\n\nWe could use the method of elimination on  to find its column space, but we already know the rank. \n\n我们可以使用 消去法 来寻得 矩阵 的列向量空间。但是我们已经知道 矩阵 的秩了。那么可以猜出两个 special solutions。\n\nTo get a basis for  we just need to find two independent vectors in this space. Looking at the equations  we might guess .Then we could use the conservation laws for node 3 to guess  and . We satisfy the conservation conditions on node 4 with , giving us a basis vector . \n\n\nThis vector represents one unit of current flowing around the loop joining nodes 1, 2 and 3; a multiple of this vector represents a different amount of current around the same loop.\n\n\nWe find a second basis vector for  by looking at the loop formed by nodes 1,3 and 4: . The vector  that represents a current around the outer loop is also in the nullspace, but it is the sum of the first two vector we found.\n\n\nWe’ve almost completely covered the mathematics of simple circuits. More complex circuits might have batteries in the edges, or current sources between nodes. Adding current sources changes the  in Kirchhoff’s current law to . Combining the equations  and  gives us :\n\n我们大致介绍了，应用数学中，对电流的应用：\n\n之前的 基尔霍夫电流方程 都没有将 电源 纳入考虑，所以引入常数项 \n从电势差公式，到欧姆定律，再到电流定律。逐级迭代获得最终结果： \n\n","categories":["Linear Algebra"],"tags":["Courses Notes","Gilbert Strang"]},{"title":"MIT18.06_8_","url":"/Linear-Algebra/mit18-06-8/","content":"Symmetric matrices and positive definiteness\nSymmetric matrices are good - their eigenvalues are real and each has a complete set of orthonormal eigenvectors. Positive definite matrices are even better.\n\n对称矩阵的性质很不错–它们的特征向量是 实值 的 完备正交特征向量集合正定矩阵，具有更加优秀的性质\nSymmetric matrices\nA symmetric matrix is one for which .If a matrix has some special property (e.g. it’s a Markov matrix), its eigenvalues and eigenvectors are likely to have special properties as well.\n\n如果矩阵有一些特殊的性质（比如说 马尔可夫矩阵），它的 特征值 和 特征向量 更可能拥有特别性质\n\nFor a symmetric matrix with real number entries, the eigenvalues are real numbers and it’s possible to choose a complete set of eigenvectors that are perpendicular (or even orthonormal).\n\n对于实值对称矩阵，特征值是实值，且有可能性选择一组相互正交的特征向量。\n\nIf  has  independent eigenvector we can write . If  is symmetric we can write , where  is an orthogonal matrix. Mathematicians call this the spectral theorem and think of the eigenvalues as the “spectrum” and think of the eigenvalues as the “spectrum” of the matrix. In mechanics it’s called the principal axis theorem.\n\n\nIn addition, any matrix of the form  will be symmetric.\n\nReal eigenvalues\nWhy are the eigenvalues of a symmetric matrix real? Suppose  is symmetric and . Then we can conjugate to get . If the entries of  are real, this becomes . (This proves that complex eigenvalues of real valued matrices come in conjugate pairs.)\n\n\nNow transpose to get . Because  is symmetric we now have . Multiplying both sides of this equation on the right by  gives:On the other hand, we can multiply  on the left by  to get:Comparing the two equations we see that  and, unless  is zero, we can conclude  is real.\n\n\nHow do we know  ?If  then .\n\n\nWith complex vectors, as with complex numbers, multiplying by the conjugate is often helpful.\nSymmetric matrices with real entries, then it will have real eigenvalues and perpendicular eigenvectors if an only if . (The proof of this follows the same pattern.)\n\nProjection onto eigenvectors\nIf , we can write:\n\nThe matrix  is the projection matrix onto , so every symmetric matrix is a combination of perpendicular projection matrices.\nInformation about eigenvalues\nIf we know that eigenvalues are real, we can ask whether they are positive or negative. (Remember that the signs of the eigenvalues are important in solving systems of differential equations.)For very large matices , it’s impractical to compute eigenvalues by solving . However, it’s not hard to compute the pivots, and the signs of the pivots of a symmetric matrix are the same as the signs of the eigenvalues:Because the eigenvalues of  are just  more than the eigenvalues of , we can use this fact to find which eigenvalues of a symmetric matrix are greater or less than any real number . This tells us a lot about the eigenvalues of  even if we can;t compute them directly.\n\nPositive definite matrices\nA positive definite matrix is a symmetric matrix  for which all eigenvalues are positive. A good way to tell if a matrix is positive definite is to check that all its pivots are positive.\n\n\nLet . The pivots of this matrix are 5 and . The matrix is symmetric and its pivots (and therefore eigenvalues) are positive, so  is a positive definite matrix. Its eigenvalues are the solutions to:The determinant of a positive definite matrix is always positive but the determinant of  is also positive, and that matrix isn’t positive definite. If all of the subdeterminants of  are positive (determinants of the  by  matrices in the upper left corner of , where ), then  is positive definite.\n\n\nThe subject of positive definite matrices brings together what we’ve learned about pivots, determinants and eigenvalues of square matrices. Soon we’ll have a chance to bring together what we’ve learned in this course and apply it to non-square matrices.\n\n","categories":["Linear Algebra"],"tags":["Courses Notes","Gilbert Strang"]},{"title":"MIT18.06_7_Application of eigen","url":"/Linear-Algebra/mit18-06-7-application-of-eigen/","content":"Diagonalization and powers of \nWe know how to find eigenvalues and eigenvectors. In this lecture we learn to diagonalize any matrix that has  independent eigenvectors and see how diagonalization simplifies calculaions. The lecture concludes by using eigenvalues and eigenvectors to solve difference equations.\n\n我们已经了解，如何找到 特征值 和 特征向量 了。在这一节中，我们学会 对角化 任意包含  个独立特征向量  的矩阵，以及利用对角化来 简化计算。\nDigagonalizing a matrix对角化矩阵 \n\nIf  has  linearly independent eigenvectors, we can put those vectors in the columns of a (square, invertible) matrix . Then\n\n对角化的一个重要前提，矩阵的个 特征向量 线性独立(这样 矩阵 可逆。这些 特征向量 组成 特征向量矩阵 。\n\n上式是 key equation 的代入。\n\n上一步涉及 列变换\n\nNote that  is a diagonal matrix whose non-zero entries are the eigenvalues of . Because the columns of  are independent,  exists and we can multiply both sides of  by :Equivalently, .\n\n这种方法的一个重要前提： 矩阵有个独立的特征向量，即满秩，可逆。\n如果 矩阵的所有特征值都不同，那么 矩阵拥有个独立特征向量（可以实现对角化\nPowers\nWhat are the eigenvalues and eigenvectors of ?\n\n\nIf ,\nthen .\n\n如何求得的特征值和特征向量?\n\nThe eigenvalues of  are the squares of the eigenvalues of .\n\n的特征值是对应的特征值的平方。\n\nThe eigenvectors of  are the same as the eigenvectors of .\n\n的特征向量与的特征向量相同。\n\nIf we write  then:Similarly,  tells us that raising the eigenvalues of  to the  power gives us the eigenvalues of , and that the eigenvectors of  are the same as those of .\n\n矩阵是由 特征向量 组成。 的表达形式，可以看出：\n\n的主对角线元素为特征值的阶。\n的形式反映，是特征向量组成方阵。\n\n\nTheorem: If  has  independent eigenveectors with eigenvalues , then  as  if and only if all \n\n定理： 当时， 当且仅当 所有.\n\n is guaranteed to have  independent eigenvectors (and be diagonalizable) if all its eigenvalues are different. Most matrices do have distinct eigenvalues.\n\n\n仅当矩阵中的所有特征值都不同，才可保证有个独立特征值。\n绝大多数的矩阵都有 不同的特征值。\n\nRepeated eigenvalues\nIf  has repeated eigenvalues, it may or may not have  independent eigenvectors.\n\n若矩阵有重复的特征值，可能但不一定有个独立的特征值。\n\nFor example, the eigenvalues of the identity matrix are all 1, but that matrix still has  independent eigenvectors.If  is the triangular matrix  its eigenvalues are  2 and 2. Its eigenvectors are in the nullspace of  which is spanned by . This particular  does not have two independent eigenvectors.\n\n\n单位矩阵 含个特征向量（也是单位矩阵\n矩阵 就只有一个特征向量。\n\nDifference equations差分方程\n\n\nStart with a given vector . We can create a sequence of vectors in which each new vector is  times the previous vector: .  is a firet order difference equation, and  is a solution to this system.We get a more satisfying solution if we write  as a combinating of eigenvectors of :\n\n\nThen:\n\nand:\n\n这里涉及 特征值，方程上的应用。\nFibonacci sequence斐波那契数列\n\nThe Fibonacci sequence is 0,1,1,2,3,5,8,13,… In general, .\n\n斐波那契的定义\n\nIf we could understand this in terms of matrices, the eigenvalues of the matrices would tell us how fast the numbers in the sequence are increasing.\n\n如果我们能从矩阵形式的角度，理解Fibonacci序列我们将明白Fibonacci增长速度\n\n was a first order system.  is a second order scalar equation, but we can convert it to first order lienar system by using a clever trick. If , then:is equivalent to the first order system .\n\n用矩阵形式表示，斐波那契数列的关系式，就得到 .\n\nWhat are the eigenvalues and eigenvectors of ?Because  is symmetric, its eigenvalues will be real and its eigenvectors will be orthogonal.\n\n矩阵是对称矩阵，故特征值为实数，并且特征向量正交。\n\nBecause  is a two by two matrix we know its eigenvalues sum to 1 (the trace) and their product is -1 (the determinant).\n\n\n\ntwo by two matrix 特征值之和为1 (equal to trace)\n特征值之积 (equal to det A)\n\n\nSetting this to zero we find ; i.e.  and .\n\n可以直接算出特征值。\n\nThe growth rate of the  is controlled by , the only eigenvalue with absolute value greater than 1.\n\n的增长速率仅仅与相关\n\nThis tells us that for large ,  for some constant .(Remember , and here  goes to zero since )\n\n 是对等式： 的分解表达。\n\nTo find the eigenvectors of  note that:\n\n\nequals  when , so  and .\n\n\nFinally,  tells us that . Because , we get:\n\n\n\nUsing eigenvalues and eigenvectors, we have found a closed form expression for the Fibonacci numbers.\n\n实用 特征值 和 特征向量 我们可以求得 Fibonacci数列的 闭合表达式\n\nSummary: When a sequence evolves over time according to the rules of a first order system, the eigenvalues of the matrix of that system determine the long term behavior of the series. To get an exact formula for the series we find the eigenvectors of the matrix and then solve for the coefficients \n\n当一个序列按照一阶系统的规则随时间演化时，该系统矩阵的特征值决定了该序列的长期行为。\n为了得到这个级数的精确公式，我们先找到矩阵的特征向量，然后解出系数\nDifferential equations and \nThe system of equations below describes how the values of variables  and  affect each other over time:\n\n\n上面等式系统，描述了变量  和  与他们关于时间的相互影响。\n\nJust as we applied linear algebra to solve a difference equation, we can use it to solve this differential equation.For example, the intial condition ,  can be written 。\n\n就如我们应用线性代数解决差分方程一样。我们利用线性代数来解决微分方程问题。这里举一个例子，向量的初始值为为这个向量的初始值。\nDifferential equations \nBy looking at the equations above, we might guess that over times  will decrease.\n\n通过观察以上等式，我们也许可以猜测，随着时间发展  会衰减。\n\nWe can get the same sort of information more safely by looking at the eigenvalues of the matrix  of our system . \n\n我们可以通过观察特征值矩阵来更保险地了解到这相同的信息。\n\nBecause  is singular and its trace is -3 we know that its eigenvalues are  and .The solution will turn out to include  and . As  increases,  vanishes and  remains constant. \n\n矩阵的trace为-3，我们可以计算得到  和 。解的组成包含元素和。随着t的增大，趋近于零，是零。\n\nEigenvalues equal to zero have eigenvectors that are steady state solutions.\n\n特征值为零，对应的特征向量是 稳定解。\n\n is an eigenvector for which .To find an eigenvector corresponding to  we solve :and we can check that . The general solution to this system of differential equations will be:\n\n对于微分方程解来说，特征值分量的形式为： 可以利用 key equation 来得到闭合解。\n\nIs  really a solution to  ? To find out, plug in :which agrees with:The two “pure” terms  and  are analogous to the terms  we saw in the solution  to the difference equation .\n\n对于阶系统，我们依然可以用 。\n\nPlugging in the values of the eigenvectors, we get:We know , so at : and .\n\n在确定 特征值 和 特征向量 后，通解形式只需要确定  和  就能待定通解。当时，可以待定得到最后的通解。\n\nThis tells us that the system starts with  and  but that as  approaches infinity,  decays to 2/3 and  increases to 1/3. This might describe stuff moving from  to .The steady state of this system is .\n\n我们可以知道，当，向量.\nStability\nNot all systems have a steady state. The eigenvalues of  will tell us what sort of solutions to expert:\n\nStability:  when .\nSteady state: One eigenvalue is 0 and all other eigenvalues have negative real part.\nBlow up: if  for any eigenvalue .If a two by two matrix  has two eigenvalues with negative real part, its trace  is negative. The converse is not true: has a positive determinant and negative trace then the corresponding solutions must be stable.\n\n\n不是所有系统，都有 稳定解 的。特征值会反映解的收敛情况：\n\n稳定性:  when .\n稳定态：一个特征值为0，其余都小于零\n发散： 存在 \n\nApplying \nThe final step of our solution to the system  was to solve:In matrix form:or , where  is the eigenvector matrix. The components of  determine the contribution from each pure exponential solution, based on the initial conditions of the system.\n\n在用 特征矩阵 表征时，会涉及到 矩阵 的线性组合来表示初始值 ，有表达式, 的组成元素（components）反映了对应 纯指数解(pure expoential solution)，与系统的 初化值 有关。\n\nIn the equation , the matrix  couples the pure solutions. We set , where  is the matrix of eigenvectors of , to get:or:This diagonalizes the system: . The general solution is then:\n\n原来的等式： 中，  在关系式中，相互耦合。为了 解耦 就是实现 对角化。\n设置  可以得到 \nMatrix exponential 含有矩阵的指数\n\nWhat does  mean if  is a matrix ? We know that for a real number ,We can use the same formula to define :\n\n解耦合的表达式，看起来一头雾水。** 其实是， 泰勒展开式的矩阵形式 **\n\nSimilarly, if the eigenvalues of  are small, we can use the geometric series  to estimate \n\n同理，** 存在 几何级数 的矩阵形式 **\n\nWe’ve said that . If  has  independent eigenvectors we can prove this from the definition of  by using the formula :It’s impractical to add up infinitely many matrices. Fortunately, there is an easier way to compute . Remember that:\n\n能够对角化的矩阵，都可以表述为上下两式子。\n\nWhen we plug this in to our formula for  we find that:This is another way to see this relationship between the stability of  and the eigenvalues of .\n\nSecond order\nWe can change the second order equation  into a two by two first order system using a method similar to the one we used to find a formula for the Fibonacci numbers. \n\n我们可以将 二阶微分方程 转化为 矩阵形式的 一阶方程就同 我们处理 Fibonacci 数列一样。\n\nIf , thenWe could use the methods we just learned to solve this system, and that would give us a solution to the second order scalar equation we started with.If we start with a  order equation we get a  by  matrix with coefficients of the equation in the first row and 1’s on a diagonal below that; the rest of the entries are 0.\n\n若我们在处理 阶方程，我们会得到一个 矩阵，仅在第一行和主对角线的下一对角线有非零元素。\nMarkov matrices; Fourier series\nIn this lecture we look at Markov matrices and Fourier series - tow applications of eigenvalues and projections.\n\n在这一节中，我们将介绍 马尔可夫矩阵 和 傅立叶级数 – 两个关于 特征值 和 投影 的应用。\nEigenvlaues of The eigenvalues of  and the eigenvalues of  are the same:so property 10 of determinants tells us that . If  is an eigenvalue of  then  and  is also an eigenvalue of .\n可以证明，矩阵和矩阵的特征值相同。\n\n,\n上等式两边取 det\n等式左边行列式为0对应，同样可以令等式右边 行列式为零。\n\nMarkov matrices马尔可夫矩阵的提出跟概率思想有关。\n\nA matrix like:in which all entries are non-negative and each column adds to 1 is called a Markov matrix. These requirements come from Markov matrices’s use in probability. Squaring or raising a Markov martix to a power gives us another Markov matrix.\n\n\n所有元素都是非负数的。\n每列元素总和为1。\n\n** 马尔可夫矩阵的任意阶乘仍然满足马尔可夫矩阵性质 **\n\nWhen dealing with systems of differential equations, eigenvectors with the eigenvalue 0 represented steady states. Here we’re dealing with powers of matrices and get a steady state when  is an eigenvalue.\n\n\n面对 微分方程系统 问题，特征值是否为0 代表着状态稳定。\n对于 幂矩阵系统 问题，特征值是否为1 决定状态稳定。\n\n\nThe constraint that the columns add to 1 guarantees that 1 is an eigenvalue. All other eigenvalues will be less than 1. \n\n稳态条件：\n\n 是一个特征值(一定存在)。\n其余特征值 。\n\n\nRemember that (if  has  independent eigenvectors) the solution to  is .If  and all others eigenvalues are less than one the system approaches the steady state . This is the  component of .\n\n is 。\n\nWhy does the fact that the columns sum to 1 guarantee that 1 is an eigenvalue? if 1 is an eigenvalue of , then:\n\n\n** 如何证明特征值一定存在 **\n\nshould be singular. Since we’ve subtracted from each diagonal entry, the sum of the entries in each column of  is zero.But then the sum of the rows of  must be the zero row, and so  is singular.The eigenvector  is in the nullspace of  and has eigenvalue 1. It’s not very hard to find .\n\n\nMarkov matrices，各列元素之和为1，主对角线各元素减去1后， 各列元素和为零。（证明 行向量线性相关）\n 对应的特征向量  是零空间  上的解，所以矩阵  列向量线性相关。\n\n\nWe’re studying the equation  where  is a Markov matrix.\n\n我们研究 矩阵 属 Markov matrix 的  问题。\n\nFor example  might be the population of (number of people in) Massachusetts and  might be the population of California. might describe what fraction of the population moves from state to state, or the probability of a single person always be positive.We want to account for all the people in our model, so the columns of  add to 1 = 100%.\n\n举个例子： 表征 麻省人口，  表征 加州人口。 和  组成人口列向量 \n矩阵描述了 从一个州转移到另一个州的 人口分数。\n** 这里可以视为 矩阵 为其列向量与  的线性组合 **\n\nFor example:For the next few values of , the Massachusetts population will decrease and the California population will increase while the total population remains constant at 1000.\n\n\nTo understand the long term behavior of this system we’ll need the eigenvectors and eigenvalues of . We know that one eigenvalue is . Because the trace  is the sum of the eigenvalues, we see that .\n\n计算矩阵的行列式时，可以利用 矩阵迹trace和行列式 来简化 特征值的运算。\n\nNext we calculate the eigenvectors:so let From what we learned about difference equations we know that:When  we have:so  and .\n\n以上就是，利用特征值分解的全过程。\n从3b1b那边看来的理解：\n** 特征值是，在特征向量方向，线性变化后，向量不改变方向，仅改变角度的解。 **我们将向量分解为，各个特征向量，在线性变化的过程中放缩，再合成，就是这里的思路。\n\nIn some applications Markov matrices are defined differently - their rows add to 1 rather than their columns. In this case, the calculations are the transpose of everything we’ve done here.\n\n在有些应用中， Markov matrices 会被定义为 行和为1，使用时注意专置。\nFourier series and projections这一节，从正交投影到傅立叶级数/\nExpansion with an orthonormal basis\nIf we have an orthonormal basis  then we can write any vector  as , where:Since  unless , this equation gives .\n\n有一组正交基，我们可以很方便地，在正交基的张成空间内表示任意向量。同时利用正交性质，可以很方便的获取向量在各个分量上的投影。\n\nIn terms of matrices, , or . So . Because the  form an orthonormal basis,  and . This is another way to see that .\n\n从矩阵乘法的角度看待  可以从另一个角度的得到 。\nFourier series\nThe key idea above was that the basis of vectors  was othonormal. Fourier series are built on this idea. We can describe a function  in terms of trigonometric functions:\n\n\n\nThis Fourier series is an infinite sum and the previous example was finite, but the two are related by the fact that the cosines and sines in the Fourier series are orthogonal.We’re now working in an infinite dimensional vector space. The vectors in this space are functions and the (orthogonal) basis vectors are 1, , , , , \n\n基于以上观点，外化可得： 从无穷维度的函数空间，抽取相互正交的正弦函数，由它们的线性组合，表征函数。\n原因： cos函数 和 sin函数 是正交的。\n\nWhat does “orthogonal” mean in this context ? How do we compute a dot product or inner product in this vector space? For vectors in  the inner product is .Functions are described by a continuum of values  rather than by a discrete collection of components . The best parallel to the vector dot product is:\n\n\n设计函数的正交点基形式的一个特点：让函数相同的基点积后为1，正交的向量点积后为0。这里就设计了类似 相关函数 的形式。\n\nWe intergrate from  to  because Fourier series are periodic:\n\n\n\nThe inner product of two basis vectors is zero, as desired. For example,\n\n\n\nHow do we find  to find the coordinates or Fourier coefficients of a function in this space ? The constant term  is the average value of the function. Because we’re working with an orthonormal basis, we can use the inner product to find the coefficients .\n\n\n\nWe conclude that . We can use the same technique to find any of the values .\n\n可能是 Euler 或 Fourier 提出的，由于三角函数具有  的周期性，正交（相关）积分被设置为在区间上。\n","categories":["Linear Algebra"],"tags":["Courses Notes","Gilbert Strang"]},{"title":"01_Introducation/Categorys/Cost Function/","url":"/Machine-Learning/01-introducation-categorys-cost-function/","content":"introducationWhat’s Machine Learning?\nArthur Samuel: “the field of study that gives computers the ability to learn without being explicitly programmed.”\nTom Mitchell: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”\n\nTwo Typical CategorySupervised LearningEvery example has its label\n\nRegression problem\nClassification problem\n\nUnsupervised Learninghaven’t given the label of example\n\nstructure learning\nclustering\n\nModel and Cost FunctionModel Representationchoose a type of model and use an algorithm to find the optimal parameters\n\nCost Functionusing a cost function, the accuracy of hypothesis function can be measured. \nThe most common Cost Function is the Squared Error Function or Mean squared error\n\nLinear Regression with one variableHypothesis: Parameters: Cost Function: Goal： \n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes"]},{"title":"MIT18.06_6_Determinant and Eigenvalues","url":"/Linear-Algebra/mit18-06-6-determinant-and-eigenvalues/","content":"Properties of determinantsDeterminants\nNow halfway through the course, we leave behind rectangular matrices and focus on square ones. Our next big topics are determinants and eigenvalues.\n\n课程过半，我们已经可以将 长方形矩阵 丢置一遍，聚焦于 方阵。下一个重要主题是 行列式 和 特征值。\n\nThe determinant is a number associated with any square matrix; we’ll write it as  or . The determinant encodes a lot of information about the matrix; the matrix is invertible exactly when the determinant is non-zero.\n\n特征值 是任何方阵相关的 数字特征，记作 以及 。\n\n特征值蕴涵着大量关于矩阵的信息。\n如果矩阵不可逆，则矩阵行列式为0。\n\nProperties\nRather than start with a big formula, we’ll list the properties of the determinant. We already know that ; these properties will give us a formula for the determinant of square matrices of all sizes.\n\n比起直接给出难记忆的公式，我们将列出 行列式的性质，透过性质我们可以推导出行列式的计算公式。\n\n\ndet \n\n\n单位矩阵的行列式为1。\n\n\nIf you exchange two rows of a matrix, you reverse the sign of its determinant from positive to negative or from nagative to positive.\n\n\n如果交换矩阵的两行，矩阵的行列式将会符号反转。\n\n\n(a) If we multiply one row of a matrix by , the determinant is multiplied by : .(b) The determinant behaves like a linear fucntion on the rows of the matrix:\n\n\n线性性质：\n\n行列式的某行或某列乘以，比起原行列式放大了倍。\n对于单独某行或者某列的加法，可以作 行列式分解。\n\n\nProperty 1 tells us that . Property 2 tells us that .The determinant of a permutation matrix  is 1 or -1 depending on whether  exchanges an even or odd number of rows.\n\n对于交换矩阵的行列式，取决于交换的行数的 奇偶性。\n\nFrom these three properties we can deduce many others:\n\n\n\nIf two rows of a matrix are equal, its determinant is zero.\n\n\n若矩阵的两行相等，矩阵的行列式为零。\n\nThis is because of property 2, the exchange rule.On the one hand, exchanging the two identical rows does not change the determinant. On the other hand, exchanging the two rows changes the sign of the determinant. Therefore the determinant must be 0.\n\n\n\nIf , subtracting  times row  from row  doesn’t change the determinant.  In two dimensions, the argument look like:The proof for higher dimensional matrices is similar.\n\n\n矩阵的一行的若干倍加减在另一行上，不会改变行列式。\n\n6.If  has a row that is all zeros, then det .  We get this from property 3(a) by letting .\n\n矩阵的某行/列 全部为0，行列式为零。可以用定理3推导得到。定理4+定理5 同样可以得到。\n\n\nThe determinant of a triangular matrix is the product of the diagonal entries(pivots).\n\n\n三角矩阵（包括上跟下）的行列式为主对角线元素之积。\n\nProperty 5 tells us that the determinant of the triangular matrix won’t change if we use elimination to convert it to a diagonal matrix with the entries  on its diagonal. Then property 3(a) tells us that the determinant of this diagonal matrix is the product  times the determinant of the identity matrix, Property 1 completes the argument.\n\n\n性质5 让我们知道，消元法过程中的相减，不会改变方阵行列式的值。\n性质3(a) 可以把三角矩阵转换为常数乘上单位矩阵。上述等于主对角线元素之积。\n\n\nNote that we cannot use elimination to get a diagonal matrix if one of the  is zero. In that case elimination will give us a row of zeros and property 6 gives us the conclusion we want.\n\n使用消元法时，主对角线出现零时，行列式为零，同样会出现一行全为0。\n\n\ndet  exactly when  is singular.\n\n\n当行列式为零时，意味着矩阵为奇异。\n\nIf  is singular, then we can use elimination to get a row of zeros, and property 6 tells us that the determinant is zero.If  is not singular, than elimination produces a full set of pivots  and the determinant is  (with minus signs from row exchanges).\n\n\n若矩阵奇异，我们可以利用消元法得到一行全为零。性质6可知，行列式也为零。\n若矩阵非奇异，消元法可以获得一组主元不为零。行列式也为这一组主元的基（不为零）。\n\n\nWe now have a very practical formula for the determinant of a non-singular matrix. In fact, the way computers find the determinants of large matrices is to first perform elimination (keeping track of whether the number of row exchanges is odd or even) and then multiply the pivots:\n\n如今我们已经有了非常实用的计算公式。\n\n利用消元法获得主对角线元素。\n行列式为主对角线元素之积。\n\n** 实际上matlab计算行列式的方式也就是这样 ** \n\n\nThis is very useful. Although the determinant of a sum does not equal the sum of the determinants, it is true that the determinant of a product equals the prodcut of the determinants.\n\n\n矩阵积的行列式 = 各个矩阵行列式的积这一点非常有用。尽管对加法不成立。\n\nFor example:because . (Note that if  is singular then  does not exist and ) Also,  and  (applying property 3 to each row of the matrix). This reminds us of volume - if we double the length, width and height of a three dimensional box, we increase its volume by a multiple of \n\n\n 性质9\n 由性质3(b)可以得到。\n\n\n\nThis lets us translate properties (2, 3, 4, 5, 6) involving rows into statements about columns. For instance, if a column of a matrix is all zeros then the determinant of that matrix is zero.\n\n\n这一性质可以将 性质(2,3,4,5,6) 全部扩展至列。\n\nTo see why , use elimination to write . The staement becomes . Rule 9 then tells us .\n\n\nMatrix  is a lower triangular matrix with 1’s on the diagonal, so rule 5 tells us that . Because  is upper triangular, rule 5 tells us that . Therefore  and .\n\n证明 行列式矩阵转置不变。对矩阵进行LU分解。  可以将性质10转为 因为为下三角矩阵，且主对角线全为1，得到，又因为  为上三角矩阵，利用性质8 可以得到 最后得到 \n\nWe have one loose end to worry about. Rule 2 told us that a row exchange changes the sign of the determinant. If it’s possible to do seven row exchanges and get the same matrix you would by doing ten row exchanges, then we could prove that the determinant equals its negative.To complete the proof that the determinant is well defined by properties 1, 2 and 3 we’d need to show that the result of an odd number of row exchanges (odd permutation) can never be the same as the result of an even number of row exchange(even permutation).\n\nDeterminant formulas and cofactors\nNow that we know the properties of the determinant, it’s time to learn some (rather messy) formulas for computing it.\n\n现在已经熟知 行列式的性质 了，该进一步了解 计算的公式 了。\nFormula for the determinant\nWe know that the determinant has the following three properties:\n\n\nExchanging rows reverses the sign of the determinant.\nThe determinant is linear in each row separately.\n\n\n上一节中的十条性质，都是从 性质123 三条推导得到的。\n\nLast class we listed seven consequances of these properties. We can use these ten properties to find a formula for the determinant of a 2 by 2 matrix:\n\n\n可以利用几条性质来将二阶行列式的公式推导出来。\n\nBy applying property 3 to separate the individual entries of each row we could get a formula for any other square matrix. However, for a 3 by 3 matrix we’ll have to add the determinants of twenty seven different matrices! Many of those determinants are zero. The non-zero pieces are:\n\n\n利用行列式的 线性性质 同样可以将 3 by 3 的行列式分解为若干个 比较稀疏的行列式。\n\n 方阵可以分解为 27 个行列式\n其中仅有6个矩阵非零\n其余21矩阵为零\n\n\n\n\nEach of the non-zero pieces has one entry from each row in each column, as in a permutation matrix. Since the determinant of a permutation matrix is either 1 or -1, we can again use property 3 to find the determinants of each of these summands and obtain our formula.\n\n每个非零向量的成分，每行每列 有切仅有 一个非零元素，就跟排列矩阵一样。\n\nOne way to remember this formula is that the postive terms are products of entries going down and to the right in our original matrix, and the negative terms are products going down and to the left. This rule of thumb doesn’t work for matrices larger than 3 by 3.\n\n\n正项： going down and to the right\n负项： going down and to the left\n\n这里可以大致理解一下。或者直接记 国内教材中，所谓 画蝴蝶法\n\nThe number of parts with non-zero determinants was 2 in the 2 by 2 case, 6 in the 3 by 3 case, and will be 24=4! in the 4 by 4 case. \n\n阶矩阵中，非零项数量为 ！\n\nThis is because there are  ways to choose an element from the first row(i.e. a value for ), after which there are only  ways to choose an element from the second row that avoids a zero determinant. Then there are  choices from the third row,  from the fourth, and so on.\n\n\n这是因为，在第一行中选取元素有种方式。\n第二行中选取元素有种方式。……\n\n\nThe big formula for computing the determinant of any square matrix is:\n\n\n\nwhere  is some permutation of . If we test this on the identity matrix, we find that all the terms are zero except the one corresponding to the trivial permutation  This agrees with the first property: . It’s possible to check all the other properties as well, but we won’t do that here.\n\n 是  的一种排列组合。\n\nApplying the method of elimination and multiplying the diagonal entries of the result (the pivots) is another good way to find the determinant of a matrix.\n\n** 应用消元法，乘上主对角线元素（主元），是另一种求得行列式的好方法 **\nExample\nIn a matrix with many zero entries, many terms in the formula are zero. We can compute the determinant of:\n\n\n\nby choosing a non-zero entry from each row and column, multiplying those entries, givign the product the appropriate sign, then adding the results.The permutation corresponding to the diagonal running from  to  is (4,3,2,1).This contributes 1 to the determinant of the matrix; the contribution is positive because it takes two row exchanges to convert the permutation (4, 3, 2, 1) to the identity (1, 2, 3, 4).Another non-zero term of  comes from the permutaion (3, 2, 1, 4). This contributes -1 to the sum, because one exchange (of the first and third rows) leads to the identity.These are the only two non-zero terms in the sum, so the determinant is 0. We can confirm this by noting that row 1 minus row 2 plus row 3 minus row 4 equals zero.\n\n这个例子给我们使用 排列法 来计算该行列式：\n\n由于矩阵非零向量比较少，非零分量只有两个。\n分别为 排列 和 \n分量的符号跟排列 逆序对 的 奇偶性 相关\n逆序对为偶数，正分量。\n逆序对为奇数，负分量。\n\n\n\nCofactor formulaCofactor formula: 翻译作 余子式公式\n余子式：原方阵某行某列划去后，阶的行列式。\n\nThe cofactor formula rewrites the big formula for the determinant of an  by  matrix in terms of the determinants of smaller matrices.\n\n余子式公式，将高阶行列式分解为低阶行列式。\n\nIn the  case, the formula looks like:\n\n\nThis comes from grouping all the multipes of  in the big formula. Each element is multiplied by the cofactors in the parenthese following it. \n\nNote that each cofactors is (plus or minus) the determinant of two by two matrix. That determinant is made up of products of elements in the row and columns NOT containing .\n每个余子式，都是由不包含 对应元素 的低一阶矩阵组成。\n\nIn general, the cofactor  of  can be found by looking at all the terms in the big formula that contain .  equals  times the determinant of the  by  square matrix obtained by removing row  and column . ( is positive if  is even and negative if  is odd.)\n\n 对应的余子式  的计算结果是，所有包含分量之和的非因式。\n**  等于  乘上  by  的方阵。**\nFor  matrices, the cofactor formula is:Applying this to a  matrix gives us:\nTridiagonal matrix三对角矩阵\n\nA tridiagonal matrix is one for which the only non-zero entries lie on or adjacent to the diagonal. For example, the  tridiagonal matrix of 1’s is:\n\n三对角矩阵 的非零元素只存在于 主对角线 和 对角线相邻 位置上。\n\n\nWhat is the determinant of an  tridiagonal matrix of 1’s ?\n\n\nIn fact, . We get a sequence which repeats every six terms:\n\n三对角阵的行列式有一个特别特殊的性质：\n\nCramer’s rule, inverse, volume\nWe know a formula for a some properties of the determinant.Now we see how the determinant can be used.\n\n这一节开始介绍行列式的性质。\nFormula for \nWe know:\n\nCan we get a formula for the inverse of a  by  or  by  matrix? \nWe expect that  will be involved, as it is in the  by  example, and by looking at the cofactor matrix  we might guess that cofactors will be involved.\n\n\n通过观察2阶方阵的逆阵表达式，我们可以推断，行列式的倒数和余子式与取逆公式有关。\n\nIn fact:where  is the matrix of cofactors – please notice the transpose！Cofactors of row one of  go into column 1 of , and then we divide by the determinant.\n\n\n 是由余子式作为元素组成的 余子式矩阵。\n\n\nThe determinant of  involves products with  terms and the cofactor matrix involves products of  terms. and  might cancel each other.This is much easier to see from our formula for the determinant than when using Gauss-Jordan elimination.\n\n从公式可以很明显的看出: 与 是可以相互消除的。\n\nTo more formally verify the formula, we’ll check that The entry in the first row and first column of the product matrix is:(This is just the cofactor formula for the determinant.) This happens for every entry on the diagonal of .\n\n对 逆矩阵 的定义式进行变形： \n\n等式两边乘上\n两边乘上\n最后得到的结论就是 .\n\n\nTo finish proving that , we just need to check that the off-diagonal entries of  are zero. In the two by two case, multiplying the entries in row 1 of  by the entries in column 2 of  gives . This is the determinant of . \n\n\n证明  在对角线外的元素为零。\n对角线外的元素值：等效于：** 有某两行元素相同的矩阵行列式 **\n\n\nIn higher dimensions, the product of the first row of  and the last column of  equals the determinant of a matrix whose first and last rows are identical. This happens with all the off-diagonal matrices, which confirms that .\n\n在更高维度的方阵中，矩阵的第一行与的最后一列的矩阵乘法，等效于一个 首行与尾行元素完全相同 的矩阵行列式。\n\nThis formula helps us answer quesions about how the inverse changes when the matrix changes.\n\nCramer’s Rule\nWe know that if  and  is nonsingular, then . Applying the formula  gives us:\n\n\n矩阵可逆时，可以得到 \n用的求解公式代入得到：.\n\n\nCramer’s rule gives us another way of looking at this equation. To derive this rule we break  down into its components. Because the th component of  is a sum of cofactors times some number, it is the determinant of some matrix .where  is the matrix created by starting with  and then replacing column  with , so:\n\n\n将  合并，得到对应的 向量。\n\n\nThis agrees with our formula .When taking the determinant of  we get a sum whose first term is  times the cofactor  of .Computing inverse using Cramer’s rule is usually less efficient than using elimination.\n\nVolumeparalledlepiped: 平行六面体。\n\nClaim:  is the volume of the box(paralledlepiped) whose edges are the column vectors of . (We could equally well use the row vectors, forming a different box with the same volume.)\n\n矩阵的行列式的绝对值，其值为以方阵的 列向量为边 的平行六面体的体积。\n（利用行向量构造的平行六面体体积仍然相等\n\nIf , then the box is a unit cube and its volume is 1. Because this agrees with our claim, we can conclude that the volume obeys determinant property 1.\n\n当方阵，方阵构成了单位正方体。\n\nIf  is an orthogonal matrix then the box is a unit cube in a different orientation with volume . (Because  is an orthogonal matrix,  and so .)\n\n如果方阵，那么矩阵是一个正交矩阵，那么组成六面体也为单位立方体。\n以不同的正交基构成了体积为1的单位立方体。\n\nSwapping two columns of  does not change the volume of the box or (remembering that ) the absolute value of the determinant( property 2). If we show that the volume of the box also obeys property 3 we’ll have proven  equals the volume of the box.\n\n交换矩阵的两列向量，不会改变形成六面体的体积。\n\n\nif we double the length of one column of , we double the volume of the box formed by its columns. Volume satisfies property 3(a).Property 3(b) says that the determinant is linear in the rows of the matrix:\n\n体积的公式满足行列式的线性规则。\nFigure 2 illustrates why this should be true.\n\n\nAlthough it’s not needed for our proof, we can also see that determinants obey property 4. If two edges of a box are equal, the box flattens out and has no volume.\n\n同样遵循 行列式的性质4 当列向量或者行向量中:\n\n有两组线性相关\n体积会为零（e.g. 折叠为一张平面\n\n\nImportant note: If you know the coordinates for the corners of a box, then computing the volume of the box is as easy as calculating a determinant. In particular, the area of a paralledlogram with edges  and  is half the area of that parallelogram, or . The area of a triangle with vertices at ,  and  is:\n\n\n如果知道 corners of box，计算体积也就同计算行列式一样简单了。\n二维平面中，两个二维向量形成的四边形面积为二阶矩阵的行列式\n\n若知道三角形的顶点位置 求三角形面积的两种方法：\n\n拓展第三维度，二维面积数值上等于高为1的体积\n做差，向量表示平面两边，再计算二阶行列式。\n\nEigenvalues and eigenvectors\nThe subject of eigenvalues and eigenvectors will take up most of the rest of the course. We will again be working with square matrices.Eigenvalues are special associated with a matrix and eigenvectors are special vectors.\n\n特征值 和 特征向量 会占据这个课程剩下的大量内容。\n\n特征值 是矩阵相关的特殊数值。\n特征向量 是矩阵相关的特殊向量。\n\nEigenvectors and eigenvalues\nA matrix  acts on vectors  like a fuction does, with input  and output . Eigenvectors are vectors for which  is paralled to . In other words:In this equation,  is an eigenvector of  and  is an eigenvalve of .\n\n定义：矩阵作用于向量，得到结果\n\n特征向量是另  与  平行的向量.\n特征值是 与  之间的缩放尺度。\n\nEigenvalue 0特征值为零时候。\n\nIf the eigenvalue  equals  then . Vectors with eigenvalue 0 make up the nullspace of ; if  is singular, then  is an eigenvalue of .\n\n\n当特征值为0时，等效 .** 等效于，特征值为0时，特征向量为零空间的向量 **\n矩阵奇异时，特征值是矩阵的一个特征值。\n\nExamples\nSuppose  is the matrix of a projection onto a plane. For any  in the plane , so  is an eigenvector with eigenvalue 1.The eigenvectors of  span the whole space (but this is not true for every matrix).\n\n是投影在一个向量上的 投影矩阵。对于平面上的任意向量  都对应着特征值 1。\n投影矩阵的 特征向量 可以张成整个空间（并不总是成立\n\nThe matrix  has an eigenvector  with eigenvalue 1 and another eigenvector  with eigenvalue -1. These eigenvectors span the space. They are perpendicular because  (as we will prove).\n\n对于例子，二阶矩阵的两个特征向量能够 张成整个空间。此外，两个特征向量，相互垂直（由于 矩阵对称\n\n\nAn  by  matrix will have  eigenvalues, and their sum of the diagonal entries of the matrix: . This sum is the trace of the matrix. For a two by two matrix, if we know one eigenvalue we can use this fact to find the second.\n\n\nshape为的方阵，有个特征值\n所有特征值之和 = 方阵主对角线元素之和\n主对角线元素之和，又称为 迹trace。\n\n\nCan we solve  for the eigenvalues and eigenvectors of ?Both  and  are unknown; we need to be clever to solve this problem:\n\n对特征值的定义式进行变换： \n\nIn order for  to be an eigenvector,  must be singular. In other words, . We can solve this characteristic equation for  to get  solutions.If we’re lucky, the solutions are distinct. If not, we have one or more repeated eigenvalues.\n\n观察公式：\n\n要使得向量存在非零解\n那么，我们可以把方程的前一部分可以视作矩阵\n条件式更变为存在非零解，即nullspace不为0\n运气好特征值全部不同。否则，会有重复值。\n\n\nOnce we’ve found an eigenvalue , we can use elimination to find the nullspace of . The vectors in that nullspace are eigenvectors of  with eigenvalue .\n\n一旦寻找到了特征值，就能利用 elimination 找到  的 nullspace。\nCalculating eigenvalues and eigenvectors\nLet . Then:\n\n二阶矩阵求特征值的行列式系数，可以表示为：迹trace 和 \n\nNote that the coefficient 6 is the trace (sum of diagonal entries) and 8 is the determinant of . In general, the eigenvalues of a two by two matrix are the solutions to:Just as the trace is the sum of the eigenvalues of a matrix, the product of the eigenvalues of any matrix equals its determinant.\n\n** 特征值的积 = 方阵的行列式 **\n\nFor , the eigenvalues are  and . We find the eigenvector  for  in the nullsapce of .\n\n\n will be in the nullsapce of . The nullspace is an entire line;  could be any vector on that line. A natural choice is .\n\n\nNote that these eigenvectors are the same as those of . Adding  to the matrix  added 3 to each of its eigenvalues and did not chagne its eigenvectors, because .\n\n在矩阵的基础上+nI，得到矩阵\n\n矩阵和矩阵特征向量相同\n矩阵和矩阵的特征值上，有个差值n\n\nA cautionSimilarly, if  and , . It would be nice if the eigenvalues of a matrix sum were always the sums of the eigenvalues, but this is only true if  and  have the same eigenvectors. The eigenvalues of the product  aren’t usually equal to the products , either.\n\n上面的推论，仅当与两矩阵的特征向量相同时，可以满足该等式。\nComplex eigenvalues复特征值\n\nThe matrix  rotates every vector in the plane by .\n\n旋转矩阵的一个重要特征：反对称（就是和 对称矩阵 两个极端。\n\nIt has trace  and determinant . Its only real eigenvector is the zero vector; any other vector’s direction changes when it is multiplied by . How will this affect our eigenvalue calculation?\n\n\n\n has solutions  and . If a matrix has a complex eigenvalue  then the complex conjugate  is also an eigenvalue of that matrix.\n\n如果特征值中，有复数存在。那么它的共轭复数，一定也是特征值。\n\nSymmetric matrices have real eigenvalues. For antisymmetric matrices like , for which , all eigenvalues are imaginary .\n\n\n对称矩阵 的特征值： 全实数。\n帆对称矩阵 的特征值：纯虚数。\n\nTriangular matrices and repeated eigenvalues三角矩阵 与 重复特征值\n\nFor triangular matrices such as , the eigenvalues are exactly the entires on the diagonal. In this case, the eigenvalues are 3 and 3:\n\n\n\nso  and . To find the eigenvectors, solve:\n\n\n\nto get . There is no independent eigenvector .\n\n重复特征值，将不会带来另外独立的特征向量。\n","categories":["Linear Algebra"],"tags":["Courses Notes","Gilbert Strang"]},{"title":"03_Multivariate Linear Regression","url":"/Machine-Learning/03-multivariate-linear-regression/","content":"Multivariate Linear RegressionMultiple FeaturesLinear regression with multiple variables \n\n\nRemark: Note that for convenience reasons in this course we assume .\nGradient Descent for Multiple FeaturesHypothesis: Parameters: Cost Function:\nGradient Descent AlgorithmRepeat{}(simultaneously update for every )\nRepeat until convergence:{}\nThe following image compares gradient descent with one variable to gradient descent with multiple variables: \n\npractical tricksFeature ScalingMain Idea: Make sure features are on a similar scale. will descend quickly on small ranges and slowly on large ranges.\nMethod: Get every feature into approximately a  range\nMean normalizationReplace  with  to make features have approximately zero mean (Do not apply to ).\nE.g.\n\n\nmean normalization formula: is the average of feature i, and  mean the range of value(the max - min) or the standard deviation (if u know)\nGradient Descent refer to Learning RateTo recap: \n\nif  is too small: slow convergence.\nif  is too large: may not decrease on every iteration and thus may not converge.\n\n\nFeatures and Polynomial RegressionThe feature and form of hypothsis function can be improved in different way.also can  multiple features into one . For instance, we can combine  and  into a new feature .\nCommon Polynomial RegrssionQuadratic Function\nCubic Function\nSquare Root Function\nRemarkOne important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.\neg. if  has range 1-1000 then range of  becomes 1-1000000 and that of  becomes 1-1000000000\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Supervised Learning"]},{"title":"05_Logistic Regression","url":"/Machine-Learning/05-logistic-regression/","content":"Classification and RepresentationClassificationLinear Regression with a threshold to divide the data in different class.But the method doesn’t work well cuz classification isn’t actually a linear function\nWe usually denote the event happen as 1 (what we pay attention to), according to  information theory, the unique event happen means lots of information trasfer.\n\nTumor: Benign （0）， Malignant （1）\nOnline Transaction: not fraudulent （0） ，fraudulent （1）\n\nHypothesis RepresentationDifferent with Linear Model, we choose the hypothesis function  to satisfy .This is accomplised by plugging  into the Logistic Function\nSigmoid Function\n\n\nThe function  maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.\n will give us the probability that our output is 1. \n\nDecision BoundaryTo get discrete 0 or 1 classification:\nSigmoid’s feature\nSo \nFrom these statements\nThe decision boundary is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.\n\nCost Functionthe Logistic Function will cause the output to be wavy, causing many local optima.（it’s not a convex function)\nCost Function for logistic regression\nSimplified Cost Function and Gradient DescentWe can compress our cost function’s two conditional cases into one case:\nEntire Cost Function\nVectorized Implementation\nGradient Descentthe general form of gradient descent is:Repeat{}work out the derivative part using calculus to get:Repeat{}Notice that this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta.\nVectorized Implementation\nAdvanced Optimization\n“Conjugate gradient”, “BFGS”, and “L-BFGS” are more sophisticated, faster ways to optimize  that can be used instead of gradient descent.Octave or Matlab have provided libraries.\n\nFirst Stepprovide a function that evaluates Cost function &amp;&amp; Gradient:\nFunction Defination\nfunction [jVal, gradient] = costFunction(theta)  jVal = [...code to compute J(theta)...];  gradient = [...code to compute derivative of J(theta)...];end\n\nSet the Options and optimization algorithmoptions = optimset('GradObj', 'on', 'MaxIter', 100);initialTheta = zeros(2,1);   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);\n\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Supervised Learning","Classification"]},{"title":"06_Overfitting and Regularization","url":"/Machine-Learning/06-overfitting-and-regularization/","content":"OverfittingModel can fit the training dataset perfectly, but can’t do genelize well to the test example.\nUnderfittingor high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. \nIt’s usually caused by few features.\noverfittingor high variance, is caused by a hypothesis function that fits the available data but doesn’t generalize well to predict new data.\nTwo main options to address the issue of overfittingReduce the no. of features\nManually select which features to keep.\nUse a model selection algorithm\n\nRegularization\nKeep all the features, but reduce the magnitude of parameters .\nRegularization works well when we have a lot of slightly useful features.\n\nRegularizationRegularization can “shrink“ some of the theta in the hypothesis function.\n\n\nThe , or lambda, is the regularization parameter.It determines how much the costs of our theta parameters are inflated. \nNote that using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting.\nRegularized Linear Regression\nNote: [8:43 - It is said that X is non-invertible if m  n. The correct statement should be that X is non-invertible if m &lt; n, and may be non-invertible if m = n.\n\nGradient DescentRepeat{}\n\nThe term  performs regularization.With some manipulation update rule can also be represented as:\n\n\n\n will always be less than 1.Intuitively you can see it as reducing the value of  by some amount on every update. Notice that the second term is now exactly the same as it was before.\n\nNormal Equation\nTo add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:\n\n\nRemarkRecall that if m &lt; n, then  is non-invertible. However, when we add the term λ⋅L, then  becomes invertible.\nas long as the parameter  is greater than 0.\nRegularized Logistic Regression\nCost Functioncost function for logistic regression:\ncost function for logistic regression with regulazation:\n\nThe second sum,  means to explicitly exclude the bias term, .I.e. the  vector is indexed from 0 to n (holding n+1 values, through ), and this sum explicitly skips  , by running from 1 to n, skipping 0. Thus, when computing the equation, we should continuously update the two following equations:\n\n\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes"]},{"title":"02_Gradient Descent/Linear Regression/Linear Algebra","url":"/Machine-Learning/02-gradient-descent-linear-regression-linear-algebra/","content":"Parameter LearningGradient DescentBy taking the derivative(the tangential line to a function) of cost function, we can get a local minima.\nThe way we iterately update the parameters is Gradient Descent\nGradient Descent Algorithmrepeat until convergence:\n\nAt each iteration j, one should simultaneously update the parameters . Updating a specific parameter prior to calculating another one on the  iteration would yield to a wrong implementation.\n\n\nGradient Descent for Linear Regressionrepeat until convergence:{\n}\nThe gradient descent is also called Batch Gradient Descent.This method looks at every example in the entire training set on every step.\n\nLinear Algebra ReviewIt’s hard to write the latex formula for the Matrix. So help urself\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Supervised Learning","Regression"]},{"title":"04_Computing Parameters Analytically","url":"/Machine-Learning/04-computing-parameters-analytically/","content":"Computing Parameters AnalyticallyNormal EquationNormal Equation is the second way of minimizing Cost Function , except Gradient Descent.\nMethod:explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. \nThe normal equation formula is given below: \nthere is something matter Matrix Derivative\nNg’s notes\nAndrew的notes的公式推导，已经将 偏导数=0 纳入考虑。\n\n不同于 \n两边左乘得到derivative，\n对两边乘上  的逆\n\n直观理解的左乘： 不一定是可逆方阵 至少是方阵（不一定可逆但是用matlab的 pinv一定能够得到，最优解（不管是否可逆\n\nIn practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.\n这里可能会需要一些 矩阵论 的知识。\nNormal Equation NoninvertibilityIf  is noninvertible, the common causes might be having:\n\nRedundant features, where two features are very closely related (i.e. they are linearly dependent)\nToo many features (e.g. m ≤ n). In this case, delete some features or use “regularization” (to be explained in a later lesson).\n\nSolutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Supervised Learning"]},{"title":"07_Neural Network","url":"/Machine-Learning/07-neural-network/","content":"这里主要集结了CS229，week 4的内容，感觉也没啥内容。。。可能是，之前就比较熟悉lenet5分类器的缘故。\nNone-linear Hypotheses提出了 Neural Network\nNeurons and the Brain大概就是，一些脑科学实验，然后从其中，抽象出，神经网络这一数学模型\nModel Representation IInput LayerOur input nodes (layer 1), also known as the “input layer”, go into another node (layer 2)。\nOutput Layerwhich finally outputs the hypothesis function, known as the “output layer”.\nHidden LayerThe intermediate layers of nodes which between the input and output layers called the “hidden layers.”\nThe other notation\nVisually, a simplistic representation looks like:\n\nlabel these intermediate or “hidden” layer nodes  and call them “activation units.”\n “activation” of unit  in layer  matrix of weights controlling function mapping from layer  to layer + 1\n\nIf we had one hidden layer, it would look like:The values for each of the “activation” nodes is obtained as follows:\n\nThe dimensions of these matrices of weights is determined as follows:If network has  units in layer  and  units in layer , then  will be of dimension .\n矩阵bias unit的设置The +1 comes from the addition in  of the “bias nodes,”  and  . In other words the output nodes will not include the bias nodes while the inputs will. The following image summarizes our model representation: \n\nModel Representation IIvectorized implementationThe vector representation of  and  is:\n\n\nSetting , we can rewrite the equation as:\n\n with dimensions \nExamples and Intuitions主要介绍，Neural Network用来表现逻辑函数\n逻辑函数\n\nMulticlass Classification\n\n\n\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Neural Network"]},{"title":"08_Backpropagation","url":"/Machine-Learning/08-backpropagation/","content":"这一节，Andrew在这一节，避开了 partial derivative(偏导数)的计算，只给出了比较直观的数学解释。推荐知乎文章：吴恩达机器学习：神经网络 | 反向传播算法详尽地推导了反向传播算法\nCost FunctionDefination\n total number of layers in the network\n number of units(not counting the bias unit) in layer l\n no. of output units(classes)\n\nCost Function of Neural Networks\nBackpropogation AlgorithmFunction BuildingObject\nCost Function J\nPartial Derivative\nAlgorithm公式(1)\n公式(2)\n公式(3)\n==来自以上知乎链接== 帮助更好的理解纠正：最后一行 \nGiven training set {}\n\nSet  for all , (having a matrix full of zeros)For training example :\n\n\nSet \n\nPerform forward propagation to compute  for \n\nUsing , compute \n\nWhere L is our total number of layers and  is the vector of outputs of the activation units for the last layer. So our “error values” for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y.\n\n\nCompute  using \n\nThe delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called g’, or g-prime, which is the derivative of the activation function g evaluated with the input values given by .\n\n\n\nThe g-prime derivative terms can also be written out as:这一项对对的求导\n\n or with vectorization, update the  matrix:\n\n\n, if \n, if \n\nThe capital-delta matrix D is used as an “accumulator“ to add up our values as we go along and eventually compute our partial derivative. Thus we get:\nMathematics intuitionwe Introduce an intermediate variable :\n输出层误差\n\nChain Rule\n当且仅当 , 上式当  是通过偏微分运算和Chain Rule推导出来的，不是简单的两向量相减\n\n隐藏层误差\n\nChain Rule\n神经网络的结点运算求偏导得：\n\n\nChain Relu当且仅当：\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Neural Network","Backpropagation"]},{"title":"09_Notes for Neural Networks in Practice","url":"/Machine-Learning/09-notes-for-neural-networks-in-practice/","content":"back prop as an algorithm has a lot of details and can be a little bit tricky to implement.\nUnrolling ParametersWith neural networks, we are working with sets of matrices:\n\nIn order to use optimizing functions such as “fminunc()”, we will want to “unroll” all the elements and put them into one long vector:\n\nthetavec = [Theta1(:);Theta2(:);Theta3(:)];deltavec = [D1(:);D2(:);D3(:)];\n\n\nIf the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the “unrolled” versions as follows:\n\nTheta1 = reshape(thetaVector(1:110),10,11)Theta2 = reshape(thetaVector(111:220),10,11)Theta3 = reshape(thetaVector(221:231),1,11)\n\n\nGradient Checking使用数值计算，获取偏导数的近似值，来验证反向传播的正确性。\n\nWe can approximate the derivative of our cost function with:With multiple theta matrices, we can approximate the derivative with respect to  as follows:A small value for  (epsilon) such as , guarantees that the math works out properly. If the value for  is too small, we can end up with numerical problems. \n\nImplementationin Matlab or Octave:\nepsilon = 1e-4;for i = 1:n,  thetaPlus = theta;  thetaPlus(i) += epsilon;  thetaMinus = theta;  thetaMinus(i) -= epsilon;  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)end;\n\nCheckcheck that gradApprox  deltaVector. \nStop the computation of gradApproxComputing the gradAppox is rather slower than the backprop algorithm.\nSummary\nRandom Initialization\nInitializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly.\n\nInstead randomly initialize weights for  matrices using the following method:\nRandom initialization: Symmetary breakingInitialize each  to a random value in \nCode ImplementationIf the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n\ninitializing each  to random value between \nrand(x,y) is just a function in octave that will initialize a matrix of random real numbers between 0 and 1.(Note: the epsilon used above is unrelated to the epsilon from Gradient Checking)\nPut it togetherarchitecture setting\nFirst, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have.\n\n\nNumber of input units = dimension of features \nNumber of output units = number of classes\nNumber of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)\nDefaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.\n\nTraining a Neural Network\nRandomly initialize the weights\nImplement forward propagation to get  for any \nImplement the cost function\nImplement backpropagation to compute partial derivatives\nUse gradient checking to confirm that your backpropagation works. Then disable gradient checking.\nUse gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.\n\n\nWhen we perform forward and back propagation, we loop on every training example:\n\nfor i = 1:m,   Perform forward propagation and backpropagation using example (x(i),y(i))   (Get activations a(l) and delta terms d(l) for l = 2,...,L\n\nIdeally, you want . This will minimize our cost function. However, keep in mind that  is not convex and thus we can end up in a local minimum instead.\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Neural Network"]},{"title":"10_Advice for Applying Machine Learning","url":"/Machine-Learning/10-advice-for-applying-machine-learning/","content":"Evaluating a Learning AlgorithmSome trouble shooting for errors:\n\nGetting more training examples\nTrying smaller sets of features\nTrying additional features\nTrying polynomial features\nIncreasing or decreasing λ\n\ntest settingTo tackle the problem of overfittingDivide the data into 2 part: training set and test set (70% and 30%).\ntraining procedure\nLearn  and minimize  using the train set\nCompute the test set error \n\ntest set error\nFor linear regression: \nFor classification ~ Misclassification error (aka 0/1 misclassification error)\n\n  if  or if()\nModel SelectionGiven many models with different polynomial degrees\nWe have a systematic approach to identify the “best” function\nIn order to choose the model of your hypothesis\nBreak Down the dataset\nTraining set: 60%\nCross validation set: 20%\nTest set: 20%\n\nProcedure\nOptimize the parameters in  using the training set for each polynomial degree.\nFind the polynomial degree d with the least error using the cross validation set.\nEstimate the generalization error using the test set with ,(d = theta from polynomial with lower error);\n\nBias and VarianceThis section is talking about the relationship between the degree of polynomial and the underfitting or overfitting hypothesis.\n\ndistinguish whether bias or variance is the problem\nHigh bias is underfitting and high variance is overfitting.\n\n\nHigh bias(underfitting): both  and  will be high.Also, .High variance(overfitting):  will be low and  will be much greater than \nRegularization and Bias/Variance\nas  increases, our fit becomes more rigid.\n\nCreate a list of lambdas ( {0,0.01,0.02,…,5.12,10.24})\nCreate a set of models with different degrees or any other variants\nIterate through the s and for each  go through all the models to learn some \nCompute the cross validation error using the learned (computed with ) on the  without regulariztion or \nSelect the best combo\nUsing the best combo  and , apply it on  to see if it has a good generalization of the problem.\n\nNote: the detail of the Step 4 is quite significant\nLearning CurvesThe X axis is m which means the size of training set.The Y axis is  of test set or train set.\nHigh Bias\nLow training size:causes  to be low and  to be high.\nLarge training size:causes both  and  to be high with \nHigh Variance\nLow training size: will be low and  will be high.\nHigh training size: increases with training set size and  continues to decrease without leveling off.Also,  but the difference between the two curves remains significant.\nReviewProblem ShootingAs follows:\n\nGetting more training examples: Fixes high variance\nTrying smaller sets of features: Fixes high variance\nAdding features: Fixes high bias\nAdding polynomial features: Fixes high bias\nDecreasing : Fixes high bias\nIncreasing : Fixes high variance.\n\nNeural NetworksFewer Parameters\nprone to underfitting\ncomputationally cheap\nHigh bias and low variance\n\nMore Parameters\nprone to overfitting\ncomputationally expensive\nLow bias and high variance\n\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes"]},{"title":"11_Machine Learning System Design","url":"/Machine-Learning/11-machine-learning-system-design/","content":"we take building a Spam Classifer as example.\nThese contents below is a little disjointed.\nPrioritizing What to work on\nImprove the accuracy of the classifier\nCollect lots of data(For example “honeypot” project but doesn’t always work)\nDevelop sophisticated features(using email header data in spam emails)\nDevelop algorithms to process ur input in different ways(recognizing misspellings in spam).\n\nError AnalysisRecommended approach\nStart with a simple algorithm which can be implement quickly and test it on cross-validation data.\nPlot learning curves to decide if more data, more features, etc.\nError analysis: Manually examine the examples(in cross validation set). See if ur spot any systematic trend in what type of examples it is making errors on.\n\nError Analysis\nNumerical Evaluation\n\nIt is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm’s performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature.  Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not. \n\nError metrics for skewed classesSkewed ClassesThere are a gap between the two proportions of 2 classes.\nError Metrics\nPrecision(Of all patients where we predicted y=1 what fraction actually has cancer?)\nRecall(Of all patients that actually have cancer, what fraction did we correctly detect as having cancer?)\nTrade off\nLogistic Regression as Example\n\nif the threshold change, the predic(P) and recall(R) value will also change.s\n\n score\nData for machine learning“It’s not who has the best algorithm that wins.It’s who has the most data.”\n\nLarge data rationale\nUse a learning algorithm with many parameters\nUse a very large training set(unlikely to overfit)\n\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes"]},{"title":"12_Support Vector Machine","url":"/Machine-Learning/12-support-vector-machine/","content":"Large Margin ClassificationSVM’s Cost Function\nSupport Vector Machines take the magenta line to replace the Sigmoid term.\n\nThis give SVM computational advantages and user an easier optimization problem.\n\nOptimization Objective\nLogistic regression\nSupport vector machine\n\n\nHypothesis\nSVM Decision Boundary: Linearly separable case\nFor the Linear separable case, SVM will choose the line(black one) which has a larges distance(the margin)\n\nThe distance is called the margin of the Support vector machine and this gives the SVM a certain robustness, because it tries to separate the data with as a large a margin as possible.\n\nThe Parameter C\nif  was large, the classifier will be more sensitive.\n plays a role similar to one over Lambda \n\nMathemativs Behind Largr Margin Classificationnorm of a vectorThe norm: means the norm of , it’s also means the euclidean length of the vector \nif the vector is 2-dim. Norm’s calculate method is as same as the Pythagoras theorem\nDerivationif the  and  all be zeros\nthe Optimization Objective will change as the following form:There are another relationship inside:where  is the projection of  onto the vector .Simplification: \n\n\nthe whole derivation is using the simplification that the parameter  i equal to zeroIt turns out that this same large margin proof works in pretty much in exactly the same way. \n\nKernelswhen we are treating Non-linear Decision Boundary, we need to choose different features.One way to have the features is called Kernel Method.\nGiven x, compute new feature depending on proximity to landmarks.\n\n\nEach of the landmark defines a new feature \nSimilarity FunctionSimilarity Function is a way to measure an examples’ proximity to a landmark. The function above is so-called Gaussian Kernel\nIf If  if far from \n\n is a hyperparameter.\nmore detailsLandmarks Choiceall the examples’s feature will be taken to be landmarks. So the no. of ’s dims is as same as the features ’s and the no. of training set.\nGiven ,choose .\nGiven example \nFor training example\n\nBias-Variance Trade OffSVM with kernelHypothesis: Given , compute features 𝕞𝟙Predict “y=1” if Training:\nSVM parameters:C( = 1 over lambda).Large C: Lower bias, higher variance. (small )Small C: Higher bias, lower variance. (large )\n sigma squareLarge : Features  vary more smoothly.Higher bias, lower variance.\nSmall : Features  vary less smoothly.Lower bias, higer variance.\nSVMs in PracticeActually we seldomly implement ourselves but the the off-the-shelf function libs(e.g. liblinear, libsvm,…)\nFirst stepchoose a kind of good software libraries for your propagramming.\nAnd than choose the parameter C\nNeed to specify:\n\nChoice of parameter C\nChoice of kernel(similarity function):\n\nE.G. No kernel(“linear kernel”) - predict “y=1” if \nSecond StepChoose the kernel or the similarity function that u wanna use.\nGaussian kernel:Need to choose \nSome packet need u to implement the Kernel(similarity) functions:\nfunction f = kernel(x1, x2)    f = ...return \n\nNoteif features take on very different ranges of value.Do perform feature scaling using the Gaussian kernel.\nOther KernelThe Linear Kernel and Gaussian Kernel are 2 most common Similarity Function.\nNOT all similarity functions  mak valid kernels.(Need to satisfy technical condition called “Mercer’s Theorem” to make sure SVM packages’s optimizations run correctly, and do not diverge)\nMercer’s Theorem: 任何半正定的函数都可以作为核函数。\nMany off-the-shelf kernels available:\n\nPolynomial kernels\nMore esoteric: String kernel, chi-square kernel, histogram intersection kernel….\n\nMulti-class\nMany SVM packages already have built-in multi-class classification functionality,\nOtherwise, one-vs.-all method\n\nLogistic Regression vs. SVMsn = no. of features (𝕟𝟙),m = no. of training examples\nn is largerelative to m\nE.g. \nUsing logistic regression, or SVM without a kernel(“linear kernel”)\nn small, m intermediateE.g. \nUse SVM with Gaussian kernel\nn small, m largeE.g. Create/add more features, then use logistic regression or SVM without akernel\n\nNeural network likely to work well for most of these settings, but may be slower to train.\n\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Support Vector Machine"]},{"title":"13_K-means and Principal Component Analysis","url":"/Machine-Learning/13-k-means-and-principal-component-analysis/","content":"Clusteringgiven unlabeled dataset and we would like to have an algorithm automatically group the data into coherent subsets\nK-meansK-means is an iterative algorithm and it does two things\n\nFirst, is a cluster assignment step.\nSecond is a moving centroid step.\n\ncentroid: n. (数)形心\n\nK-means algorithm is by far the most popular by far the most widely used clustering algorithm.\n\nAlgorithmInput:\n\n\n {}\n\nRandomly initialize  cluster centroids Repeat{ for  to  index(from 1 to ) of cluster centroid closest to  for  to  average(mean) of points assigned to cluster }\nNote\nFor the First Step, assign the data into  group according the distance to centroid.\nThe other part(Step 2), move the centroid step. If there is a cluster centroid with zero points to it . In that case, the most common thing to do is just eliminate if u can accept a  dim output\nOtherwise, randomly reinitidized.\n\nOptimization Objectivedefinition\nOptimization objectivethe optimization objective is also called distortion function or the distortion of the K-means algorithm\n\nRandomly Initialize\nShould have \nRandomly pick  training examples\nSet  equal to these  examples.\n\nTo avoid K-means has gotten stuck to the local optima, the solution is trying multiple, randomly initializations.(run hundred times)\nFor 1 to 100{.}\nPick clustering that gave lowest cost \nChoosing KElbow method\nVary K(from 1) and compute the distortion J, and choose the turning point(like humans’ elbow)\nThere is a situation with no clear elbow.\n\nElbow method is worth the shot but not necessarily have a very high expection of its working for any particular problem.\nOther wayThinking about the purposed of downstream, or other way to set the parameter K. \nDimensionality ReductionThere are two motivations for Dimensionality Reduction\ndata compression\nuse up less computer memory.\nspeed up running time of learning algorithm.\n\n\nVisualizationReduce from n-dimension to k-dimension: Find  vectors  onto which to project the data, so as to minimize the projection error.\nPrincipal Component Analysis (PCA)by far the most popular algorithm. This section will present the problem formulation for PCA\nWhat PCA does formally is it tries to find a lower dimensional surface.\nThe goal of PCA is finding a direction(a vector () onto which to project the data so as to minimize the projection error.\nDifference with Linear Regression\n\nfor linear regression, sum the vertical error\nBut PCA, accumlate the distance to the line.\n\nAlgorithmTraining set: \nPreprocessing(feature scaling/mean normalization):Replace each  with If different features on different scales(e.g. ), scaling features to have comparable range of values.\nMain processReduce data from dimensions to dimentsionsCompute “covariance matrix”:Compute “eigenvectors“ of matrix :The plan B to compute the eigenvectors\neig(Sigma)\n\nSummary codeSigma = (1/m) * X' * X;[U,S,V] = svd(Sigma);Ureduce = U(:, 1:k);z = Ureduce'*x;\n\nReconstruction from PCA\n\nChoosing k ( No.of principal components)Average squared projection error: Total variation in the data: \nTypically, choose  to be the smallest value so that“99% of variance is retained”\n\n[U, S, V] = svd(Sigma)\nPick the smallest value of  for which(99% of variance is retained)\nAdviceNote:the Mapping  should be defined by running PCA only on training set. This mapping can be applied as well to examples  and  in the cross validation and test sets.\nBad use of PCATo prevent overfittingUse  instead of  to reduce the number of features to .Thus, fewer features, less likely to overfit.\nThis might work, but isn’t a good way to address overfitting.Use regularization instead:\n\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Unsupervised Learning"]},{"title":"14_Anomaly Detection","url":"/Machine-Learning/14-anomaly-detection/","content":"Anomaly DetectionDensity estimationDataset: {}Is  anomalous?\n\n.\nApplication\nFraud detection\n\nManufacturing\n\nMonitoring computers in a data center.\n\n\nGaussian Distributionaka Normal DistributionSay . If  is a distributed Gaussian with mean , variance \n\n\nAlgorithm\nChoosing features  that you think might be indicative of anomalous examples.\n\nFit parameters \n\nGiven new example , compute \n\n\nAlgorithm evaluationFit model  on training set{}On a cross validation/test example , predict\n\nPossible evaluation metrics:    - True positive, false positive, false negative, true negative    - Precision/Recall    - Can also use cross validation set to choose parameter \n\nChoosing featuresFor non-gaussian features, take some type of transform to make the histogram looks much more GaussianError analysis for anomaly detectionWant  large for normal examples . small for anomalous examples .\nMost common problem:\n\nNote: if the Anomalous Detection Algorithm can’t distinguish the anomalous or non-anomalous examples, it’s useful that coming up with more features to do that.\nAnomaly Detection vs. Supervised LearningAnomaly Detection\nVery small number of positive examples.(0-20 is common).\nLarge number of negtive(y=0) examples.\nMany different “types” of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like;\nfuture anomalies may look nothing like any of the anomalou examples we’ve seen so far\n\nSupervised learning\nLarge number of positive and negative examples.\nEnough positive examples for algorithm to get a sense of what positive examples are like\nfuture positive examples likely to be similar to ones in training set.\n\nMultivariate Gaussian Distribution𝕟\n\nSigma and contour\n\n\n\nOriginal model vs. Multivariate VersionOriginal Model\n\nManualyy create features to capture anomalies where  take unusual combinations of values.\nComputational cheaper (alternatively, scales better to larbge  E.g. n=10000,100000)\nOk even if (training set size) is small\n\nMultivariate Version\n\nAutomatically captures correlations between features\nComputationally more expensive \nMust have  or else  is non-invertible.\n\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Unsupervised Learning"]},{"title":"15_Recommender systems","url":"/Machine-Learning/15-recommender-systems/","content":"Recommender systemswhat recommender system do is looking at users and predict how they would have rated other movies that they have not yet rated.\nMotivation(Reasons\nimportant application of machine learning.\nit’s a typical big think of machine learning\n\nProblem FormlaitonOther notation\nContent Based\nFor each user , learn a parameter . Predict user  as rating movie  with  stars.\nTo learn :\nFor convenience, we are going to get rid of the term  without changing the value of theta  that it can be taken out of the optimization.\nOptimizaition objective:To learn  (parameter for user ):To learn :\nGradient descent update:\nCollaborative Filteringfeature learningGiven , to learn :\nGiven ,to learn :\nAlgorithmMinimizing  and  simultaneously:\n\nInitialize  to small random values.\nMinimize  using gradient descent( or an advanced optimization algorithm).E.g for every :\n\n\n\nFor a user with parameters  and a movie with(learned) features , predict a star rating of .\n\nVectorization： Low rank matrix factorization\nMean normalizationIf there is a user who haven’t rate any item before.The Collaborative Filtering Algorithm will predict all the user’s rated score ZERO. So the idea of mean normalization will fix the problem.\n\nFinding related moviesFor each product , we learn a feature vector .Some features are over human being’s comprehensive ability\nHow to find movies  related to movie \n5 most similar movies to movie Find the 5 movies  with the smallest \n后记协同滤波算法的一大特点就是，学习一个多用户多目标的大系统。 不适合单个用户的喜好推荐（大材小用）协同滤波是一种学到的第一个半监督算法\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Semi-supervised Learning","Collaborative Filtering"]},{"title":"17_Problem Description and Pipeline","url":"/Machine-Learning/17-problem-description-and-pipeline/","content":"这边一章的内容，靠图展示比较强推荐CSDN链接： https://blog.csdn.net/CodingRae/article/details/103985629\nPhoto OCRPhoto OCR means Photo Optical Character Recognition.\nPhoto OCR pipline\nText detction, go through the image and find the regions where there’s text and image.\nCharacter segmentation, given the rectangle around that text region\nCharacter classification, having segmented out into individual characters\n\n\n\nIn many complex machine learning systems, these sorts of pipelines are common, where u can have multiple modules. each of which may be machine learning component, or sometimes it may not be a machine learning component but to have a set of modules that act one after another on some piece of data in order to produce the output you want.\n\nDesign the pipeline: give a problem, break the problem down into a sequence of different modules. \nSliding windowsPedestrian Detection\n\n\n\ntrain a classifer to recognize the pedestrians in the image.\nSlide different size of windows to find region of pedestrians in the image.\n\nText Detection\ntrain a character classifer to recognize the characters.\n\nAfter getting the region of single characters, using an expansion to get the region of strings.\n\nSummary\nGetting lots of Data and Artificial Databasesthere is a fascinating idea called artificial data synthesis.\nArtificial data synthesis for photo OCR\n\nTake characters from diffrent fontspaste these characters against different random backgrounds.\n\nSynthesizing data by introducing distortions\n\noverlayed picture with the grid lines just for the purpose of illustration.\ntake the image and introduce artificial warpings (artificial distortions)\n\nDistortion introduced should be representation of the type of noise/distortions in the test set.Usually does not help to add purely random/meaningless noise to your data.\nDicussion on getting more data\nMake sure you have a low bias classifier before expending the effort.(Plot learning curves). E.g. keep increasing the number of features/number of hidden units in neural network until you have a low bias classifier.\n“How much work would it be to get 10x as much data as we currently have?”\n\n\nArtificial data synthesis\nCollect/label it yourself\n“Crowd source”(E.g. Amazon Mechanical Turk)\n\nCeiling AnalysisWhat Part of the Pipline to Work on Next\n\nCeiling Analysis can sometimes give you a very strong signal, a very strong guidance on what parts of the pipeline might be the best use of your time to work on.\n\nEstimating the errors due to each component(ceiling analysis)\nThough Ceiling Analysis we can realize what’s the most promising components.\nThe table above is the centrel of method。\nCoursera SummaryMain topics\nU ARE an Expect\nThank u Andrew Ng\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Photo OCR","Artifical Example"]},{"title":"01_Hung-yi Lee_Machine Learning","url":"/Machine-Learning/01-hung-yi-lee-machine-learning/","content":"写在前面之前已经做过 Andrew Ng 的 CS229了，但是那门课，已经算是比较古早了（2016），课程的作业设计虽然做的非常精致，但基于Matlab语言编写的。现在主流的科研和开发语言，已经是python了。还是离实际算法落地，有一段距离。 CS229(2016)，更多是练习写 toy model，但过了一遍一些机器学习的理论，感觉还是很不错。这门课就当，强化巩固。\nThis course focuses on Deep Learning\n课程网站： ML 2022 spring课程Github：https://github.com/virginiakm1988/ML2022-SpringThe repository contains code and slides of 15 homeworks for Machine Learning instructed by Hung-yi Lee.Prof. Lee 把15节课程的录播都已经上传，在油管上会周更一些新的内容。这边基于2022版给予的课程录像，进行学习。加油。\nIntro of the coursesHW1: COVID-19 Case PredictionHW2: Phoneme ClassificationHW3: Image ClassificationHW4: Speaker ClassificationHW5: Machine TranslationHW6: Anime Face Generation\n\nLecture 7: Self-supervised Learning\n用引擎上爬下来的众多unlabel的图片来 pretrain，来获得更好的训练效果\nPre-trained Model(aka. Foundation Model) vs Downstream Tasks\n\nLecture 6: GAN\n\nLecture 12: Reinforcement Learning (RL\nLecture 8: Anomaly Detection\nLecture 9: Explainable AI\n\nLecture 10: Model Attack\nLecture 11: Domain Adaptation\nLecture 12: Network Compression\n\nLecture 13: Life-long Learning\nLecture 14: Meta Learning\n\nFew-shot learning is usually achieved by meta-learning.\nMachine LearningAngdrew: 一种训练机器的隐式编程。Hung-yi: Machine Learning  Looking for Function.\nDifferent types of FunctionsRegressionThe func outputs a scalar.\nClassificationGiven options(classes), the func outputs the correct one.\nStructured Learningcreate something with structure(image, document)\nPipelineFunction with Unknown Parameters\n: no. of views on 2/26, : no. of views on 2/25 and  are unknown parameters(learned from data) weight and bias vector of features.\nDefine Loss from Training DataLoss is a func of parameters , to measure how good a set of values is.\n例子：频道人数预测（用预测日期  前的流量数据作为输入预测误差，label 和 predic结果之差的一个函数if  and  are both probability distributions  Cross-entropy\nOptimization\n这门课唯一涉及的方法： Gradient Descent\n\n(Randomly) Pick an initial value \nCompute hyperparameter: the parameters given by human being.\nUpdate  iteratively\n\nGradient Descent 有个问题，就是会收敛到 Local minima\n\n对模型的修改，往往都来自，对于模型都理解（domain knowledge）\nNeural NetworkLinear models have severe limitation. Model Bias, so we need more sophisticated models.\nPiecewise Linear Curves\nContinuous curve can be approximated by a piecewise linear curve, need sufficient pieces.\n\nSigmoid\n\n上面的蓝色Function，叫作 Hard Sigmoid\n\n\nVectorization: featureUnknown parameters: \nupdate ML pipeline\nfunc with unknown\nLoss func\n\n\nLoss is a func of parameters \nLoss means how good a set of values is.\n\n\n\nOptimization\n\n\n(Randomly) Pick initial values \ncompute gradient:\ncompute gradient:\ncompute gradient:\n\n is the Loss func computed from 1st Batch, and then update the parameter.1 epoch = see all the batches once.1 update = update the parameters once.\nRectified Linear UnitRectified Linear Unit(ReLU)两个特定的ReLU可以生成一个 sigmoid\n\nSigmoid and ReLU r named as Activation func\n\n\n\n\nlinear\n10 ReLU\n100 ReLU\n\n\n\n2017-2020\n0.32k\n0.32k\n0.28k\n\n\n2021\n0.46k\n0.45k\n0.43k\n\n\nMultiple Layer\n\nLoss for multiple hidden layers\n100 ReLU for each layer\ninput features are the no. of views in the past 56 days\n\n\n\n\n\n\n\n1 layer\n2 layer\n3 layer\n\n\n\n2017-2020\n0.28k\n0.18k\n0.14k\n\n\n2021\n0.43k\n0.39k\n0.38k\n\n\nDeep LearningDeep Learning 可以替代 Feature Engineering\nFully Connect Feedforward NetworkGiven network structure, define a function set\n\n当我们写 Neural Network 的式子的时候，一般会把它写成矩阵运算的形式，方便用GPU加速。\nHidden Layers are seen as Feature extractor replacing feature engineering.\n一般在做 Neural Network 的时候，Output Layer will be Softmax to implement Multi-class Classifier.\nselect no. of layers\nQ: How many layers? How many neurons for each layer?\n\nQ: Can the structure be automatically determined?\n\nE.g. Evolutionary Artificial Neural Networks\n\n\n\nUniversality TheoremDeep is better?Any continuous function fCan be realized by a network with one hidden layer (given enough hidden neurons)\nBackpropagation之前在CS229的时候，做过BP的推算，见文章https://carp2i.github.io/2022/01/10/ML08/\nforward passCompute  for all parameters\nBackpropagationCompute  for all activation func inputs z\n\n\n\n\n\nRegressionEstimating the CP of a pokemonCP: the Combat Power\n\nStep1 ModelLinear model: \nStep2 Goodness of Function\nTraining Data: 10pokemons \nLoss func :Loss function是function的functionInput: a func, output: how bad it is\n\nSum over examples\nEstimated  based on input function\n\nStep3 Best FunctionWhat we really care about is the error on new data(testing data)\nSelecting another Model\n\nBest Function\nTesting:\n\nIf the initial function set perform badly, u should come back to step 1 to Redesign the Model\nRedesign\n如果你是大木博士的话，会有很多的domain knowlege,所以只能将所有参数都放入model\nRegularization\nQ: why smooth functions are preferred?A: If some noises corrupt input  when testing, a smoother function has less influence.\n\nTraining error: larger lambda, considering the training error lessWe prefer smooth function, but don’t be too smooth.\n\nSelect  obtaining the best functionwhen u are preparing for regularization, the bias shouldn’t be taken into account.\nClassificationPokemon Classification\n\nTotal：sum of all stats that come after this, a general guide to how strong a pokemon is\nHP: hit points, or health, defines how much damage a pokemon can withstand before fainting\nAttack: the base modifier for normal attacks(eg. Scratch, Punch)\nDefense: the base damage resistance against normal attacks\nSP Atk: special attack, the base modifier for special attacks(e.g. fire blast, bubble beam)\nSP Def: the base damage resistance against special attacks\nSpeed: determines which pokemon attacks first each round\n\nHow to do Classification\nTraining data for ClassificationClassification as Regression?Binary classification as exampleTraining: Class 1 means the target is 1; Class 2 means the target is -1Testing: closer to 1  class 1; closer to -1  class 2\n\n用Regression的方法来做二元分类，会惩罚那些“too correct” result\nIdeal Alternatives\nFunc(Model):\n\nLoss functions:The number of times  get incorrect results on training data.\n\nFind the best function:\n\nExample: perceptron, SVM (classic way)\n\n\n\nGaussian Distribution一般认为 Pokemon 的属性值遵循正态分布（高斯分布\nInput: vector x, output: probability of sampling xThe shape of the function determines by mean  and covariance matrix \nAssume the points are sampled from a Gaussian distributionFind the Gaussian distribution behind them  Probability for new points\n\nMaximum Likelihood\nThe Gaussian with any mean  and covariance matrix  can generate these points.   Different Likelihood\nLikelihood of a Gaussian with mean  and covariance matrix \n\nWe have 79 sample We assume  generate from the Gaussian  with the maximum likelihood\n\nIf \n\n\n做完发现，就算用了全部7个 features，预测的结果还是很差\nModifying Model\n其实不常看到，每个func都有自己的 means 与 covariancecovariance matrix 的参数数量是 feature 数量的平方，较大的特征向量会使得 model的参数很多，更加容易 overfitting\n\n\nMaximum likelihoodFind  maximizing the likelihood \n\n &amp;  is the same     \nif the boundary is linear, we seem model as linear model\nRecall\n\nif u assume all the dimensions are independent then you are using Naive Bayes Classifier.\nPosterior Probability\n\n一波操作，最终化简结果：\n\n\nIn generative model, weestimate \n","categories":["Machine Learning"],"tags":["Courses Notes","Hung-yi Lee"]},{"title":"16_Learning with Large Scale Dataset","url":"/Machine-Learning/16-learning-with-large-scale-dataset/","content":"Stochastic Gradient Descent\nwe have a very large training set, gradient descent becomes a computationally very expensive procedure.\n\nAlgorithm\n\nRandomly shuffle dataset.Repeat { {}}\n\nTipsSGD Algorithm is much more faster than Batch Gradient Descent when we have large scale dataset.\nLearning rate  is typically held constant. Can slowly decrease  over time if we want  to converge. (E.g. )\nMini-Batch Gradient Descent\nBatch gradient descent: Use all m examples in each iteration\nStochastic gradient descent: Use 1 example in each iteration\nMini-Batch gradient descent: Use b examples in each iteration\n\nAlgorithmSay .Repeat{{}}\nTips小批量梯度下降，是介于 批梯度下降 和 随机梯度下降之间的方法。\nChecking for convergencePlot , averaged over the last 1000(say) examples\nAdvanced TopicsOnline LearningRepeat forever { Get  corresponding to user. Update  using }\n\nget an example\nlearn the example\ndiscard the example\n\nProduct search(learning to search)\nUser searches for \"Android phone 1080p camera\"\nHave 100 phones in stores. Will return 10 results.\nx = features of phone, how many words in user query match name of phone, how many words in query match description of phone, etc.\n\n if user clicks on linke.  otherwise.Learn Use to show user the 10 phones they’re most likely to click on.\nOther examples: Choosing special offers to show user; customized selection of news articles product recommendation;\nMap Reduce\n\nMany learning algorithms can be epressed as computing sums of functions over the training set.\n\nE.g. for advanced optimization, with logistic regression, need:\n","categories":["Machine Learning"],"tags":["CS229 (Andrew Ng)","Courses Notes","Gradient Descent","Large Scale Dataset","Map Reduce"]},{"title":"02_Hung-yi Lee_Pokemon Classification & strategy","url":"/Machine-Learning/02-hung-yi-lee-pokemon-classification-strategy/","content":"Pokemon vs. Digimon\nFunction with unknown Parameters\n： number of candidate functions(model “complexity”)\nLoss of a func(given data)\nGiven a dataset \nLoss of a threshold  given data set \n\n\nTraining Examples\nIf we can collect all Pokemons and Digimons in the universe , we can find the best threshold \nWe only collect exampels  from \n\n\n\n\n\nif we can collect all Pokemons and Digimons in the universe , we can find the best threshold $$h^{all} = \\arg \\min\\limits h L(h, \\mathcal{D}{all}) \\qquad \\text{理想}$$\nWe only collect some examples  from 现实\n\nwe hope  and  are close.\nNote: can be smaller than \n\n\nmodel-agnostic\ndon’t have assumption about data distribution\nany loss function can be used\n\n\n\n\n\nHoeffding’s Inequality:\n\nThe range of loss  is [0, 1]\n is the number of examples in \n\nTo make P smaller:  Larger  and smaller What if the parameters are continuous?\n\nEverything that happens in a computer is discrete.\nVC-dimension(not this course)\n\nTradeoff of Model Complexity\nStrategyFramework of MLTraining data: \nTesting data: \npipeline\nGeneral Guide\nSplit ur training data into training set and validation set for model selection\nOptimization issue\nGaining the insights from comparision\n\n\n这是 Residuals Network 论文上的结果并不是overfitting，这代表着 56-layer的Optimization 并没有做好。56-layer 的network，一定可以做到 20-layer 的泛化能力\n\nStart from shallower networks(or other models which are easier to optimize)\n\nIf deeper networks do not obtain smaller loss on training data, then there is optimization issue.\n\nSolution: More powerful optimization technology.\n\n\nOverfitting\nSmall loss on training data, large loss on testing data\n\n\nData augmentation\n\nAugementation要有道理，一般不会将图像上下颠倒\n\nconstrained model\n\nBias-Complexity Trade-off\nCross Validation\n过多的利用 Public Testing Set 去 select model，会使得模型很容易在公开测试集上过拟合。 因此不太推荐。\nN-flod Cross Validation适用于小模型\nmismatch\nYour training and testing data have different distributions.\nMost HWs do not have this problem, except HW11\n\n\n","categories":["Machine Learning"],"tags":["Courses Notes","Hung-yi Lee"]},{"title":"03_Hung-yi Lee_What to do if my network fails to train1","url":"/Machine-Learning/03-hung-yi-lee-what-to-do-if-my-network-fails-to-train1/","content":"Local minima &amp; Saddle pointThe points with zero gradient is called critical point\nLocal minima局部最小值，梯度为0，周围的点都比minima大。\nSaddle point鞍点，梯度为0，周围的点，有部分比该点的值大，另一部分比该点的值小。\nTayler Series Approximation around  can be approximated belowGradient g is a vector\nHessian  is a matrix\nHessian\nAt critical point:\nFor all = H is positive definite = All eigen values are positive\nFor all = H is negative definite = All eigen values are negative.\n    Some eigen values are positive, and some are negative.\nDon’t afraid of saddle point\n\nSaddle Point v.s. Local Minima一般我们的模型纬度非常的高，很少会遇到 Local Minima 的情况\n但是会停滞在 plateau\n\nBatch &amp; Momentum\n\n1 epoch = see all the batches once  Shuffle after each epoch\nSmall Batch v.s. Large Batchconsider 20 examples(N = 20)Batch size = N(Full batch)Update after seeing all the 20 examplesBatch size = 1Update for each example, Upadate 20 times in an epoch.\n\nLarger batch size does not require longer time to compute gradient(unless batch size is too large)\n\n\n\nSmaller batch requires longer time for one epoch(longer time for seeing all data once)\n\n\n\n\n\nSmaller batch size has better performanceWhat’s wrong with large batch size? Optimization Fails“Noisy: update is better for training\n\nSmall Batch 可以避免更新陷入 critical point\nOverfitting ProblemOn Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima\n\n\n\n论文解释说： test set 的分布跟 train set 很不一样，在 Sharp Minima 中 test loss 会很大，而小的 batch 不容易被 Sharp Minima 困住。\n\n\nBatch-size is a hyperparameter u have to decide.\nReference of Large Batch-size Training\nLarge Batch Optimization for Deep Learning: Training BERT in 76 minutes(https://arxiv.org/abs/1904.00962)\nExtremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes(http://arxiv.org/abs/1711.04325)\nStochastic Weight Averaging in Parallel: Large-Batch Training That generalizes Well(https://arxiv.org/abs/2001.02312)\nLarge Batch Training of Convolutional Networks(http://arxiv.org/abs/1708.03888)\nAccurate, large minibatch sgd: Training imagenet in 1 hour(https://arxiv.org/abs/1706.02677)\n\nMomentumGradient Descent + Momentum一种模拟物理情景的优化器设计\nMovement: movement of last step minus gradient at presentMovement not just based on gradient, but previous movement.\n\n is the weighted sum of all the previous gradient: \nConcluding Remarks\nCritical points have zero gradients.\nCritical points can be either saddle points or local minima.\nCan be determined by the Hessian matrix.\nIt is possible to escape saddle points along the direction of eigenvectors of the Hessian matrix.\nLocal minima may be rare.\n\n\nSmaller batch size and momentum help escape critical points.\n\nError surface is ruggedTips for training: Adaptive Learning Rate\nTraining stuck\n一般实验中，critical point 往往不是问题，魔王往往是其他问题\n\nTraining stuck  Small Gradient\n\nPeople believe training stuck because the parameters are around a critical point…\n\n\nTraining can be difficult even without critical points\n\nAdaptive Learning rateFormulation for one parameter:\n\nAdagrad\n\n\n同一方向的学习率不能动态改变\nRMSProp找不到论文出自哪里，Hinton在Coursera上开了 Deep learning 的课程，在上面讲过RMS Prop\n\n\n\nThe recent gradient has larger influence, and the past gradients have less influence.\n\nAdam: RMSProp + Momentum\nLearning Rate SchedulingLearning Rate DecayAs the training goes, we are closer to the destination, so we reduce the learning rate.\nWarm Up\n出现在 \n\nResidual Networks https://arxiv.org/abs/1512.03385\nTransformer https://arxiv.org/abs/1706.03762\n\n\nPlease refer to RAdam https://arxiv.org/abs/1908.03265\nSummary of Optimization(Vanilla) Gradient DescentVarious Improvements\n\n","categories":["Machine Learning"],"tags":["Courses Notes","Neural Network","Hung-yi Lee"]},{"title":"04_Hung-yi Lee_What to do if my network fails to train2","url":"/Machine-Learning/04-hung-yi-lee-what-to-do-if-my-network-fails-to-train2/","content":"Classification as Regression\nRegression\nClassification as regression\n\nClass as one-hot vector避免不同类，被认为是相互临近关系。比如：Class 1 = 1， Class 2 = 2, Class 3 = 3。这种情况，容易被认为 Class 1 与 Class 2更加临近，与Class 3 没那么相似\n所以使用： one-hot encode但是存在缺点，就是编码冗余较大\n\n\nRegression\n\n\n\nClassification\n\n\nSoftmax\n\n\n\n\nLoss of Classification\n\nMean Square Error(MSE)   Cross-entropy    \n","categories":["Machine Learning"],"tags":["Courses Notes","Classification","Neural Network","Hung-yi Lee"]},{"title":"05_Hung-yi Lee_Convolutional Neural Networks(CNN)","url":"/Machine-Learning/05-hung-yi-lee-convolutional-neural-networks-cnn/","content":"CNN一种常用的神经网络框架，被用于 图像识别，影像辨识。\nImage Classification图形预处理\n图像识别系统 处理的问题一般会有不同的scale，一般的做法是：  将输入的，要处理的图像，～rescale～resize为统一的、要处理的图像大小。All the images to be classified have the same size.\nOutput 和 Label： one-hot vector\nCriterion： Cross-entropy\n\n\n图像数据组织一般图像数据会被表示为（high  width  channels)\n\nObservation\nwe needn’t identify the full picture.\nWe just need identify some critical patterns.Some patterns are much smaller than the whole image.\n\n\nSome patterns appear in different regions\nSubsampling the pixels will not change the object(下采样不会改变图像表征的物体)\n\nSimplificationReceptive field感受野，是一个定义 Neuron（“神经元”） 感知范围的一个概念。每个神经元只感受自己的感受野的内容。\n\n一些精心设计的 Neural Network 会有 Cover only some channels 的设计（这种情况特殊处理，只有pattern出现在某些channel的情况下）\n最经典的 Receptive field 的安排：会看全部channel\ncover all the channels 后，会感受野只剩下两个参数，high&amp;width，这俩属性共同决定： kernel size\n一般一个 receptive field 会有一组 Neuron 来感知。\n\n\n这里的几个定义的参数，其实也就是conv()函数的传入参数。\n\nkernel size、stride 会影响生成“图像”的纬度\npadding 关乎越界情况下的填充处理，只会影响生成的值\n\nparameter sharing不同位置，同样pattern的感知使用的参数不同。两个 Neuron 相同。\n\nTwo Neurons with the same receptive field has a set of neurons\n\n\nEach receptive field has a set of neurons (e.g., 64 neurons)\nEach receptive field has the neurons with the same set of parameters\n\nPooling（池化解决图像的缩放的感知问题\nMax Pooling下采样过程中，感受野里选最大的\nMean Pooling选均值\nConvolutional Layers + Poooling\n实践中，我们常常在 卷积层 后跟上一层 pooling，Pooling 做的事情就是把图片变小。 \n近两年的论文设计中，很多模型都抛弃了 Pooling（运算资源够多，支撑不做 pooling\n但是在近期，图像识别的论文中，出现了大量 Full-convolution 的网络。\n常见CNN\nBenefit of Convolutional Layer\n\nsome patterns are much smaller than the whole image.\nthe same patterns appear in diff regions\n\nfrom Filter\n\nThe values in the filters are unknown parameters.\nthe output of the filters are called Feature Map\n\nMutiple Convolutional Layers\n\nthe 2 version introduction\n\n\nNeron Version\nFilter Version\n\n\n\nEach neuron only considers a receptive field\nThere are a set of filters detecting small patterns\n\n\nThe neurons with different receptive fields share the parameters.\nEach filter convolves over the input image.\n\n\nThey are same story.\nApplication:Alpha Go把棋盘当成图片\nsimilarity between Go playing and Image\nSome patterns are much smaller than the whole image\n\nThe same patterns appear in different regions\n\n\n\n是否使用 Pooling 还得看 问题的性质是否适合池化，下围棋不可能下采样\nMore Applications\nHW3: Image ClassificationObjective\nSolve image classification with convolutional neural networks.\nImprove the performance with data augmentations.\nUnderstand popular image model techniques such as residual.\n\nTricksModel Selection\nvisit torchvision.models for a list of model structures, or go to timm\nIf Pretrained weights are not allowed, specially set pretrained = False\n\nData Augmentation\nModify the image data so non-identical inputs are given to the model each epoch, to prevent overfitting of the model\nVist torchvision.transforms for a list of choices and their corresponding effect. Diversity is encouraged! Usually, stacking multiple transformations leads to better results.\nCoding: fill in train_tfm to gain this effect\n\n# Normally, We don't need augmentations in testing and validation.# All we need here is to resize the PIL image and transform it into Tensor.test_tfm = transforms.Compose([    transforms.Resize((128, 128)),    transforms.ToTensor(),])# However, it is also possible to use augmentation in the testing phase.# You may use train_tfm to produce a variety of images and then test using ensemble methodstrain_tfm = transforms.Compose([    # Resize the image into a fixed shape (height = width = 128)    transforms.Resize((128, 128)),    # You may add some transforms here.    # ToTensor() should be the last one of the transforms.    transforms.ToTensor(),])\n\n\nAdvanced Data Augmentationmixup\n\nCoding:\n\nIn your torch.utils.Dataset, getitem() needs to return an image that is the linear combination of two images/\nIn your torch.utils.Dataset, getitem() needs to return a label that is a vector, to assign probabilities to each class.\nYou need to explicitly code out the math formula of the cross entropy loss, as CrossEntropyLoss does not support multiple labels.\n\nTest Time Augmentation\nThe sample code tests images using a deterministic “test transformation”\nYou may using the train transformation for a more diversified representation of the images, and predict with multiple variants of the test images.\nCoding: You need to fill in train_tfm, change the augmentation method for test_dataset, and modify prediction code to gain this effect.\n\n\n\nUsually, test_tfm will produce images that are more identifiable, so you can assign a larger weight to test_tfm results for better performance.\n\n\n\nEx: Final Prediction = avg_trian_tfm_pred * 0.5 + test_tfm_pred* 0.5\n\nCross Validation\n\nCross-validation is a resampling method that uses diff portions of the data to validate and train a model on different iterations. Ensembling multiple results lead to better performance.\nCoding: You need to merge the current train and validation paths, and resample from those to form new train and validation sets.\n\n","categories":["Machine Learning"],"tags":["Courses Notes","Classification","Neural Network","Hung-yi Lee","CNN"]},{"title":"06_Hung-yi Lee_why deep learning?","url":"/Machine-Learning/06-hung-yi-lee-why-deep-learning/","content":"ValtrainTraining Set \nModel \nModel \nModel \nvalidationValidation Set  \n =  {        }\n\nUsing validation set to select model = considered as “training by \nYour model is  {    }\n\nValidation 这件事情，也可以看作是在 Validation Set 上做训练。\n\n\nOverfitting: 抽到不好的 training data 会造成泛化能力与潜在数据分布不匹配\n\n可选择的 func 越多，模型越复杂  越大，模型泛化能力差的几率越大\n\nHopefully  is smallIf your  is large, u still have high \nValidation Set 引入后，如果待选择的模型，过多，仍然有可能 Overfitting\nDeep learning\n: 模型的函数集\n\n模型的函数集越大越大，理想的Loss可以更低，但是 理想和现实的gap 比较大\n模型的函数集越小越小，理想的Loss越高，但是 gap 比较小\n\n鱼与熊掌兼得的深度学习深度学习可以使得，的数量很少的同时，损失函数很小\nWhy Hidden Layer?理论上：有一个 hidden-layer 的函数模型，可以制造任何形式的函数。\n\nSeide Frank, Gang Li, and Dong Yu. “Conversational Speech Transcription Using Context-Dependent Deep Neural Networks.” Interspeech. 2011.\n11年的论文，实验验实了，deep越深，error越小\nFat .vs Deep同样的参数量下，与其把 Network 变胖，不如 deep 的架构。\n\nYes, one hidden layer can represent any function.\nHowever, using deep structure is more effective.\n\n\nDeep learning 的真正强项反而是，更不容易 Overfitting\n类比解释\n\n逻辑电路可以构建任意形式的逻辑函数，但是，我们都不会以枚举的方式来构造：因为浪费逻辑门同理，构建计算机，也会用一些比较精巧的方式。\n\n2. 软件程序的设计也同理，会有大量的自函数复用，以提升开发效率，节省程序内存\n3. 折纸的例子\n直觉解释上面是，上课提的一个非常简单直觉的例子，同样生成一个较为复杂的模型，Deep的模型所需要的参数少很多。\ndeeper is better\n\nDeep networks outperforms shallow ones when the required functions are complex and regular.\nDeep is exponentially better than shallow even when .\n\nSpatial Transformer Layer\nCNN is not invariant to scalling and rotation\nCNN 有一些 translation invariance, 可能是 maxpooling 的关系，人物稍微移动一小部分距离识别上，不会有大碍。\n\n\n\nspatial transformer layer: 可以让输入图像进行 旋转 和 缩放，这一层的module也是，神经网络\nEnd-to-end learn(train):  spatial transformer layer 与 CNN 的参数可以堆叠在一起进行训练\n\nCan also transform feature map, CNN 的中间层，也可以被视为 image 被 transform.\nHow to transform an image/feature map这里的part感觉不是太懂\nGeneral Layer: \nIf we want translate as above: \nImage Transformation\nExpansion, Compression, Translation\n\n\n\nRotation\n\n\naffine transformation6 parameters to describe the affine transformation\n\n\n如果 affine matrix 有小数时，要对最后的输出结果取 四舍五入。\n以上的内容时无法使用 gradient descent解\n\nInterpolationsNow we can use gradient descent\n\n","categories":["Machine Learning"],"tags":["Courses Notes","Hung-yi Lee","Validation","Deep Learning"]},{"title":"07_Hung-yi Lee_Self-attention","url":"/Machine-Learning/07-hung-yi-lee-self-attention/","content":"Seq as inputSophisticated Input\nInput is a vector\nInput is a set of vectors\n\nVector Set as Input举例子:\n\n文字处理文本单词的处理形式常见的有两种: One-hot Encoding, Word Embedding\n\n\nOne-hot encoding: 不会表征不同label之间的关系（语义关系）\nWord Embedding: 会有对单词进行 语义聚类 一般的处理\n\nTo learn more: https://youtu.be/X7PH3NuYW0Q(in Mandarin)\n\n语音辨识（简化\n\nGraph \n\nSocial Network \nDrug Discovery \n\n\n\nOutput\nEach vector has a label\nThe whole sequence has a label\nModel decides the no. of labels itself. seq2seq\n\nSequence Labeling上面的第一种，输入输出方式，被称为 Sequence Labeling\nSelf-attentionFC： Fully-connected network\n\n\nSelf-attention 会吃掉（接收）一整个 Sequence 的输入，然后 input 几个vector就会有几个Output\n\nself-attention 是可以多次使用的\nAttention is all u needAttention is all you need.\n在上面的论文中，Google 第一次提出了 transformer 的网络架构。transformer 里面最重要的 Module 就是 Self-attention\n\n李沐：在 Transformer(aka 变形金刚)之后，Model的名字，变得越来越Fantasy\n\nRelevant\nDot-product\nAdditive\n\n\n其中进入Softmax前的参数被称为 attention score\nExtract information based on attention scores\n\nVectorization\n\n\n\n做了很多复杂的操作，最后需要学习的参数也只有 \nMulti-head Self-attentionDifferent types of relevance\n2 heads每个同上标的量只跟，同上标的对应量进行操作。\n\n\nPositional EncodingEach column represents a positional vector \n\nNo position information in self-attention\nEach position has a unique positional vector \nhand-crafted\nlearned from data\n\n比较新的研究： https://arxiv.org/abs/2003.09229提出和比较了不同的 positional encoding\n\nApplicationTransformer: https://arxiv.org/abs/1706.03762BERT: https://arxiv.org/abs/1810.04805\nWidely used in Natural Language Processing(NLP)!\nSelf-attention for Speechhttps://arxiv.org/abs/1910.12977\n\n如果按照之前的 注意力机制 来设计，Attention Matrix 的参数量与 seq 的长度为平方关系。占用大量的 memory， 因此一般会使用 Truncated Self-attention，只考虑一部分的 seq\n\nSelf-attention for ImageAn image can also be considered as a vector set.\nSelf-Attention GANhttps://arxiv.org/abs/1805.08318\nDetection Transformer(DETR)https://arxiv.org/abs/2005.12872\nSelf-attention v.s. CNN\nCNN: self-attention that can only attends in a receptive field\nCNN is simplified self-attention.\n\n\nSelf-attention: CNN with learnable recptive field\nSelf-attention is the complex version of CNN\n\n\n\n\nCNN 的感受野是 人为划定 的，而 Self-attention 的感受野是机器 自己学习出来 的\n\nRelationship\n19年的一篇论文：On the Relationship between Self-Attention and Convolutional Layers \n这篇paper以数学的方式严谨的证明了：\n\nCNN就是Self-attention的特例，Self-attention只要设定合适的参数，就能做到与CNN一样的的事情\nSelf-attention是更flexible的CNN\n\namount of dataset\nAn image is Worth 16x16 Words: Transformers for Image Recognition at Scale \nSelf-attention v.s. RNN\nRNN 与 Self-attention 一样，都是要处理 input 为一个 sequence 的状况\n有一个 memory 的 vector 和 RNN 的 block balabala 李老师没讲清楚\n\n\n\nRNN 是无法平行输出的，Self-attention 的机制可以平行输出\n运算速率上，Self-attention 更加 effective，很多的模型都用 Self-attention来取代RNN的架构\n\nTransformer are RNNs: Fast Autoregressive Transformers with Linear Attention\nSelf-attention for GraphGraph 也可以看作是，一堆vector，那么一堆vector就能当用 self-attention 来处理。\nSelf-attention 有自己寻找不同对象之间的关联性机制。但是 Graph 自身的属性，就已经包含了关联性信息。\n\nConsider edge: only attention to connected nodesThis is one type of Graph Neural Network(GNN).\n\n\nTo learn moreLong Range Arena: A Benchmark for Efficient Transformers注意力机制的弊端： 运算量很大，这篇paper尝试量各种各样不同变体，测试了不同性能和训练效率\nEfficient Transformers: A Survey\n","categories":["Machine Learning"],"tags":["Courses Notes","Hung-yi Lee","Self-attention","Machine Learning"]},{"title":"01_Object Oriented Programming","url":"/Code-like-tourist/01-object-oriented-programming/","content":"ObjectWhat’s an object\nObject &#x3D; Entity\nObject may be\nVisible or \ninvisible\n\n\nObject is variable in programming languages.\n\n\n\n\n对象在编程语言中，以变量的形式出现。\n\n\n\nObjects &#x3D; Attributes + Services\n\nData: the properies or status\nOperations: the functions\n\n\nMapping\nFrom the problem space to the solution one.\n\n\n\n\n\n写程序的过程&#x3D;用计算机程序描述问题+用计算机程序解决问题。这是一种从 问题空间-&gt;解决空间 的映射\n\n\n\nWhat’s object-oriented\nA way to organize\nDesigns\nImplementations\n\n\nObjects, not control or data flow, are the primary focus of the design and implementation.\nTo focus on things, not operations\n\nObject Oriented Programming这节主要讲OOP的基本理念\n\nObjects send and receive messages(objects do things!)\n\n\nsend messages\nMessage are\nComposed by the sender\nInterpreted by the receiver\nImplemented by methods\n\n\nMessages\nMay cause receiver to change state\nMay return results\n\n\n\n\n\n\n消息以函数形式传递，消息可能会改变接受者状态（也可以返回结果）\n\n\n\nclass &amp; objectThe fish is fish. The bird is bird. (Aristotle)\n\nObjects (cat)\nRepresent things, events, or concepts\nRespond to messages at run-time\n\n\nClasses (cat class)\nDefine properties of instances\nAct like types in C++\n\n\n\nOOP Characteristics\nEverything is an object\nA program is a bunch of objects telling each other what to do by sending messages.\n\n\n这里是 what to do 不是 how to do\n\n\n\n\nEach object has its own memory made up of other object.\n\n\n每个对象都有由其他对象组成的自己的内存\n\n\n\n\nEvery object has a type\nAll objects of a particular type can receive the same messages.\n\ninterface\nThe interface is the way it receives messages.\nIt is defined in the class the object belong to.\n\nfunctions of the interface\nCommnication\nProtection\n\n\n接口保护对象内部状态\n\n\n\n\n\nThe Hidden Implementation\nInner part of an object, data members to present its state, and the actions it takes when messages is rcvd is hidden.\nClass creators vs. Client programmers\nKeep client programmers’ hands off portions they should not touch.\nAllow the class without worrying about how it will affect the client programmers.\n\n\n\n","categories":["Code like tourist"],"tags":["C\\C++","Object Oriented Programming","Kai Weng"]},{"title":"操作系统的真象","url":"/uncategorized/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%9C%9F%E8%B1%A1/","content":"开始尝试学习操作系统。按照，北京大学郑钢前辈所作的 《操作系统真象还原》 一书。\n有分层概念的计算机计算机具有分层概念。\n由各个部分组合而成一个系统，每一部分就是一个功能模块，各司其职。\n\n只完成一定的工作，并将工作的结果（输出）交给下一层模块（外设、硬件）。系统就是，各个模块的工作不断累加，形成流水线式的上下游协作\n\n软件访问硬件硬件的更新换代日新月异，为实现通用，提出了IO接口\n硬件IO分类串行输入输出串行硬件通过串行接口与CPU通信，CPU通过串行接口与串行设备数据传输。\n并行输入输出与串行类似\n访问外部硬件的两种方式内存映射","tags":["operating system"]},{"title":"R-CNN 和 Fast R-CNN","url":"/Object-Detection/r-cnn-%E5%92%8C-fast-r-cnn/","content":"R-CNNR-CNN: Region with CNN feature\nR-CNN是利用深度学习进行目标检测的开山之作。作者Ross Girshick多次在PASCAL VOC的目标检测竞赛中折桂，曾在2010年带领团队获得终身成就奖。\nPaperRich feature hierarchies for accurate object detection and semantic segementation\npiplineR-CNN算法流程可分为4个步骤\n\n一张图像生成 1k～2K个候选区域*（使用Selective Search方法）\n对每个候选区域，使用深度网络提取特征\n特征送入每一类的SVM分类器，判别是否属于该类\n使用回归期精细修正候选框位置（SS算法不是很准，专门设计回归期修正\n\n\nSelective Search\n利用 Selective Search 算法通过图像分割的方法得到一些原始区域，然后使用一些合并策略进行合并，得到一个层次化的区域结构，而这个结构就包含着可能需要的物体。\n\nfeature extraction\n将2000候选区缩放到 227x227 pixel, 接着将候选区域输入是预先训练好的AlexNet CNN网络获取4096维的特征得到2000x4096维矩阵。\n\nSVM判别\n2000x4096唯独特征与20个SVM组成的权值矩阵4096x20相乘，获得2000x20纬度矩阵的概率矩阵，每行代表一个候选框归于每个目标类别的概率。\n分别对上述2000x20维矩阵中每一列即每一类进行非极大值一直剔除重叠建议框，得到该列即分类中得分最高的一些建议框。\n\nNon maximum suppression非极大值抑制\nIoU: Intersection over UnionIoC score = \n\n寻找得分最高的目标\n计算器他目标与该目标的IoU值\n删除所有IoU大于给定阈值的目标\n返回1.\n\n回归期修正分别用20个回归器对NMS处理后剩余的建议框进一步筛选。\n20个回归器对20个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的得分最高的bouding box。\n\nR-CNN 模型结构图\nR-CNN 存在的问题\n测试速度慢\n测试一张图片约53s（CPU）用Selective Search算法提取候选框用时约2秒，一张图像内候选框之间存在最大重叠，提取特征操作冗余。\n\n\n训练速度慢\n过程及其繁琐\n\n\n训练所需空间大：\n对于SVM和bbox回归训练，需要从每个图像中的每个目标候选框提取特征，并写入磁盘。\n对于非常深的网络，如VGG16，从VOC07训练机上的5k图像上提取的特征需要数百GB的存储空间。\n\n\n\nFast R-CNNFast R-CNN是作者Ross Girshick继R-CNN后的又一力作。同样适用VGG16作为网络的backbone，与R-CNN相比训练时间快了9倍，测试推理事件快213倍，从准确率从62%上升到了66%（在Pascal VOC数据集上\npaperFast R-CNN\npipeline\n一张图像生成1K～2K个候选区域（使用Selective Search方法）\n将图像输入网络得到对应的特征图，将SS算法生成的候选框投影到特征图上获得相应的特征矩阵\n将每个特征矩阵通过ROI pooling层缩放到7x7大小的特征图，接着将特征图战平通过一系列全联接层得到预测结果\n\n\nFast R-CNN利用了原图位置与特征图位置的映射关系，直接获取候选框对应特征图上的位置。（参考了SPP-net）\n训练数据采样（正样本，负样本）正样本：目标类负样本：背景在采样的过程中，需要平衡正负样本的balance\n\n\n\n对每张图片，2000个候选框中采集了64个候选区域，其中一部分为正样本，另一部分为负样本。正样本定义： 候选框与真实目标边界框 IoU&gt;0.5负样本定义： 与真实边界框 0.1&lt;IoU&lt;0.5\n\n\n\nRoI Pooling Layer不对输入图像大小进行限制，将图片切割成7X7大小，每个图片网格\n分类器与回归器\nRoI pooling layer的输出为 7x7 的feature map\n将 7x7 的tensor展平处理，通过两个FC（全链接层）-&gt; RoI feature vector\nRoI feature vector -&gt; （并联）两个全链接层\n目标概率预测： 通过 softmax 进行N+1分类（其中第一类为背景）\n候选框回归： 回归决定（N+1）个类别的候选框4参数：（）总共 4*（N+1）个参数\n\n\n\n分别为候选框的中心(x,y)坐标，以及高宽分别为最终预测的边界框中心(x,y)，以及宽高\nMulti-task loss对应边界框回归期预测的对应类别u对回归参数（）对应真实目标的边界框回归参数()\nreference: https://www.cnblogs.com/wangguchangqing/p/12021638.html\nFast R-CNN结构图\n模型缺点比起 R-CNN 网络结构以及被划分为两个结构了\n\nSelective Search算法提取候选框（Region proposal\nCNN融合完成了三个任务（特征提取、分类、候选框回归）\n\nReferencehttps://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection/faster_rcnn\n","categories":["Object Detection"],"tags":["R-CNN","Fast R-CNN","two-stage"]},{"title":"Faster R-CNN","url":"/Object-Detection/faster-r-cnn/","content":"Faster R-CNNFaster R-CNN 是 Ross Girshick继Fast R-CNN后的又一力作。同样使用VGG16作为网络的backbone,推理速度在GPU上达到5fps（包括候选区域的生成），准确率也有进一步的提升。在2015年的ILSVRC以及COCO竞赛中获得多个项目第一名。\npipeline\n将图像输入网络得到相应的特征图\n使用RPN结构生成候选框，将RPN生成的候选框投影到特征图上获得相应的特征矩阵\n将每个特征矩阵通过ROI pooling层缩放到7x7大小的特征图,接着将特征图展平通过一系列全链接层得到预测结果\n\nFaster R-CNN = RPN + Fast R-CNN\n核心内容是，用RPN替代了Fast R-CNN中的候选框提取部分\npaperFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\nRegion Proposal Network\n","categories":["Object Detection"],"tags":["Faster R-CNN","one-stage"]},{"title":"Colab_Tutorial","url":"/Tutorial/colab-tutorial/","content":"Link of this tutorial(Colab): https://reurl.cc/Epg3M0Link of the class:  https://www.youtube.com/watch?v=YmPF0jrWn6Y\nOutline\nIntroWhat’s Colab?   – Colab, or “Colaboratory”, allows you to write and execute Python in your browser with\n\nZero configuration required\nFree access to GPUs\nEasy sharing\n\nGetting StartedCreating a new cell+Code: create a code cell+Text: create a text cell\nThere are options for moving your cell up/down or copy or delete it.\n\n\nUsing an exclamation mark(!) starts a new shell, does the operations, and then kills that shell while percentage(%) affects the process associated with the notebook, and it’s called a magic command.\n\nUse % instead of ! for cd(change directory) command\nOther magic commands are listed here\nRuntime SettingChanging RuntimeTo utilize the free GPU provided by google,click on “Runtime” -&gt; “Change Runtime Type” -&gt; select “GPU“ for “Hardware Accelerator”Doing this will restart the session, so make sure you change to the desired runtime before executing any code\nCheck GPU TypeUse the command nvida-smi to check the allocated GPU typeAvailable GPUs:(But most of the time you get K80 using the free Colab)\n\nFile ManipulationDownload files via Google Drive\nUpload and Download Files\nMounting Google Driveif u want some files to be saved permantly, u can mount ur own google drive to colab and directly download/save the data to ur google drive\nfrom google.colab import drivedrive.mount('/content/drive')Output: Mouted at /content/frive\n\n\nExecute the following three code blocks in orderThis will download the image to your google drive, and u can access it later\n\nSaving Notebook\nDownload the .ipynb file to ur local device (file &gt; Download.ipynb)\nSave the colab notebook to your google drive (file &gt; Save a copy in Drive)\nConvert .ipynb to .py and download (file &gt; Download.py)\n\nUseful Linux Commands(in Colab)ls: List all files in the current directoryls -l: List all files in the current directory with more detailpwd: Output the working directorymkdir: Create a directorycd: Move to directory gdown: Download files from google drivewget: Download files from the internetpython : Executes a python file\nProblems\nColab will automatically disconnect if idle timeout(90 min., sometimes varying) or when your screen goes black\n\nGPU usage is not unlimited! (ur account will be stopped for a period for a period if u reached the max gpu usage 12 hrs)\n\n\n","categories":["Tutorial"],"tags":["Hung-yi Lee","Google Colab","Hardware Accelerate"]},{"title":"Python_tutorial","url":"/Tutorial/python-tutorial/","content":"写在前面\n课程网站：廖雪峰_Python教程\n\n主要内容分为： Python基础、函数编程、高级语法特性、面向对象 ## 都不会太深入（开发人员可能需要很懂\n\n内容很多，其实这可能是我，第四次点开廖老师的网站了。因为想把机器学习的作业做好一点。先前做到数据挖掘跟可视化的地方，人直接尬住。就很吃瘪。这里主要是，为了给学习使用 NumPy、Pandas还有Matplotlib打下基础。\n\n\n\n真正开始做AI相关的科研学习过程中，一定会遇到一些自己以前从来没见过的，函数跟库。不同科研人员编写代码的习惯也不同。但是问题不大，到时候看文献查文档就是了。\n\n\n\n\n这里再推一波自己去北大软微神仙室友的Python教程：https://blog.csdn.net/zimuzi2019/article/details/127195751?spm=1001.2014.3001.5502\n\n\n\n具体学习方法\n自己配置开发环境\n看着我这篇的大纲还有廖老师的博客学习\n可以不看我写了什么，这篇tutorial其实就是一个语言学习划的提纲\n一定要自己写代码\n\n面向AI的极简Python学习，我们开始吧！\n安装任何Python的教程都会教，配置开发环境，我就不做。给三种Python环境配置。\nPython官网法https://www.python.org/\nPycharm环境配置https://www.jetbrains.com/pycharm/\nAnaconda环境配置https://www.anaconda.com/一般我们安装Anaconda两个方法\n\n挂梯子上官网\n上清华源\n\nvscode我是用VScode作为开发环境的，其实vscode只是一个文本编辑器，安装插件后，既可以跑ipynb的文件（像jupyter notebook一样）也能跑py文件（其实就是调用CPython解释器）\n第三方库见后文的 安装第三方库\n第一个程序\n输入与输出 I/OOutput'''用 print() 在括号中加上字符串，就可以向屏幕上输出指定的文字。 比如输出 'hello_world', 用代码实现如下：'''&gt;&gt;&gt; print('hello, world')'''print()函数可以接受多个字符串，用逗号\",\"隔开，就可以连成一串输出：'''print('The quick brown fox', 'jumps over','the lazy dog')'''print()还可以打印整数'''&gt;&gt;&gt; print('100 + 200 =', 100 + 200)\n以上接受过个字符串，遇到逗号”,”会输出一个空格\n\nInput#input 函数的输入，读取到的数据类型为 字符串，要使得其用于其他用途，要进行类型转换。\n'''Python 提供了一个 input()，可以让用户输入字符串，并存放到一个变量里'''&gt;&gt;&gt; name = input()# 输入 name = input() 并按下回车后，Python交互式命令行就在等待输入，可以输入任何字符，然后按回车后，完成输入。Michael&gt;&gt;&gt; name'Michael'&gt;&gt;&gt; \n\n一个交互性比较强的输入输出代码\nname = input('please enter your name:')print('hello', name)\n\n格式化输出学好格式化输出，可以帮你避免 看得懂别人写的代码，自己写不出来的尴尬一般深度学习打表用的是第三种，f-string 格式化输出\n格式化方法1在Python中，采用的格式化方式和C语言是一致的，用 % 实现，举例如下：\n&gt;&gt;&gt; 'Hello,%s' % 'world''Hello, world'&gt;&gt;&gt; 'Hi, %s, you have $%d.' %('Michael', 1000000)'Hi, Michael, you have $1000000.'\n\n%运算符就是用来格式化字符串的。在字符串内部，%s表示用字符串替换，%d表示用整数替换，有几个%?占位符，后面就跟几个变量或者值，顺序要对应好。如果只有一个%?，括号可以省略\n常见占位符表整体格式化的形式和C语言相同\n\n\n\n占位符\n替换内容\n\n\n\n%d\n整数\n\n\n%f\n浮点数\n\n\n%s\n字符串\n\n\n%x\n十六进制整数\n\n\n格式化方法2另一种 格式化字符串输出 的方法是使用字符串的format()方法，它会用传入的参数依次替换字符串内的占位符{0}、{1}…，不过这种方式写起来比%要麻烦得多：\n&gt;&gt;&gt; 'Hello, {0}, 成绩提升了 {1:.1f}%'.format('小明', 17.125)'Hello, 小明, 成绩提升了 17.1%'\n\n格式化方法3以f开头的字符串，称之为f-string，它和普通字符串不同之处在于，字符串如果包含{xxx}，就会以对应的变量替换：\n&gt;&gt;&gt; r = 2.5&gt;&gt;&gt; s = 3.14 * r ** 2&gt;&gt;&gt; print(f'The area of a circle with radius {r} is {s:.2f}')The area of a circle with radius 2.5 is 19.62\n\n上述代码中，{r}被变量r的值替换，{s:.2f}被变量s的值替换，并且:后面的.2f指定了格式化参数（即保留两位小数），因此，{s:.2f}的替换结果是19.62。\n比较常见的，打印训练信息的格式化输出。\nPython基础\nPython作为计算机编程语言，不能有歧义\n以#开头的语句是注释，内容任意，解释器会忽略注释。\n当语句以冒号:结尾时，缩进的语句视为代码块。\n\n数据类型Python有以下几种，可以直接处理的数据类型\n整数\n程序上的表示方法和数学上的写法一摸一样: 1, 100, -8080, 0\n十六进制表示法，0x前缀和0-9,a-f表示: 0xff00, 0xa5b4c3d2\n很大的数字Python允许下划线_分隔。\n\n浮点数浮点数在计算机内部存储方式与整数不同，不同于整数，浮点数有舍入误差很大或很小的浮点数，就必须用 科学计数法 表示，把10用e替代: 1.23e9, 1.2e-5 这样\n字符串单引号‘或者双引号“括起来的版本，比如’abc’,’xyz’，如果字符串中有一些特殊字符，用转义字符\\来标识\n&gt;&gt;&gt; 'I\\'m \\\"OK\\\"!'# 输入的字符串为 I'm \"OK\"!\n\n转义字符 \\ 可以转义很多字符，比如 \\n 表示换行，\\t 表示制表符，字符 \\ 本身也要转义，所以 \\ 表示的字符就是 \\，可以在Python的交互式命令行用 print() 打印字符串看看：\n&gt;&gt;&gt; print('I\\'m ok.')I'm ok.&gt;&gt;&gt; print('I\\'m learning\\nPython.')I'm learningPython.&gt;&gt;&gt; print('\\\\\\n\\\\')\\\\\n若字符串中有大量的字符需要转义， 为了方便，Python允许用 r’’表示’’内部的字符串默认不转义，r:raw\n&gt;&gt;&gt; print('\\\\\\t\\\\')\\       \\&gt;&gt;&gt; print(r'\\\\\\t\\\\')\\\\\\t\\\\\n字符串内部有大量换行，用 \\n 写在一行内不好阅读，为了简化，Python允许用’’’…’’’的格式表示多行内容\n&gt;&gt;&gt; print('''line1... line2... line3''')line1line2line3\n在写py文件的时候，不需要写…，这是交互命令行的解释器生成的。\nBoolpython的Bool值只有俩，True 和 False，两者都是首字母大写条件判断的结果也是 Bool值\nBool值的与或非# and&gt;&gt;&gt; True and TrueTrue&gt;&gt;&gt; True and FalseFalse&gt;&gt;&gt; False and FalseFalse&gt;&gt;&gt; 5 &gt; 3 and 3 &gt; 1True# or&gt;&gt;&gt; True or TrueTrue&gt;&gt;&gt; True or FalseTrue&gt;&gt;&gt; False or FalseFalse&gt;&gt;&gt; 5 &gt; 3 or 1 &gt; 3True# not&gt;&gt;&gt; not TrueFalse&gt;&gt;&gt; not FalseTrue&gt;&gt;&gt; not 1 &gt; 2True\n\n空值空值是Python里的一个特殊的值，用 None 表示。None只是一种特殊的空值，其余的空值还包括 列表、字典等数据类型的空\n变量\n变量名必须是 大小写英文、数字和下划线_的组合，且不用能数字开头\n在Python语法中 等号 = 是赋值语句，可以把任意数据类型赋值给变量，同一变量可以反复赋值。\n变量本身类型不固定的语言称之为 动态语言，与之相对应的叫 静态语言\n\n常量不能改变值的变量，但是 Python没有机制保证常量不被改变。\n基本运算：除法&amp;阶乘Python有三种除法，阶乘的写法也很特别（指与 Matlab 不用，很容易写错\n# 浮点除法&gt;&gt;&gt; 10/33.33333333# 地板除法&gt;&gt;&gt; 10//33# 余除法&gt;&gt;&gt; 10%31# 阶乘 **&gt;&gt;&gt; 10**2100\n\n此外 Python 的不等于写法与C相同\n数据结构：List and TupleListList是Python内置的一种数据类型。List是一种有序的集合，可以随时添加和删除其中的元素\n创建列表的方法[]\n&gt;&gt;&gt; classmates = ['Michael', 'Bob','Tracy']&gt;&gt; classmates['Michael','Bob','Tracy']\n\n使用len()函数，就可以获得list元素的个数\n\nindexList 的索引是用[],跟其他编程语言一样（除了matlab），顺序都是从0开始count，所以最后一个元素的index是 len-1\nList可以倒序索引[-1]，[-2],…倒数第一第二…这样，但是要注意越界的报错。\nList方法\nappend()\n\n#在List的末尾插入一个元素，List 是一个有序的列表\n&gt;&gt;&gt; classmates.append('Adam')&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy', 'Adam']\n\n\ninsert()\n\n#在指定位置插入元素，比如索引号为 1 的位置：\n&gt;&gt;&gt; classmates.insert(1, 'Jack')&gt;&gt;&gt; classmates['Michael', 'Jack', 'Bob', 'Tracy', 'Adam']\n\n\npop()\n\n#取出(有调出值的过程，然后删除）list末尾元素，用pop()方法，删除指定索引位置的元素，pop(i)\n# 就跟 stack 的用法一样&gt;&gt;&gt; classmates.pop()'Adam'&gt;&gt;&gt; classmates['Michael', 'Jack', 'Bob', 'Tracy']# 还可以用pop(i)&gt;&gt;&gt; classmates.pop(1)'Jack'&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy']\n可以直接利用索引和赋值来改变列表元素的值\n\nsort()\n\n#list 可以排序，如果都是字符的话，是字典序从小到大\ntupletuple 相较 list 唯一的区别在于，无法改变数据元素tuple 相较 list 更加安全创建方式()\ntuple引用一样用[]来取引用\ntuple方法因为元素无法更改，没有list所拥有的方法，但是如果元素是list的话，list元素中的元素可以更改\n数据结构：dict and setdict在一些其他的语言中，叫作map，就是键值对字典的内部存放顺序与key的放入方式无关创建方法\n&gt;&gt;&gt; d = {'Michael': 95, 'Bob': 75, 'Tracy': 85}&gt;&gt;&gt; d['Michael']95\ndict 键的查找\n\n’key’ in d: 返回一个条件判断结果\n方法get()&gt;&gt;&gt; d.get('Thomas')&gt;&gt;&gt; d.get('Thomas', -1)-1# 如果查找的键，在字典中，会返回 键所对应的值，如果，不存在，要么不输出，要么输出预设的值\ndict的pop()方法&gt;&gt;&gt; d['Adam'] = 67&gt;&gt;&gt; d.pop('Adam')# 再次查找的时候，字典中就不再会有Adam这个键-值对了\n\n与list相比较，dict有几个特点：\n\n查找和插入的速度极速，不会随着key的增加而变慢；\n需要占用大量的内存，内存浪费多。而list相反。dict 是用空间来换取时间的一种方法。dict的key必须是不可变对象\n\nsetset与dict类似，也是一组key的集合，但不存储value。创建set\n&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; s{1,2,3}\n集合是，元素是无序的且不重复，重复输入，会被语言自动过滤\n集合方法\nadd(key)\n&gt;&gt;&gt; s.add(4)# 集合s加入一个元素 4\n\nremove(key)\n&gt;&gt;&gt; s.remove(4)# 4被拿走\n\n集合的交、并\n&gt;&gt;&gt; s1 &amp; s2&gt;&gt;&gt; s1 | s2\n\ncontrol flowif&amp;elif# 注意冒号部分 以及 缩进if &lt;条件判断1&gt;:    &lt;执行1&gt;elif &lt;条件判断2&gt;:    &lt;执行2&gt;elif &lt;条件判断3&gt;:    &lt;执行3&gt;else:    &lt;执行4&gt;\n\n循环Python的循环有两种，一种是for … in 一种是 whilebreak与continue跟c中的一样\nfor … infor x in list:'''将list中的元素，按顺序依次带入x进行循环中的运算'''\n有时候，我们只需要一个自然数序列有函数 range(n) 生成一个长度为 n 的自然数递增序列（0-&gt; n-1)\nwhilewhile &lt;条件1&gt;:    执行&lt;语句1&gt;\n\n函数调用函数调用函数，只需要函数的名称和参数如果传入的参数数量不对，会报错 TypeError\n数据类型转换数据类型转换是 Python 内置的常用函数。\n&gt;&gt;&gt; int('123')123&gt;&gt;&gt; int(12.34)12&gt;&gt;&gt; float('12.34')12.34&gt;&gt;&gt; str(1.23)'1.23'&gt;&gt;&gt; str(100)'100'&gt;&gt;&gt; bool(1)True&gt;&gt;&gt; bool('')False\n\n函数的引用拿函数值做赋值，会生成对函数对引用，就跟 Cpp 中的语法一样，直接“起一个别名”\n&gt;&gt;&gt; a = abs # 变量a指向abs函数&gt;&gt;&gt; a(-1) # 所以也可以通过a调用abs函数1\n\n\n定义函数Python的函数定义\ndef my_abs(x):    if x &gt;= 0:        return x    else:        return -x# quite easy\n如果函数体内，没有 return 语句，函数执行完毕，也会返回结果，只是结果为 None\nimport如果上看的 my_abs()函数定义保存为了 abstest.py 文件，可以在当前目录下启用Python解释器，用 from abstest import my_abs 来导入 my_abs() 函数\npass定义一个函数，什么也不做，函数体用 pass\ndef nop():    pass\n参数检查*这个section是进阶内容如何编写报错提示\ndef my_abs(x):# 这个函数会检测 x 的数据类型，如果不属于 int 和 float，会TypeError报错，字符串提示    if not isinstance(x, (int, float)):        raise TypeError('bad operand type')    if x &gt;= 0:        return x    else:        return -x\n\n多值返回\nreturn后面多值用,分隔，返回值是一个 tuple\n取用函数返回值时，多变量同时接收，并且按位置赋值\n\nimport mathdef move(x, y, step, angle=0):    nx = x + step * math.cos(angle)    ny = y - step * math.sin(angle)    return nx, ny&gt;&gt;&gt; x, y = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(x, y)151.96152422706632 70.0&gt;&gt;&gt; r = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(r)(151.96152422706632, 70.0)\n\n函数的参数这个section比较重要， 之前看一些源码会因为，一些参数设置不熟悉而犯懵逼。\n\n定义函数的时候，我们把参数的名字和位置确定下来，函数的接口定义就完成了。对于函数的调用者来说，只需要知道如何传递正确的参数，以及函数将返回什么样的值就够了，函数内部的复杂逻辑被封装起来，调用者无需了解。\n\n位置参数就是我们平时最常见的参数，函数输入必须有对应位置参数的输入。如果缺少，位置参数的输入，系统会报错。\n默认参数在函数设定的时候，就已经设定好默认值的参数。如果，额外输入该参数值，参数值会被替代。\ndef power(x, n=2):    s = 1    while n &gt; 0:        n = n - 1        s = s * x    return s\n设定默认参数的注意事项\n\n位置参数必须在前\n如何设置参数\n\n传入多个参数时，将变化大的参数设置在前面，将变化小的（甚至大多数可以用默认值替代的参数放在后面）\n默认参数一定要指向不变对象！！！！！\n# 以下为没有将 默认参数设定为不可改变对象 的一个漏洞例子def add_end(L=[]):    L.append('END')    return L# 没有出bug的函数操作&gt;&gt;&gt; add_end([1, 2, 3])[1, 2, 3, 'END']&gt;&gt;&gt; add_end(['x', 'y', 'z'])['x', 'y', 'z', 'END']# 出bug的函数操作&gt;&gt;&gt; add_end()['END']&gt;&gt;&gt; add_end()['END', 'END']&gt;&gt;&gt; add_end()['END', 'END', 'END']# 正确的例子def add_end(L=None):    if L is None:        L = []    L.append('END')    return L\n\n可变参数顾名思义：传入参数个数是可变的，允许传入0个参数值一般输入一个List或者Tuple，而函数体内会有for…in循环对变量进行一个遍历\ndef calc(*numbers):    sum = 0    for n in numbers:        sum = sum + n * n    return sum\n设置方式是，在变量名前加上*，可变参数会自动组装成一个 Tuple\n\n可以先将参数值组装成List或Tuple\n但是在输入参数的时候，要在那个可迭代参数前加上*&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums)14\n\n关键字参数关键字参数，会在函数内部自动组装称一个dict\ndef person(name, age, **kw):    print('name:', name, 'age:', age, 'other:', kw)\n一般会有一个大杂烩输出（大字典，键值对）\n同理，可以先组装称字典再将dict变量传入传指定参数前，要加上两个**\n&gt;&gt;&gt; extra = {'city': 'Beijing', 'job': 'Engineer'}&gt;&gt;&gt; person('Jack', 24, **extra)name: Jack age: 24 other: {'city': 'Beijing', 'job': 'Engineer'}\n\n命名关键字参数关键字参数可以接收任意不受限制的关键字参数。这导致，传入了哪儿些，查找很麻烦。\n命名关键字参数，会限制关键字参数的名字“key”\n# 限制方式为，在*号之后的都为命名关键字参数def person(name, age, *, city, job):    print(name, age, city, job)\n\n命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错\n如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了：def person(name, age, *args, city, job):    print(name, age, args, city, job)\n\n参数组合参数定义的顺序必须是：位置(必选)参数、默认参数、可变参数、命名关键字参数和关键字参数。命名关键字在关键字前面 这两者跟文档编写顺序相反\n递归函数因为这是所有语言都有的特性，所以不特别说明。递归就是：函数调用函数。\n高级特性后补: 我又看了一下，挺重要的。。。挺重要的（打脸）\n这个part的语法特性在一些数据科学的工业级教科书上出现过。还是可以学一下的，但是感觉，不是非常重要，就AI科研学习而言（之前不懂事写的\n\n但是在Python中，代码不是越多越好，而是越少越好。代码不是越复杂越好，而是越简单越好。基于这一思想，我们来介绍Python中非常有用的高级特性，1行代码能实现的功能，决不写5行代码。请始终牢记，代码越少，开发效率越高。\n\n切片slice经常取指定索引范围的操作，用循环就非常繁琐，利用切片就能大大简化\n&gt;&gt;&gt; L = ['Michael', 'Sarah', 'Tracy', 'Bob', 'Jack']&gt;&gt;&gt; L[0:3]['Michael', 'Sarah', 'Tracy']### 引用是，左边闭区间，右边是开区间### 负数索引也是一样，右边仍然是开区间&gt;&gt;&gt; L[-2:]['Bob', 'Jack']&gt;&gt;&gt; L[-2:-1]['Bob']### 加上步长的slice 跟matlab的语法一样，步长放在第三个地方&gt;&gt;&gt; L[:10:2][0, 2, 4, 6, 8]\n一些小trick&gt;&gt;&gt; 'ABCDEFG'[:3]'ABC'&gt;&gt;&gt; 'ABCDEFG'[::2]'ACEG'\n\n迭代若给定一个list或者tuple，我们可以用 for…inPython 的 for 循环抽象程度要高于 C 的 for 循环，因为 Python 的 for 循环不仅可以用在 list 或 tuple 上，还可以作用在其他可迭代对象上\n# 字典也可以进行迭代&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}&gt;&gt;&gt; for key in d:...     print(key)...acb\n因为 dict 的存储不是按照 list 的方式顺序排列，所以，迭代出的结果顺序很可能不一样。\n默认情况下，dict 迭代的是key。如果要迭代value，可以用 for value in d.values(),如果要同时迭代 key 和 value，可以用 for k,v in d.items()。\n\n由于字符串也是可迭代对象，因此，也可以作用于for循环\n\n&gt;&gt;&gt; for ch in 'ABC':...     print(ch)...ABC\n判断一个对象是否可迭代方法是通过 collections.abc 模块的 Iterable 类型判断：\n&gt;&gt;&gt; from collections.abc import Iterable&gt;&gt;&gt; isinstance('abc', Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False\n\n下标循环这个part太重要了，因为不熟悉，之前看到代码都感觉见过，又想不起来具体是干什么的\n&gt;&gt;&gt; for i, value in enumerate(['A', 'B', 'C']):...     print(i, value)...0 A1 B2 C\nfor循环其实可以同时使用两个甚至多个变量，比如dict的items()可以同时迭代key和value：\n具体做法就是，List中的元素依次与index绑定生成二元素tuple\n# 二元 for...in例子1&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:...     print(x, y)...1 12 43 9# 例子2&gt;&gt;&gt; d = {'x': 'A', 'y': 'B', 'z': 'C' }&gt;&gt;&gt; for k, v in d.items():...     print(k, '=', v)...y = Bx = Az = C\n\n列表生成式List Comprehensions 是 Python 内置的简单且强大的可以用来创建 list 的生成式。\n\n生成 1-&gt;10 的整数序列，list(range(1,11))\n\n生成 1-&gt;10 各个数值的平方\n&gt;&gt;&gt; [x * x for x in range(1, 11)]\n\nfor 循环后还能加上 if 条件判断\n&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100]\n\n列表生成式进阶应用\n进阶版，两层循环生成全排列\n&gt;&gt;&gt; [m + n for m in 'ABC' for n in 'XYZ']['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ']\n生成当前目录下的所有文件和目录名\n&gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到&gt;&gt;&gt; [d for d in os.listdir('.')] # os.listdir可以列出文件和目录['.emacs.d', '.ssh', '.Trash', 'Adlm', 'Applications', 'Desktop', 'Documents', 'Downloads', 'Library', 'Movies', 'Music', 'Pictures', 'Public', 'VirtualBox VMs', 'Workspace', 'XCode']\n使用两变量生成list\n&gt;&gt;&gt; d = {'x': 'A', 'y': 'B', 'z': 'C' }&gt;&gt;&gt; [k + '=' + v for k, v in d.items()]['y=B', 'x=A', 'z=C']\n\n把一个list中所有的字符串变成小写\n&gt;&gt;&gt; L = ['Hello', 'World', 'IBM', 'Apple']&gt;&gt;&gt; [s.lower() for s in L]['hello', 'world', 'ibm', 'apple']\n\n列表生成式的if…else# if 的列表生成式&gt;&gt;&gt; [x for x in range(1, 11) if x % 2 == 0][2, 4, 6, 8, 10]# 错误的if-else&gt;&gt;&gt; [x for x in range(1, 11) if x % 2 == 0 else 0]  File \"&lt;stdin&gt;\", line 1    [x for x in range(1, 11) if x % 2 == 0 else 0]                                              ^SyntaxError: invalid syntax# 正确的if-else&gt;&gt;&gt; [x if x % 2 == 0 else -x for x in range(1, 11)][-1, 2, -3, 4, -5, 6, -7, 8, -9, 10]\n\n生成器generatorgenerator 能够“记住” 生成序列的算法，在需要调用的时候，根据算法依次推导各个值，是一种 用时间换空间 的设计\n\n如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。\n\n创建生成器的两种方法\n把一个列表生成式的[]改成()，就创建了一个generator&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt;\n\n\nL 与 g 的区别\n最外层的 [] 和 （）\nL 是一个 list，而 g 是一个generator\n\n\n\n可以通过 next() 函数获得 generator 的下一个返回值：\n&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)4&gt;&gt;&gt; next(g)9&gt;&gt;&gt; next(g)16&gt;&gt;&gt; next(g)25&gt;&gt;&gt; next(g)36&gt;&gt;&gt; next(g)49&gt;&gt;&gt; next(g)64&gt;&gt;&gt; next(g)81&gt;&gt;&gt; next(g)Traceback (most recent call last):  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;StopIteration\ngenerator 保存的是算法，每次调用 next(g)，就计算出 g 的下一个元素的值，直到最后一个元素，此时再次调用会报错\n&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; for n in g:...     print(n)... 0149162536496481\n\n如果推算算法比较复杂，用类似列表生成式的 for 循环无法实现： 还可以用2. 函数实现Fibonacci 就是无法用列表生成式表示的。\ndef fib(max):    n, a, b = 0, 0, 1    while n &lt; max:        yield b        a, b = b, a + b        n = n + 1    return 'done'\n相比函数的打印，函数实现generator，只需要将 print(b) -&gt; yield b\n\n这里，最难理解的就是generator函数和普通函数的执行流程不一样。普通函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。\n\n请务必注意：调用generator函数会创建一个generator对象，多次调用generator函数会创建多个相互独立的generator。\ngenerator也是可迭代的\n&gt;&gt;&gt; for n in fib(6):...     print(n)...112358\n\n迭代器可迭代数据类型： list, tuple, dict, set, str一类是 generator, 包括 生成器 与 带 yield 的 generator function\n\n这些可以直接作用于for循环的对象统称为可迭代对象：Iterable。可以使用isinstance()判断一个对象是否是Iterable对象：\n\n&gt;&gt;&gt; from collections.abc import Iterable&gt;&gt;&gt; isinstance([], Iterable)True&gt;&gt;&gt; isinstance({}, Iterable)True&gt;&gt;&gt; isinstance('abc', Iterable)True&gt;&gt;&gt; isinstance((x for x in range(10)), Iterable)True&gt;&gt;&gt; isinstance(100, Iterable)False\n\n迭代器(Iterator)：可以被 next()函数不断调用并返回下一个值\n可以使用isinstance()判断一个对象是否是Iterator对象：\n&gt;&gt;&gt; from collections.abc import Iterator&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance({}, Iterator)False&gt;&gt;&gt; isinstance('abc', Iterator)False\n\n\n你可能会问，为什么list、dict、str等数据类型不是Iterator?这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。\n\n模块强烈建议还是看一下 廖老师的网站\n\n为了编写可维护的代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少，很多编程语言都采用这种组织代码的方式。在Python中，一个.py文件就称之为一个模块（Module）。\n\n使用模块的好处\n提高代码的可维护性。\n避免函数名和变量名的冲突，但要注意不要与Python内置模块命名冲突\n\n为避免模块名冲突，Python又引入了按目录来组织模块的方法，称为包(package)\n\n廖老师这里写的太好了，强烈建议去看看这篇。\n使用模块以内建的 sys 模块为例，编写一个 hello 模块（廖老师写的）\n#!/usr/bin/env python3# -*- coding: utf-8 -*-' a test module '__author__ = 'Michael Liao'# 导入变量sys后，就可以利用变量sys指向该模块，并且访问sys模块的所有功能。import sysdef test():    # sys模块有个argv变量，用list存储命令行中的所有参数（至少有一个参数，因为第一个参数永远是.py文件名    '''运行python3 hello.py获得的sys.argv就是['hello.py']；'''    '''运行python3 hello.py Michael获得的sys.argv就是['hello.py', 'Michael']。'''    args = sys.argv    if len(args)==1:        print('Hello, world!')    elif len(args)==2:        print('Hello, %s!' % args[1])    else:        print('Too many arguments!')if __name__=='__main__':    test()\n\n第1行和第2行是标准注释，第1行注释可以让这个hello.py文件直接在Unix/Linux/Mac上运行，第2行注释表示.py文件本身使用标准UTF-8编码；第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释；第6行使用__author__变量把作者写进去，这样当你公开源代码后别人就可以瞻仰你的大名；\n\nname &amp; main trickif __name__=='__main__':    test()\n这个技巧非常重要\n\n当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。\n\n作用域我们可能会定义许多 变量 和 函数，在一个模块中。有的变量，我们只希望在模块内部被调用，这时我们对变量名前后加上 _ 来表示为 private一般的变量，我们认为是 public的\n如模块中的__name__,__auther__就是特殊变量(private)，理论上这些不该被调用，但是Python没有防止其被调用的硬性手段。\n一个private函数的小例子：\ndef _private_1(name):    return 'Hello, %s' % namedef _private_2(name):    return 'Hi, %s' % namedef greeting(name):    if len(name) &gt; 3:        return _private_1(name)    else:        return _private_2(name)\n\n我们在模块里公开greeting()函数，而把内部逻辑用private函数隐藏起来了，这样，调用greeting()函数不用关心内部的private函数细节，这也是一种非常有用的代码封装和抽象的方法，即：外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。\n\n安装第三方模块pip\n在Python中，安装第三方模块，是通过包管理工具pip完成的。\n如果你正在使用Mac或Linux，安装pip本身这个步骤就可以跳过。Windows用户需要在安装Python的时候就勾选pip的安装。\n\n例如，我们要安装一个第三方库——Pillow，在cmd或者bash或者各种shell这些终端里\npip install Pillow\nconda\n在使用Python时，我们经常需要用到很多第三方库，例如，上面提到的Pillow，以及MySQL驱动程序，Web框架Flask，科学计算Numpy等。用pip一个一个安装费时费力，还需要考虑兼容性。我们推荐直接使用Anaconda，这是一个基于Python的数据处理和科学计算平台，它已经内置了许多非常有用的第三方库，我们装上Anaconda，就相当于把数十个第三方模块自动安装好了，非常简单易用。\n\n\n可以从Anaconda官网下载GUI安装包，安装包有500~600M，所以需要耐心等待下载。下载后直接安装，Anaconda会把系统Path中的python指向自己自带的Python，并且，Anaconda安装的第三方模块会安装在Anaconda自己的路径下，不影响系统已安装的Python目录。\n\n\n安装好Anaconda后，重新打开命令行窗口，输入python，可以看到Anaconda的信息：\n\n\n可以尝试直接 import numpy 等已安装的第三方模块。\n模块搜索路径当我们试图加载一个模块时，Python会在指定的路径下搜索对应的.py文件，如果找不到，就会报错\n默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中：\n&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path['', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python36.zip', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6', ..., '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages']\n添加路径\n直接修改sys.path，添加要搜索的目录：\n&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path.append('/Users/michael/my_py_scripts')\n这种方法是在运行时修改，运行结束后失效。\n\n第二种方法是设置环境变量PYTHONPATH，该环境变量的内容会被自动添加到模块搜索路径中。设置方式与设置Path环境变量类似。注意只需要添加你自己的搜索路径，Python自己本身的搜索路径不受影响。\n\n\n面向对象这个章节非常重要，因为，用 PyTorch 框架设置模型的时候，我们需要运用一些面向对象的知识。\n面向对象编程 – Object Oriented Programming 是一种 程序设计思想。把 对象 作为程序的基本单元，对象 包含了 数据 和 操作数据 的函数。\n\n面向过程： 计算机程序视为一系列的命令合集，即一组函数的顺序执行。函数-&gt;切分的子函数，通过将大块函数切割成小块函数，来降低系统的复杂度\n面向对象： 把计算机程序视为一组对象的集合。\n\n自定义对象： 类（Class)\n\n采用面向对象的程序设计，首先考虑的是将 数据类型 视为一个对象。\n对象拥有若干个 属性\n还要内置 方法class Student(object):    def __init__(self, name, score):        self.name = name        self.score = score    def print_score(self):        print('%s: %s' % (self.name, self.score))# 与 对象交互 实际上就是调用对象对应的关联函数，我们称之为对象的方法（Method）。面向对象的程序写出来就像这样：bart = Student('Bart Simpson', 59)lisa = Student('Lisa Simpson', 87)bart.print_score()lisa.print_score()\n\n\n\n\n类(Class)是一个抽象的概念\n实例(Instance)是一个具体的概念\n方法(Method)是与类相关联的函数\n\n类与实例实例(Instance)是根据类创建出来的一个个具体的“对象”\nclass Student(object):    pass\n\n\nclass后面紧接着是类名，即Student，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的，继承的概念我们后面再讲，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。\n\n创建实例&gt;&gt;&gt; bart = Student()&gt;&gt;&gt; bart&lt;__main__.Student object at 0x10a67a590&gt;&gt;&gt;&gt; Student&lt;class '__main__.Student'&gt;\n\n实例的初始化通过定义一个特殊的__init__方法，在创建实例的时候，就把 name, score 等属性绑上去：\nclass Student(object):    def __init__(self, name, score):        self.name = name        self.score = score\n这个函数定义后，在创建实例的时候就得，Student后的括号中传入 name 与 score的参数了\n\n方法的第一个参数永远是 self，表示实例本身。在方法内部，可以通过 self 指向创建的实例本身。\n有了初始化方法后，就不能换入空的参数了。\n\n限制访问函数内部有些属性，不想被访问。就在命名的时候，命名前加上双下划线 __\n优点： 隐藏了内部的复杂逻辑\n继承与多态当定义一个 Class 的时候，可以从某个现有的 Class 继承，新的 Class 称为 子类(Subclass)，而被继承的 Class 称为基类、父类或超类(Base class, Super class)\n继承继承: 子类可以使用父类定义的方法，获得父类所有的属性。\n# 定义 Animal 作为父类class Animal(object):    def run(self):        print('Animal is running...')# 定义两个子类class Dog(Animal):    passclass Cat(Animal):    pass&gt;&gt;&gt; dog = Dog()&gt;&gt;&gt; dog.run()&gt;&gt;&gt; cat = Cat()&gt;&gt;&gt; cat.run()Output:Animal is running...Animal is running...\n\n\n如果在子类中，定义了与父类同名的方法，会覆盖父类方法。\n\n判断类的类型的方法：\na = list() # a是list类型b = Animal() # b是Animal类型c = Dog() # c是Dog类型&gt;&gt;&gt; isinstance(a, list)True&gt;&gt;&gt; isinstance(b, Animal)True&gt;&gt;&gt; isinstance(c, Dog)True\n继承关系中，实例即被视为子类，又被视为父类，但是，作为最小的类，范围最短。\n多态方法的传参中，传入类的类型\n# 输入参数为数据的类def run_twice(animal):    animal.run()    animal.run()# 输入类后，会调用那个类中定义的方法&gt;&gt;&gt; run_twice(Animal())Animal is running...Animal is running...&gt;&gt;&gt; run_twice(Dog())Dog is running...Dog is running...&gt;&gt;&gt; run_twice(Cat())Cat is running...Cat is running...\n\n任何依赖Animal作为参数的函数或者方法都可以不加修改地正常运行，原因就在于多态。多态的好处: 需要传入Dog、Cat、Tortoise，只需要接收Animal类型，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思：\n对于一个变量，我们只需要知道它的类型，无需确切地知道它的子类型，就可以放心地调用父类方法，而具体调用的方法是作用在本身还是子类对象上，由运行时该对象的确切类型决定。多态真正的威力：调用方只管调用，不管细节，而当新增一种子类时，只要确保方法编写正确，不用管原来的代码是如何调用的。\n\n著名的“开闭”原则：\n对扩展开放：允许新增子类\n对修改封闭：不需要修改依赖类型的方法函数。\n继承还可以一级一级地继承下来，就好比从爷爷到爸爸、再到儿子这样的关系。而任何类，最终都可以追溯到根类object，这些继承关系看上去就像一颗倒着的树。\n\n\n\n\n获取对象信息type()可以判断 基本数据类型 和 类的类别，还可以判断 函数的类型\n&gt;&gt;&gt; type(123)&lt;class 'int'&gt;&gt;&gt;&gt; type('str')&lt;class 'str'&gt;&gt;&gt;&gt; type(None)&lt;type(None) 'NoneType'&gt;&gt;&gt;&gt; type(abs)&lt;class 'builtin_function_or_method'&gt;&gt;&gt;&gt; type(a)&lt;class '__main__.Animal'&gt;\n\n*不要求掌握的方法：\n&gt;&gt;&gt; import types&gt;&gt;&gt; def fn():...     pass...&gt;&gt;&gt; type(fn)==types.FunctionTypeTrue&gt;&gt;&gt; type(abs)==types.BuiltinFunctionTypeTrue&gt;&gt;&gt; type(lambda x: x)==types.LambdaTypeTrue&gt;&gt;&gt; type((x for x in range(10)))==types.GeneratorTypeTrue\n\ninstance()对于class的继承关系来说，使用type()就很不方便。我们要判断class的类型，可以使用isinstance()函数。    object -&gt; Animal -&gt; Dog -&gt; Husky\n&gt;&gt;&gt; a = Animal()&gt;&gt;&gt; d = Dog()&gt;&gt;&gt; h = Husky()&gt;&gt;&gt; isinstance(h, Husky)True&gt;&gt;&gt; isinstance(h, Dog)True&gt;&gt;&gt; isinstance(h, Animal)True&gt;&gt;&gt; isinstance(d, Dog) and isinstance(d, Animal)True\n\n\n总是优先使用isinstance()判断类型，可以将指定类型及其子类“一网打尽”。\n\ndir: 获取类所有属性与方法如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：\n&gt;&gt;&gt; dir('ABC')['__add__', '__class__',..., '__subclasshook__', 'capitalize', 'casefold',..., 'zfill']\n仅仅是列出 属性 与 方法 是不够的，配合 getattr(), setattr() 以及 hasattr()，我们可以直接操作一个对象的状态：\n&gt;&gt;&gt; hasattr(obj, 'x') # 有属性'x'吗？True&gt;&gt;&gt; obj.x9&gt;&gt;&gt; hasattr(obj, 'y') # 有属性'y'吗？False&gt;&gt;&gt; setattr(obj, 'y', 19) # 设置一个属性'y'&gt;&gt;&gt; hasattr(obj, 'y') # 有属性'y'吗？True&gt;&gt;&gt; getattr(obj, 'y') # 获取属性'y'19&gt;&gt;&gt; obj.y # 获取属性'y'19\n如果试图获取不存在的属性，会抛出AttributeError的错误\n# getattr()的勘误机制&gt;&gt;&gt; getattr(obj, 'z', 404) # 获取属性'z'，如果不存在，返回默认值404404# 方法对象一同处理&gt;&gt;&gt; hasattr(obj, 'power') # 有属性'power'吗？True&gt;&gt;&gt; getattr(obj, 'power') # 获取属性'power'&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn = getattr(obj, 'power') # 获取属性'power'并赋值到变量fn&gt;&gt;&gt; fn # fn指向obj.power&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn() # 调用fn()与调用obj.power()是一样的81\n\n实例属性、类属性给实例绑定属性的方法是通过实例变量，或者通过 self 变量：\nclass Student(object):    def __init__(self, name):        self.name = names = Student('Bob')s.score = 90\n\n\n定义在类中的属性，类属性。定义后，所有实例都可以访问到。\n实例属性，属于实例的属性。\n实例与类属性重名，会覆盖类属性。\n\n第三方模块\n基本所有第三方模块都会在 https://pypi.org/ 注册，找到模块名 pip 安装。什么？你问我 pip 怎么用？ 自己不会上网学啊？\nconda install\n\n虚拟环境虚拟环境就是，重新开个新号，可以在一个完全干净的python环境里，配置自己需要的开发环境\n\nvirtualenv 配置\nconda 配置\n\n这个一定要去学！！！ 看网上的一些 虚拟环境 配置的教程，或者别的，一般conda配置环境比较方便\n","categories":["Tutorial"],"tags":["Python","Prepare for Machine Learning","Python Learning"]},{"title":"08_Hung-yi Lee_Transformer","url":"/Machine-Learning/08-hung-yi-lee-transformer/","content":"transformer一个 Sequence to Sequence 的model\nSeq2seqInput a sequence, output a sequence    The output length is determined by model\n\n在一些翻译问题中，我们一般直接翻译（不会有语音转文字转另一种语言的中间过程\n\n\nSeq2seq 的架构，由 Encoder 和 Decoder组成\n最早的 Seq2seq 架构在2014年被发表在 arxiv 上，Seq2seq learning with Neural Networks \n发扬光大的paper（右） Attention is all you need(Transformer)\n\n语言翻译硬train一发： 最粗暴的训练\n\n做了一个，直接将 语音信号，转换为 繁体中文字母 的 Seq2seq模型\n利用youtube上的 乡土剧 （闽南语语言，繁体中文字幕）的资料为训练数据\nUsing 1500 hours of data for training\n\n\n\nBackground music &amp; noises? —— Don’t care\nNoisy transcriptions  —— Don’t care\nPhonemes of Hokkien？ —— No…\n\n直接台语声音信号转中文，不是不可能实现，但是一些 倒装 等语言习惯难以学会。\nText-to-Speech Synthesis文本转合成语音,Text-to-Speech 文本转语音。\n\nSeq2seq for Chatbot\nNotesSeq2seq NLP应用\nSeq2seq model 在 NLP 的领域的应用非常的多。大多数 NLP 的问题，都可以被视为 Question Answering（QA） 的问题\n\n\nhttps://arxiv.org/abs/1806.08730https://arxiv.org/abs/1909.03329\n客制化对于NLP任务而言，一般客制化设计问题的解决方案，可以获得更好的结果。但是 Seq2seq model 一般被认为是一种通用模型的解决方案。\n一些问题转换文法剖析：\n\n一些并非是 Sequence 结构的东西，也可以用一些技巧转换，然后用 Seq2seq model去完成。\n一篇14年的paper Grammar as a Foreign Language\nMulti-label ClassificationMulti-class: 单纯的多分类问题Multi-label: An object can belong to multiple classes.\nhttps://arxiv.org/abs/1909.03434https://arxiv.org/abs/1707.05495\nSeq2seq for Object Detection\nEncoderencoder: 输入一同样长的, Self-attention, rnn, cnn 都可以实现同样的\ntransformer的encoder就是 \bSelf-attention 的结构\n\n\nResidual： Resnet中提出的结构设计，输入与前馈给系统的输出\nnorm: 这里的 norm，为 Layer Norm ，一个样本的所有dimension进行标准化\n全联接层被写为 FC standfor: fully-connected\n\n\n\n输入后会加入 Positional Encoding\nAdd &amp; Norm = Residual + Layer norm\nFeed Forward 是 FC 层\nencoder 就是 若干个这样的block叠加的结构\n\nBert 使用了与 transformer encoder 一样的结构\nTo learn more\nOn Layer Normalization in the Transformer Architecture. \n原始论文的设计并不一定是最好的论文设计，里面发现了，右下的结构有更好的表现。\n\n\nPowerNorm: Rethinking Batch Normalization in Transformers\n这篇证明了 BatchNorm 为什么在 Transformer 中，不如 Layer Norm\n\n\n\nDecoderAutoRegressive(AT)\n\nDecoder 会先把 Encoder 的输出读入。以处理语音辨识为例。\n输入一个 special token (Begin Of Sentence, BOS)  \n完成以上两步后，在 softmax 之后，会输出一个 vector\nvector 的长度很长，跟 vocabulary size 是一样的（加上 END\n输出最大字后，作为输入再次进入Decoder获得下一个字节\n\n\n\n除开中间的 Multi-Head Attention(从外面输入) 与 Add &amp; Norm， 结构与 Encoder 相近\n\nMasked Self-attention 比起原版本，被车轮地依次纳入计算，就像Decoder的循环运行\n\n\n\n\nDecoder 的论文中，图上，特别强调了是一个 Masked Self-attention, decoder 的 token 是一个个产生的（非并行）。\n\nDecoder 自己决定，输出 Sequence 的长度，在某次输出为END后，Decoder的输出就结束了。这样Decoder就自己决定了Sequence的长度。\n\n\n\nNon-autoregressive(NAT)\n\nHow to decide the ouput length for NAT decoder?\nAnother predictor for output length\nOutput a very long sequence, ignore tokens after END\n\n\nAdvantage: parallel, more stable generation(e.g., TTS)\nNAT is usually worse than AT(why? Multi-modality)现在如果要让 NAT 的 Decoder 达到 AT 但表现效果，需要非常多的trick让这件事情办到。\n\n在 Self-attention 出现以后，NAT decoder 已经成为了一个热门的研究领域（大坑）\n在语音合成的应用上，NAT的架构，甚至能很方便的 控制输出的长度，讲话变快、变慢\nTo learn morehttps://youtu.be/jvyKmU4OM3c\nEncoder-Decoder\n\nencoder 的输出会被在中间的 Multi-Head Attention 模块输入，计算出 Cross attention\n\n\n经过 Self-attention(Mask) 生成的 query 与 encoder 生成  做相关性计算，生成attention score.\n最后 weighted sum 最后生成 v 进入 fully-connected network.\nCross AttentionListen, attend and spell: A neural network for large vocabulary conversational speech recognition\n跟 state of art 的结果只差一点点。\n那个时候的paper，只要是 Seq2seq 的model，paper名就要有3个动词\n上面图表：\n\n上面的是声音信号，是机器的输入，横轴是时间，纵轴是一排排的vector\n左边那排是，encoder的输出（ 当作特殊的词汇来处理了）\n灰度方块（自己造的）反应了不同时间节点的 attention score\n\nhttps://arxiv.org/abs/2005.08081\ndecoder 接受的输入不一定要是 encoder最后一层的输出，有各式各样的组合\nTrain\n\n训练时的 loss-function 为 cross entropy\n训练中，每次出现一个输出，解决一次分类\n\n\n\n在训练的输入，依次输入为 Ground Truth， 这个机制叫作 Teacher Forcing\n最后的  字段，也是被要求在内\n\nTipCopy Mechanism\n对于很多任务而言，不需要 encoder 产生输出\n\nChat-botUser: 你好，我是库洛洛Machine：库洛洛你好，很高兴认识你\nUser: 小杰不能使用念能力了！Machine： 你所谓的“不使用念能力”是什么意思？\nSummarizationhttps://arxiv.org/abs/1704.04368\n要让机器能说合理的句子，至少要 百万篇文章\n对于摘要而言，Copy Mechanism 尤为重要\nPointer Network: video from YoutubeIncorporating Copying Mechanism in Seq2seq Learning: paper\nGuided Attention\nMontonic Attention\nLocation-aware attention\n\n\nIn some tasks, input and output are monotonically aligned.For example, speech recognition, TTS, etc.\n\n\nBeam SearchThe red path is Greedy Decoding.The green path is the best one.Not possible to chech all the paths…  Beam Search\n\n不一定有用paper: The Curious Case of Neural Text Degeneration \n\n这篇paper处理的任务叫作 content completion, Beam search 加入后的训练结果会是，不断重复。\n\nRandomness is needed for decoder when generating sequence in some tasks.\n\nBeam Search 是否有用，基于要处理的 task\n\n任务结果非常明确，有唯一答案，通常比较有用\ncreative task, 很重要。语音合成TTS 要加入 Noise\n\nOptimizing Evaluation MetricsBLEU score: 依次比对，输出与 ground trues最大化 BLUE score 不等效于 最小化 Cross-entropy\n\n训练的时候 minimize Cross entropy\nValidation: Maximize BLEU score\nBLUE score 无法微分，所以不方便作为train时，用的loss func\n但是 test score 要用 BLUE score， 所以validation的时候，可以取BLUE score\n\ntraining with BLUE score只用当作 强化学习 问题，直接训练。https://arxiv.org/abs/1511.06/32\nScheduled Samplingmismatch: exposure bias如果test的时候，一环的output错误，之后的输出，倾向于全部输出错误。\n可以考虑的技巧叫作 Scheduled Sampling\n\n\nOriginal Scheduled Sampling: https://arxiv.org/abs/1506.03099\nScheduled Sampling for Transformer: https://arxiv.org/abs/1906.07651\nParalled Scheduled Sampling: https://arxiv.org/abs/1906.04331\n\n\n","categories":["Machine Learning"],"tags":["Courses Notes","Hung-yi Lee","transformer","Seq2seq"]},{"title":"Gumdam(driving) license Test","url":"/WalkThroughs/gumdam-driving-license-test/","content":"这篇是第一篇 walkthrough 因为其适配的操作系统仅限于浙江省的 机动战士高达驾驶资格证考试（Gumdam driving license Test），所以这里用中文写作。\n笔者费时贰年零扒个月才完成高达驾驶员的试炼。回忆过去，思绪万千，觉得自己完成了一番了不起的事业，心有所想，便留下此攻略。\n最大的收获： 狮子搏兔（就算是打小学生也要用尽全力），生命只有一次，但是科目三可以考4次（还能更多）\n味道太冲，我有点受不了了\n个人经历\n笔者于2019年7月报名机动车（战士）资格证考试\n19年7月底接受初次试炼（民间也称科目一）\n20年6月接受第二次试炼（聪明的你一定知道举一反三吧）\n20年6月底迎来首次第三试炼，战败\n2021年1月，第三试炼，战败\n2022年2月10日，第三次，战败\n2022年3月1日，成绩合格\n2022年3月2日，通过最后试炼，获得机动战士高达驾驶资格证\n\n此图片来自于互联网（饶命\n高达屹立于大地被教练带到一个山头的驾校场地里，把我扔到了一个 虚拟驾驶舱（方向盘模拟器），让我打方向盘，踩离合，然后他自己就去睡觉了，没有任何反馈，太极推手大概是那个时候学会的。\n然后教练就让我操控了真正的ms，那天高达引擎被它自带的操作系统熄灭了很多次。高达驾驶员要适应高达内有很多人，很挤，但是外面又很热。\n初次试炼初次试炼是 100道题？忘记了。成为一名机师还是要能适应应试教育的，当年阿姆罗就是这方面能力特别强，操作手册一看就会。我天赋差一点，前一天，做了一天。刷题进度1000/1600的样子，然后那天组织考试的考官迟到了，后来电脑又出了什么问题，要等个俩小时。闲着也闲着就爆种把题目刷完了（我可能是新人类吧\n科目二感觉驾校的场地和实际场地还是很不一样的。远古的记忆，我考科目二的时候，我几岁啊？？？那时，我的Gumdam教练员，夜里开大车赚外快，早上把学员接到驾校自己睡觉（现在想来师傅境界非常高），徒弟传徒弟。 我的功夫是师兄教的，师兄的功夫是师兄师兄教的。。。后来我成了师兄，因为疫情期间难以预约啥的，一直无法进行第二次试炼，大概在场地里给别人当了一两个月的师兄吧，教了几个人倒车入库（现在自己已经不大行了）\n科目三噩梦开始的地方。一共开了7圈，5次0分，1次85，1次100分，没有人比我更懂科目三。生命只有一次，但是科目三可以考4次（还能更多）\n\n第一次踩油门右转过红灯，大概是上30码了，我被送走。\n第二圈安全过了那个路口，直线行驶的时候，右翼遭铲车人奇袭\n进程过半，十字路口过后，选道环节偶遇擎天柱，换赛道时未打转向灯\n发射高达时，忘记关闭测试环节的灯光\n灯光测试项目5变成了，“夜间直行通过路口”，反直觉测试题，至此一直为0分。\n在路上，但是机动战士”档位于车速不匹配“-10，完美着陆后，忘记熄火-5：85\n我感受到了 阿姆罗 和 盖亚 的意志成为了最强的机师。\n\n特别注意： 一次补考场地费用 -150，补考费用额外算 -80（你不是人，今天去考科四还收了我 80*3\n科三流程起步之前安全员在把你叫到驾驶座上的时候，你的ID CARD就会被嘟进机体，你的考试信息就会被录入。 \n开始考试之前，有一个准备环节，准高达驾驶员们，需要调整座椅高度与前后位置，还有反光镜，这一步有没有好好做，安全员就了解你作为一个机师的大致水平。（这样TA就不会乱踩刹车了，大概\n这里，你要注意，观察仪表盘转速，来看机动战士有无点火，务必确保机动车处于点火状态，再跟安全员确认，开始考试。\n开始考试开始考试，机体里对着你的camera会，给你进行一个照的拍。然后会说“balabala开始考试“，然后你就要出机体，检查机体的情况，具体模拟形式就是，先去看机体的屁股，那里有个红色的按钮，用手指嘟进去，机体内部系统会说”右后方有学员经过“，同样在机体前再来一遍，”左前方有学员经过“。（最后一次考试的时候，在我之前的准机师，就两次都没嘟前面\n以上操作完毕，进入机体，挤上安全带\n挤上安全带后，“模拟灯光考试。。。巴拉巴拉”\n模拟灯光一共考5个操作\n\n第一个都是 打开前照灯\n机师教官一般会发张纸，16条也不知道18条，请吸烟刻肺\n“夜间通过”“夜间超越”是 远近光灯交替\n“请打开远光灯”“夜间在照明不良” 是 远光灯\n“会车”“夜间直行通过路口”“同方向近距离跟车”都是 近光灯\n“临时停车”“车哪里有问题” 示廓灯+双跳灯\n\n一定要注意看表盘，不同灯光操作，给的反应是不同的，学会这个技能，机师能够辨别是否操作失误，或者按键太轻，没能做到正常操作。\n非常重要有的模拟灯光题，会故意将，双跳灯操作放置在第四个来让机师措手不及。这种反思维定势的操作，让第五个操作错误率骤升，只有细心的机师，才能逃过一劫。而且在关闭灯光的操作上，很容易误操作。有为平时训练的肌肉记忆。\n起步先打上左方向灯（大于4秒），挂一档放下手刹的以后，记得 按喇叭，松离合不能太快，仔细观察路况和左后方来车\n熄火如果惨遭熄火，首先 “冷静、理性、理智”，熄火后重新点上不会让你不及格。\n千万别拉手刹！！！！\n先挂上空档，你后面的车辆可能会鸣喇叭，不用管他，不用慌。\n轻踩刹车（如果你没有安全感的话\n旋转车钥匙点火。\n这次松离合一定要慢\n直线行驶这里只讲，直线形式，开车的时候，记得看前面，用心体会，好的机师都用心开高达，不能开歪，开得时候，目视前方，（美术生都懂得一个东西叫透视，了解一下）\n档位与速度匹配问题当时在网上看了很多，说法，最后用我教练教的。换二档：上10码出头换三档：18码左右，四档：上30码三档  二档：车速不要低于15码四档  三档：车速不要低于25码\n停车教练都会教你从车身照参照物来开车的，前后车轮离白实线距离不能超过30cm\n最后当你拉起手刹后，播报“下车请注意安全，请注意左后方来车”这个时候，你不能松安全带马上开门\n请先，熄火（5分），然后解开安全带，再开门闭门！！！！\n科目四驾考宝典：模拟题，做个10套差不多就行了\n记住：\n\n凡是迅速，都不行\n凡是在马路上遇到奇怪的人，奇怪的司机，奇怪的自行车，钢铁侠，绿巨人，基恩家的扎古。。。能减速慢行/停车等待就选那个。\n\nSummary祝大家都能成为闪光的机师\n","categories":["WalkThroughs"],"tags":["Driving License Test","Creative writing","how old are u?"]},{"title":"02_Overview on Suprema and Limits","url":"/Information-Theory/02-overview-on-suprema-and-limits/","content":"ReliableReliable = Arbitarily small error probability\n\nFix a noisy channel.\nWhat’s the maximun transmission efficiency(channel code rate)\n什么是最大传输效率（用最少的bit位表达传递全部的信息）\n\n\nWhat’s the arbitarily small error probability\nerror 可以任意小，更critical的指标用更高的系统复杂度置换\n任意小，不严格为0\n\n\n\n\nChannel Capacity\nDefinition of Channel Capacity\nChannel Capacity is the maximum reliable transmission code rate for a noisy channel.\n超过 C(Channel Capacity) 误差率\n\n\n\nMutual informationMutual information: 共同信息，共识。（Shannon定义的信息是 uncertainty。\n\n\nMutual Information 其实是指系统设计时（信号传输前，这样我们就不知道，系统会传输哪种信号）的内容，是系统本身的性质。\n\n下面的维恩图，反应了左边 8bit 为传输端的内容， 2bit 为公共，6bit 为需要传输的内容。\n\n好的系统要，尽可能多地使用公共部分传输信息。\n\n\n\nthe design of a good transmission code should relate to the “common uncertainty“(or more formally, the mutual information) between channel inputs and channel outputs.\n\n\n\n\nIt is then natual to wonder whether or not this “relation” can be expressed mathematically.\n\n\n\n\nit was established by Shannon that the bound on the reliable transmission rate(information bits per channel usage) is the maximum channel mutual information(i.e. “common uncertainty” prior to the transmission begins) attainable.\n\nSummary\ninformation theory’s 蹲马步A.1 Supremum and maximumSupremum: 一个集合的最小上界 （Least upper bound or supermum)\n\nThroughout, we work on subsets of . the set of real numbers.\n\n\n\n\n\nCompleteness Axiom:(Least upper bound property) Let  be a non-empty subset of  that is bounded above. Then  has a least upper bound(in )\n\n\n\n只要集合有上界就一定有supremum\n\n对集合的拓展\n对无穷上下界的引入\n\nMaximum： If sup, then sup  is also called the maximum of , and is denoted by max .else we say that the maximum of  does not exist.\nProperties of the maximum\n\n(), if max  exists in  {}\nmax \n\n从以上性质证明  是对最大值：\n\n反向：中的任意元素都小于等于 \n正向： 在集合中可以取到。\n\nA.2 Infimum and minimumInfimum跟minimum类似上面的两个数学概念\n\nMinimum: 最小值，在集合内。\n\nA.3 Boundedness and suprema operations\n\n\n集合乘积拓展不能用以上规律套用，除非集合的值都是正的。\n\n\n\nA.4 Sequences and their limist\n\n\n\n信息论中，常常将正负无穷引入到研究的集合中，认为正负无穷也是一种收敛（而不是diverage）\n对于单调函数（单调序列），没有上界的时候，可以认为是收敛于正无穷\n\nlimsup and liminf\n\n如果 limsup 和 liminf 两个值重合的话，极限存在。\n\n直观理解：群集点（clustering point\nlimsup 跟 liminf 是一个无穷远处的概念，序列位号很大以后的上下界。\n\n\nLimit\nSufficiently large &amp; Infinitely often\n\nA.5 Equivalence\nWe close this appendix by providing some equivalent statements that are often used to simplify proofs.\ninstead of directly showing that quantity  is less than or equal to quantity , one can take an arbitrary constant  and prove that .\nSince  is a larger quantity than , in some cases it might be easier to show  than proving \n\n\n","categories":["Information Theory"],"tags":["Courses Notes","information therory","Po-ning chen"]},{"title":"lldb-tutorial","url":"/uncategorized/lldb-tutorial/","content":""},{"title":"MEME_Hung-yi Lee","url":"/Funny-Stuff/meme-hung-yi-lee/","content":"collect some memes appeared in the courses Machine Learning instructed by Hung-yi Lee\nPokemon &amp; Digimon从左到右依次是： 小火龙（李老师说他笑的很猥琐）、波波、亚古兽、“手刀乌龟”（经过考据，就叫做“乌龟兽”\nAttack on Titan\nExplainable AI用监督学习来分辨 Pokemon &amp; Digimon，all the pictures of Pokemons are Png format but the other are in Jpg format\nArea of ML探讨机器学习的领域的时候，Prof. Lee: “如果我们说，机器学习就是Regression与Classification，那么就跟说，世界只有五大洲一样，我们都知道，在这五大洲之外，还有更大的黑暗大陆。“（鬼灭之刃出现以前，就已经前往黑暗大陆了，鬼灭之刃完结后，竟然还没有到。）\nDeep LearningProf. Lee 说，换个名字（deep learning）就会使得人变潮，卖草鞋的，叫自己汉左将军中山靖王\nPokemon MasterQ: How many layers? How many neurons for each layer?\nA: It’s as hard as to become a master of Pokemon\n","categories":["Funny Stuff"],"tags":["how old are u?","memes"]},{"title":"NumPy_Pandas_Matplotlib","url":"/Tutorial/numpy-pandas-matplotlib/","content":"写在前面\n在进修这篇前需要有 Python基础，可以先学习前面的 Python_tutorial，或者从其他渠道习得。\n学习 NumPy 和 Pandas 的本质，是学习它们的 数据结构。 NumPy的 Array， Pandas中的 Series 和 DataFrame。\n学完，会有，自己修完 《数据结构》这门课一样的感觉，“我好像都学过，遇到问题不会用”。 感觉还是得，多做（做中学）\n\njupyter\n居于网页的用于交互计算的应用程序。其可被应用于全过程计算： 开发、文档编写、代码运行和结果展示\n编程时具有语法高亮、缩进、tab补全的功能\n可直接通过浏览器运行代码，同时在代码块下方展示运行结果。\n可以到处不同格式，如pdf、html、py 文件\n对代码编写说明文档或语句时，支持Markdown语法\n\n\n其实可以直接用vscode跑ipynb文件\n\nNumpy学习 NumPy 所看的教程，是一个 YouTuber(Derek Banas) 的 Tutorial\nTutorial给的代码 Github： https://github.com/derekbanas/NumPy-Tutorial笔者自己的写的ipynb文件： https://github.com/Carp2i/NumPy-Pandas-Matplotlib/tree/master/NumPy\n其中 爱因斯坦求和， 克罗内克积 条件数 Crammer法则的落地 NumPy的经济学包\n# importimport numpy as npimport matplotlib.pylab as pltfrom numpy import random\n\nCreating创建列表\n从 list 生成 array\nlist_1 = [1, 2, 3, 4, 5]np_arr_1 = np.array(list_1, dtype=np.int8)\n\n从 函数生成\n\n这里许多函数（方法）都与 Matlab 相同\n\n\n\nnp.arrange(1, 10)           # 生成 [1, 10) 的自然数列np.linspace(0, 5, 6)        # 生成 [0, 5] 包含首尾，平均分段 6 个点的阵列np.zeros(4)                 # 生成 长度为4的一维全零数组np.ones((2, 3))             # 生成 2x3 的数组np.random.randint(10, 50, 5)            # 生成[10, 50) 长度为 5 的一维矩阵np.random.randint(10, 50, size=(2, 3))  # 生成[10, 50) 形状为 2x3 的二维矩阵\n\n查看数组属性# array 的 数据类型 跟 数组size 的信息，以属性的形式保存arr.size    # 返回数组的形状arr.dtype   # 返回元素类型\n\n查看文件的使用方法np.random.randint?\n\nSlicing &amp; Indexes索引索引与切片 永远重要\n# 同样用 list 的方式索引元素值(中括号)arr[0, 0] = 2# 同样方法可以用 itemset 方法实现arr.itemset((0,1), 1)# 但这种时候就必须用 tuple 来表示元素位置# 两种相同的索引方式arr[0, 1]arr.item(0, 1)# take 方法取出，以 list 为index的元素组成的 List， 元素编号为 行优先存储np.take(arr, [0, 3, 6])# put 的第二个可变参数 传入替代的元素位置 第三个可变参数传入元素值np.put(arr, [0, 3, 6], [10, 10, 10])\n\n切片# 类Matlab的生成式arr[:5:2]   # [0:5) 步长为2的取切片# 取出了二位矩阵的第二列arr[:, 1] arr_[::-1] # 按行倒序输出# 索引中，用[]在中括号中加入条件，最后会 按行优先 输出符合条件的元素加入最后数组evens = arr[arr % 2 ==0]# 最后会赛选出 偶数 元素np.unique(arr)# 有序，消除重复\n\nReshape Arraysreshape 方法arr.reshape((1, 9))\nresize 方法np.resize(np_m_arr_1, (2,5))\n\n其他变换# 二维转置arr.transpose()# 交换坐标系arr.swapaxes(0, 1)# flatten方法，如果不额外输入参数，默认行优先，参数设置为 ‘F’ 则列优先arr.flatten('F')# 矩阵排序arr.sort(axis = 1)\n\nStacking &amp; SplittingStacking把矩阵横着或者竖着堆起来\narr1 = np.random.randit(10, size=(2,2))arr2 = np.random.randit(10, size=(2,2))# 竖直/水平 堆叠stacknp.vstack((arr1, arr2))np.hstack((arr1, arr2))# 删除第二行的所有元素np.delete(arr1, 1, 0)# 另一种 竖直/水平 堆叠# 注意输入得是 tuplenp.row_stack((arr1, arr2))np.column_stack((arr1, arr2))arr3 = np.random.randint(10, size=(2,10))np.hsplit(arr3,(2,4)) # 会删去数组的前 2x4 的元素np.vsplit(....)\n\nCopying\n直接赋值的时候不会形成复制根据Python的机理，直接赋值，只会让两个标签指向同一内容\n\n\n如果利用其中一个元素修改内容，另一个索引得到的结果也会修改\n\n\n正确的copy方法arr2 = arr1.copy()# 只是内容上的复制\n\nBasic Mathnp.add(arr1, arr2)  # 里面是常数也行，# 类似的算术 .substract .multiply .divide 按位加减乘除np.remainder(arr1)  # 取余数np.power(arr1, arr2)  # 按位 幂乘，arr2为幂np.sqrt(arr)        # 开方, cbrt()立方根np.gcd.reduce([9, 12, 15])    # 求list所有元素的 最大公约数np.lcm.reduce([9, 12, 15])    # 求list所有元素的 最小公倍数\n\nReading from files文件读取永远重要\nimport pandas as pdfrom numpy import genfromtxt# pandas 读取方法ic_sales = pd.read_csv('icreamsales.csv').to_numpy()ic_sales# numpy 读取方法 这种写法特别少ic_sales2 = genfromtxt('icecreamsales.csv', delimiter=',')ic_sales2 = [row[~np.isnan(row)]] for row in ic_sales2]\n\nStatistics Functionarr1 = np.arrange(1, 6)np.mean(arr) # .median .average .std .varnp.var([4, 6, 3, 5, 3])# nanmedian, nanmean, nanstd, nanvar. just ingore nannp.percentile(ic_sales, 50, axis=0)ic_sales[:, 1]np.corrcoef(ic_sales[:, 0], ic_sales[:, 1]) # correlation coefficent\n\nTrig Functions# 特地查了一个单词 三角函数# np.pi pi的值被放在了t_arr = np.linspace(-np.pi, np.pi, 200)plt.plot(t_arr, np.sin(t_arr))  # 第二个参数也可以改成 cos# 求反函数的方法np.arctan(1)# Also arctan2, sinh, cosh, tanh, arcsinh, arccosh, arctanh# 这个方法是 degree to Rad 角度转弧度np.deg2rad(180)# Rad 2 degreenp.rad2deg(np.pi)# htpotebyse 三角形求直角边np.hypot(10, 10)\n\nLinear Algebra Functionfrom numpy import linalg as LA# Matrix multiplication with Dot Productnp.dot(arr1, arr2)# LA的多目标点成，可以接受多个参数，依次点乘LA.multi_dot([arr1, arr2, arr3])# Inner product: 内积np.inner(arr1, arr2)# Tensor Dot Productarr_9 = np.array([[[1, 2],                    [3, 4]],                    [[5, 6],                    [7, 8]]])arr_10 = np.array([[1, 2], [3, 4]], dtype=object)np.tensordot(arr_9, arr_10)# 生成矩阵平方(矩阵乘矩阵)LA.matrix_power(arr, 2)# Compute eigenvaluesLA.eig(arr) # this version is going to return eigvector, LA.eigvals(arr)   # return eig value# Get Vector Norm sqrt(sum(x**2))LA.norm(arr)# Get Multiplicative Inverse of a matrixLA.inv(arr)# Get Condition number of matrix 条件数，矩阵敏感度指标* 有点超纲LA.cond(arr)# Determinates (行列式) are used to compute volume, area, to solve systems# of equations and more. It is a way u can multiply values in a matrix# to get 1 number.# For a matrix to have an inverse its determinate must not equal 0# det([[a, b], [c, d]]) = a * d - b * carr_12 = np.array([[1, 2], [3, 4]])LA.det(arr_12)  # 1*4 - 2*3# 加试内容： Einstein Summation， Kronecker Products， Crammer Rule\n\n\nSaving &amp; Loaderarr_15 = np.array([[1, 2], [3, 4]])np.save('randarray', arr_15)print('arr_15\\n', arr_15)arr_16 = np.load('randarray.npy')arr_16np.savetxt('randcsv.csv', arr_15)arr_17 = np.loadtxt('randcsv.csv')arr_17\n\nComparision Functioncarr_1 = np.array([2, 3])carr_2 = np.array([3, 2])np.greater_equal(carr_1, carr_2)np.less_equal(carr_1, carr_2)np.not_equal(carr_1, carr_2)np.equal(carr_1, carr_2)\n\nPandas虽然 莫烦老师 的课录的已经比较早了（2017），但是 NumPy 与 Pandas 这两年变化并不大。学习课程网站： https://www.bilibili.com/video/BV1Ex411L7oT?p=12&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click莫烦老师给的代码：https://github.com/MorvanZhou/tutorials/tree/master/numpy%26pandas笔者的ipynb文件：\n\nBasically Pandas is going to provide numerous tools to work with tabular data like you’d find in spreadsheets or databases and will be working with spreadsheets and databases.And it’s widely used for data preparation cleaning as well as analysis,, and it can work with a wide variety of different types of data\n\n# import moduleimport pandas as pdimport numpy as np\n\nData StructurePandas有两个基本的数据结构，分别为 Series 与 DataFrame （注意大小写）\nSeriesSeries 基本 数据结构，一维标签数组，能够保存任何数据类型，索引依据为标签\nDataFrameDataFrame 基本 数据结构，二维数组，是一组有序的列\n\nrow的索引为 index\ncolumns的索引为 columns\n两者一般都是字符串类型\n\n数据结构的创建# 没有主动设置 Index 的话，默认0-1s = pd.Series([1, 3, 6, np.nan, 44, 1], name='ndarray')# 生成 Index标签列表（数据类型应该需要相等dates = pd.date_range('20130101')df1 = pd.DataFrame(np.random.randn(6, 4), index = dates)# 不人为规定 Index List 会自动生成 自然数Index{0, 1, 2, 3, ...}df2 = pd.DataFrame(np.arange(12).reshape((3, 4)))df3 = pd.DataFrame({'A' : 1.,                       'B' : pd.Timestamp('20130102'),                        'C' : pd.Series(1,index=list(range(4)),dtype='float32'),                        'D' : np.array([3] * 4,dtype='int32'),                        'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]),                        'F' : 'foo'})\n上面那个例子可以看出，其实，DataFrame的创建还有Broadcast机制。纬度不够会广播填充。\n常见操作对象属性\ndtypes注意是 dtypes（有s的df.types# 返回各个列对应的数据元素类型\nindexdf.index# 返回 DataFrame Index元素类型与元素列表\ncolumnsdf.columns# 返回 Columns的元素类型与元素列表# 类型一般为 字符串\nvalues# 返回按行优先的元素值# 除去index与columnsdf.values\n\n对象常用的方法\n.describe() 是常用的快速获取数据分布特征信息的方法\n.T 矩阵转置\n.sort 排序\n\n调用函数 Call ur Element索引 index索引永远重要\ndates = pd.date_range('20130101', periods=6)df = pd.DataFrame(np.arange(24).reshape((6, 4)), index=dates, columns=['A', 'B', 'C', 'D'])df_A = df['A']print(f'{df_A} \\n {df}')\n\n切片 Slice切片跟索引一样重要\n* \nprint(df['A'], df.A)df1 = df[0:3]df2 = df['20130102':'20130104']print(f'{df1}\\n{df2}')\n\n\nDataFrame 可以像字典一样，直接用 字符串（关键字）索引\n可以像 Array 或者，2-dim List 一样切片\n也可以利用 关键字 来代替传统的数字索引\n\nSelect by Label：loc利用pandas中的loc后缀，可以利用标签来索引元素\n# 会把 index 为 '20130102' 的行拿出来作为 Seriesprint(df.loc['20130102'])print(df.loc[:, ['A','B']])# loc后缀的使用方式，只是将label替代数字索引print(df.loc[:, ['A','B']])print(df.loc['20130102', ['A', 'B']])# 分别打印 index为20130102的行，以及 20130102与AB列\nSelect by position: ilocprint(df.iloc[[1, 3, 5], 1:3])\n\n莫烦的视频里有 DataFrame.ix 方法，可以混合使用混合索引的方法但是在版本更新中，被删除了（deprecated\n# Boolean indexingprint(df)print(df[df.A &lt;8])\n把条件语句放入中括号中，会返回符合条件的行\nPandas 设置值dates = pd.date_range('20130101', periods=6)df = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns = ['A', 'B', 'C', 'D'])df.iloc[2, 2] = 1111df.loc['20130101', 'B'] = 222df[df.A&gt;0] = 0# 可以直接加上空的columnsdf['F'] = np.nan# 可以直接加上已经定义好的列df['E'] = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range('20130101', periods=6))print(df)\n\n\n\n直接加上，不存在的列，会增添新列\n会利用广播机制\n\n缺失值处理如果数据值确实，会被用 np.nan 填充缺失\n缺失探测\n.isnull()方法的使用\nnp.any(df.isnull()) == True 的全局监测print(df.isnull())  # 会返回各个元素依次判断的Boolean矩阵(形状，缺失值为Falseprint(np.any(df.isnull())==True)  # 查看是否包含True值 （的确缺失，返回True\n\n缺失值处理方法print(df.dropna(axis=0, how='all')) # how = {'any', 'all'}print(df.fillna(value=0))           # 填充 np.nan 为指定值\n\n文件的读入/读出\n读入\nread_csv\nread_excel\n…\n\n\n导出\nto_csv\nto_excel\n…\n\n\n\ndata = pd.read_csv('student.csv')print(data)# csv文件上，若没有index## 会默认附上从0开始依次递增的索引data.to_csv('student_copy.csv')\n\n合并Concatenatings同属性数据合并\ndf1 = pd.DataFrame(np.ones((3, 4))*0, columns=['a', 'b', 'c', 'd'])df2 = pd.DataFrame(np.ones((3, 4))*1, columns=['a', 'b', 'c', 'd'])df3 = pd.DataFrame(np.ones((3, 4))*2, columns=['a', 'b', 'c', 'd'])print(df1)print(df2)print(df3)res = pd.concat([df1, df2, df3], axis=0, ignore_index=True)# 最后的参数，会将原先的参数全部忽略，重新排列## 原本的index是0 1 2 0 1 2 0 1 2print(res)\n\n默认 ignore_index = False\n如果默认设置合并，会有相同index元素堆叠\nignore_index = True, 相同index元素数值合并\n\njoinres = pd.concat([df1, df2], join='inner', ignore_index=False)# join = {'outer','inner'}## outer 缺失值用 np.nan 填充## inner 缺失值 所在列直接删除# join_axes 在 莫烦 的视频里有出现，但是在20年的时候被取缔# appenddf1 = pd.DataFeame(np.ones((3,4))*0, columns=['a', 'b', 'c', 'd'])df2 = pd.DataFeame(np.ones((3,4))*1, columns=['a', 'b', 'c', 'd'])df3 = pd.DataFeame(np.ones((3,4))*2, columns=['a', 'b', 'c', 'd'])s1 = pd.Series([1, 2, 3, 4], index = ['a', 'b', 'c', 'd'])# 把df2、df3与res合并，并且重组indexres = df1.append([df2, df3], ignore_index=True)print(res)\n\n\nmerge# merging two df by key/keys. (may be used in database)# simple exampleleft = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],                                  'A': ['A0', 'A1', 'A2', 'A3'],                                  'B': ['B0', 'B1', 'B2', 'B3']})right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],                                    'C': ['C0', 'C1', 'C2', 'C3'],                                    'D': ['D0', 'D1', 'D2', 'D3']})print(left)print(right)# 以column ‘Key'为关键词 合并res = pd.merge(left, right, on='key')print(res)# consider two keys## 二维的也一样left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],                             'key2': ['K0', 'K1', 'K0', 'K1'],                             'A': ['A0', 'A1', 'A2', 'A3'],                             'B': ['B0', 'B1', 'B2', 'B3']})right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],                              'key2': ['K0', 'K0', 'K0', 'K0'],                              'C': ['C0', 'C1', 'C2', 'C3'],                              'D': ['D0', 'D1', 'D2', 'D3']})print(left)print(right)res = pd.merge(left, right, on=['key1', 'key2'], how='inner')  # default for how='inner'# how = ['left', 'right', 'outer', 'inner']res = pd.merge(left, right, on=['key1', 'key2'], how='left')print(res)# indicatordf1 = pd.DataFrame({'col1':[0,1], 'col_left':['a','b']})df2 = pd.DataFrame({'col1':[1,2,2],'col_right':[2,2,2]})print(df1)print(df2)res = pd.merge(df1, df2, on='col1', how='outer', indicator=True)# give the indicator a custom nameres = pd.merge(df1, df2, on='col1', how='outer', indicator='indicator_column')# merged by indexleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'],                                  'B': ['B0', 'B1', 'B2']},                                  index=['K0', 'K1', 'K2'])right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],                                     'D': ['D0', 'D2', 'D3']},                                      index=['K0', 'K2', 'K3'])print(left)print(right)# left_index and right_indexres = pd.merge(left, right, left_index=True, right_index=True, how='outer')res = pd.merge(left, right, left_index=True, right_index=True, how='inner')# handle overlappingboys = pd.DataFrame({'k': ['K0', 'K1', 'K2'], 'age': [1, 2, 3]})girls = pd.DataFrame({'k': ['K0', 'K0', 'K3'], 'age': [4, 5, 6]})res = pd.merge(boys, girls, on='k', suffixes=['_boy', '_girl'], how='inner')print(res)# join function in pandas is similar with merge. If know merge, you will understand join\n\nPandas‘ visualization这一章的可视化部分，其实就是 pandas 的 matplotlib 函数库的调用\nimport matplotlib.pyplot as plt\n\nplot data# Seriesdata.pd.Series(np.random.randn(1000),index=np.arrange(1000))data = data.cumsum()data.plot()# plt.plot(x=, y=)data.plot()   # 利用内置方法()plt.show()# DataFramedata = pd.DataFrame(np.random.randn(1000, 4),        index = np.arrange(1000),        columns = list['ABCD'])data.plot()# show()就是 pyplot 的打印plot.show()\n\n常见plot方法\n‘bar’\n‘hist’\n‘box’\n‘kde’\n‘scatter’\n‘hexbin’\n\nScatter写法\nax = data.plot.scatter(x='A', y='B', color='DarkBlue', label='Class 1')data.plot.scatter(x='A', y='C', color='DarkGreen', label='Class 2', ax=ax)plt.show()\n\nMatplotlib","categories":["Tutorial"],"tags":["Prepare for Machine Learning","Python Learning","NumPy","matplotlib","Pandas"]},{"title":"pytorch_tut_offcial","url":"/Tutorial/pytorch-tut-offcial/","content":"DatasetCreating a Custom DatasetA custom Dataset class must implement 3 functions: _init_, _len_, and _getitem_.\nThe implementation below: \n\nthe FashionMNIST images ar stored in a directory img_dir\nthe labels are stored separatedly in a CSV file\n\nimport osimport pandas as pdfrom torchvision.io import read_imageclass CustomImageDataset(Dataset):    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):        self.img_labels = pd.read_csv(annotations_file)        self.img_dir = img_dir        self.transform = transform        self.target_transform = target_transform    def __len__(self):        return len(self.img_labels)    def __getitem__(self, idx):        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])        image = read_image(img_path)        label = self.img_labels.iloc[idx, 1]        if self.transform:            image = self.transform(image)        if self.target_transform:            label = self.target_transform(label)        return image, label\n\n_init_函数__init__ 只在 创建数据集实例 的时候被调用。\n初始化列表包括：\n\n图像\n标签连标文件\n两者的转换器（transforms）\n\nimport osimport pandas as pdfrom torchvision.io import read_imageclass CustomImageDataset(Dataset):    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):        self.img_labels = pd.read_csv(annotations_file)        self.img_dir = img_dir        self.transform = transform        self.target_transform = target_transform    def __len__(self):        return len(self.img_labels)    def __getitem__(self, idx):        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])        image = read_image(img_path)        label = self.img_labels.iloc[idx, 1]        if self.transform:            image = self.transform(image)        if self.target_transform:            label = self.target_transform(label)        return image, label\n\n_len__len_函数返回数据集中的数目Ex：\ndef __len__(self):    return len(self.img_labels)\n\n_getitem__getitem_函数 根据 索引idx 加载和返回对应样本。\n\nBased on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them(if applicable), and returns the tensor image and corresponding label in a tuple.\n\ndef __getitem__(self, idx):    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])    image = read_image(img_path)    label = self.img_labels.iloc[idx, 1]    if self.transform:        image = self.transform(image)    if self.target_transform:        label = self.target_transform(label)    return image, label\n\nPreparing data for training with DataLoadersDataset返回数据集的 特征 与 标签，但是在训练模型的时候，我们经常需要传递一个“minibatches”规模的样本，并且在每个epoch的时候 shuffle，减少模型的过拟合，以及利用 Python 的 multiprocessing来加速检索\n\nDataLoader is an iterable that abstracts this complexity for us in an easy API.\n\nfrom torch.utils.data import DataLoadertrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\nIterate through the DataLoader\nWe have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers).\n\n# Display image and label.train_features, train_labels = next(iter(train_dataloader))print(f\"Feature batch shape: {train_features.size()}\")print(f\"Labels batch shape: {train_labels.size()}\")img = train_features[0].squeeze()label = train_labels[0]plt.imshow(img, cmap=\"gray\")plt.show()print(f\"Label: {label}\")\n\n\nTransformstransforms 用来对数据集进行预处理，以使其适合训练或者进行一些图形处理。\ntransform: to modify the featurestraget_transform: to modify the labels\nThe torchvision.transforms module offers several commonly-used transforms out of the box.\nThe FashionMNIST features are in PIL Image format, and the labels are integers.\nTo make these transformations, we use ToTensor and Lambda.\nimport torchfrom torchvision import datasetsfrom torchvision.transforms import ToTensor, Lambdads = datasets.FashionMNIST(    root=\"data\",    train=True,    download=True,    transform=ToTensor(),    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)))\n\nToTensor()ToTensor converts a PIL image or NumPy ndarray into a FloatTensor. and scales the images’s pixel intensity values in the range \nLambda TransformsLambda transforms apply any user-defined lambda function.\n# we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y.target_transform = Lambda(lambda y: torch.zeros(    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n\nAutomatic Diff with TORCH.AUTOGRADTo compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd.It supports automatic computation of gradient for any computational graph.Consider the simplest one-layer neural network, with input x, parameters w and b, and some loss func. It can be defined in PyTorch in the following manner:\nimport torchx = torch.ones(5)  # input tensory = torch.zeros(3)  # expected outputw = torch.randn(5, 3, requires_grad=True)b = torch.randn(3, requires_grad=True)z = torch.matmul(x, w)+bloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n\nTensors, Functions and Computational graphThis code defines the following computational graph:\n\nIn this network,  and  are parameters, which we need to optimize.we need to compute the gradients of loss func with respect to those variables.(set the requires_grad property of those tensors)\n\nYou can set the value of requires_grad when creating a tensor, or later by using x.requires_grad_(True) method.\n\nA func that we apply to tensors to construct computational graph is in fact an object of class Func. This object knows how to compute the func in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation func is stored in grad_fn property of a tensor.\n我们用来构造计算图（pipeline）的函数，实际上就是一个 函数类 的对象。该对象知道如何 正向计算 函数，也知道如何计算其微分通过 反向传播 的步骤关于 反向传播 函数被存储在了 一个张量的 grad_fn 属性中。\nprint(f\"Gradient function for z = {z.grad_fn}\")print(f\"Gradient function for loss = {loss.grad_fn}\")\n\nComputing GradientsTo optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need  and  under some fixed values of  and . To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad:\nloss.backward()print(w.grad)print(b.grad)\n\n\nWe can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call.\n\nDisablling Gradient TrackingBy default, all tensors with requires_grad=True are tracking their computational history and support gradient computation.However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only wanna do forward computations through the network.\ntorch.no_grad() blockstop tracking computations by surrounding our computation code with torch.no_grad() block:\nz = torch.matmul(x, w) + bprint(z.requires_grad)with torch.no_grad():    z = torch.matmul(x, w) + bprint(z.requires_grad)\n\ndetach()z = torch.matmul(x, w)+bz_det = z.detach()print(z_det.requires_grad)\n\nThere are reasons you might want to disable gradient tracking:\n\nTo mark some parameters in your neural network as frozen parameters. This is a very common scenario for finetuning a pretrained network\nTo speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n\nMore on Computational Graphs\nConceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects.In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n\nIn a forward pass, autograd does two things simultaneously:\n\nrun the requested operation to compute a resulting tensor\nmaintain the operation’s gradient function in the DAG.\n\nThe backward pass kicks off when .backward() is called on the DAG root(最后的结点). autograd then: \n\ncomputes the gradients from each .grad_fn\naccumulates them in the respective tensor’s .grad attribute\nusing the chain rule, propagates all the way to the leaf tensors.\n\n\nDAGs are dynamic in PyTorch An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.\n\nTensor Gradients and Jacobian ProductsIn many cases, we have a scalar loss function, and we need to compute the gradient with respect to some parameters.However, there are cases when the output function is an arbitrary tensor.In this case, PyTorch allows you to compute so-called Jacobian product, and not the actual gradient.\nFor a vector function , where  and , a gradient of  with respect to  is given by Jacobian matrix:\n\nInstead of computing the Jacobian matrix itself, PyTorch allows you to compute Jacobian Product 𝕧 for a given input vector . This is achieved by calling backward with  as an argument. The size of  should be the same as the size of the original tensor, with respect to which we want to compute the product:\ninp = torch.eye(5, requires_grad=True)out = (inp+1).pow(2)out.backward(torch.ones_like(inp), retain_graph=True)print(f\"First call\\n{inp.grad}\")out.backward(torch.ones_like(inp), retain_graph=True)print(f\"\\nSecond call\\n{inp.grad}\")inp.grad.zero_()out.backward(torch.ones_like(inp), retain_graph=True)print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")\n\nNoticewhen we call backward for the second time with the same argument, the value of the gradient is different.This happens because when doing backward propagation, PyTorch accumulates the gradients, i.e. the value of computed gradients is added to the grad property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to zero out the grad property before. In real-life training an optimizer helps us to do this.\n\nPreviously we were calling backward() function without parameters. This is essentially equivalent to calling backward(torch.tensor(1.0)), which is a useful way to compute the gradients in case of a scalar-valued function, such as loss during neural network training.\n\n","categories":["Tutorial"],"tags":["PyTorch"]},{"title":"PyTorch_Tutorial (TA Yuan Tseng)","url":"/Tutorial/pytorch-tutorial-ta-yuan-tseng/","content":"Videos LinkPyTorch_Tutorial1  https://www.youtube.com/watch?v=85uJ9hSaXigPyTorch_Tutorial2  https://www.youtube.com/watch?v=VbqNn20FoHM\nOutline\nBackground: Prerequisites &amp; What is Pytorch?\nTraining &amp; Testing Neural Networks in Pytorch\nDataset &amp; Dataloader\nTensors\ntorch.nn;     Models, Loss Functions\ntorch.optim:  Optimization\nSave/load models\n\nTutorial1Prerequisites\nWe assume u r already familiar with…\n\n\nPython3if-else, loop, function, file IO, class, \nDeep Learning BasicsProf. Lee’s 1st &amp; 2nd lecture videos.\n\nSome knowledge of NumPy will also be useful!\nPyTorch\nAn machine learningframework in Python\nTwo main features:\nN-dimensional Tensor computation(like NumPy) on GPUS\nAutomatic differentiation for training deep neural networks\n\n\n\nTraining Neural Networks\nStep1–Load dataset\n\nDataset:      stores data samples and expected values\nDataloader:   groups data in batches, enables multiprocessing\n\ndataset = MyDataset(file)dataloader = DataLoader(**dataset**, batch_size, shuffle=True)\n\nDataset &amp; Dataloaderfrom torch.utils.data import Dataset, DataLoaderclass MyDataset(Dataset):    def __init__(self, file):        self.data = ...    # Read data &amp; preprocess    def __getitem__(self, index):        return self.data[index]    # Returns one sample at a time    def __len__(self):        return len(self.data)    # Returns the size of the dataset\nTensorTensor–Shape of Tensor\nCheck with.shape()\n\n\nNote: dim in Pytorch == axis in NumPy\nTensor–Creating Tensors\nDirectly from data(list or numpy.ndarray)x = torch.tensor([[1, -1], [-1, 1]])x = torch.from_numpy(np.array([[1, -1], [-1, 1]]))Output:tensor([1., -1.],        [-1, 1.])\nTensor of constant zeros &amp; onesx = torch.zeros([2, 2])x = torch.ones([1, 2, 5])Output:tensor([[0.,0.],        [0.,0.]])tensor([[[1.,1.,1.,1.,1.],         [1.,1.,1.,1.,1.]]])\n\nTensors–Comon OperationsCommon arithmetic func are supported, such as:\n\nAddition  z = x + y\nSubtraction z = x - y\nPower y = x.pow(2)\nSummation y = x.sum()\nMean  y = x.mean()Transpose: transpose 2 specified dimensions&gt;&gt;&gt; x = torch.zeros([2, 3])&gt;&gt;&gt; x.shapetorch.Size([2, 3])&gt;&gt;&gt; x = x.transpose(0, 1)&gt;&gt;&gt; x.shapetorch.Size([3, 2])\nSqueeze: remove the specified dimension with length = 1&gt;&gt;&gt; x = torch.zeros([1, 2, 3])&gt;&gt;&gt; x.shapetorch.Size([1, 2, 3])&gt;&gt;&gt; x = x.squeeze(0)# 消除第一纬度&gt;&gt;&gt; x.shapetorch.Size([2, 3])\nUnsqueeze: expand a new dimension&gt;&gt;&gt; x = torch.zeros([2, 3])&gt;&gt;&gt; x.shapetorch.Size([2, 3])&gt;&gt;&gt; x = x.unsqueeze(1) # dim = 1&gt;&gt;&gt; x.shapetorch.Size([2, 1, 3])\n\nCat: concatenate multiple tensors\n&gt;&gt;&gt; x = torch.zeros([2, 1, 3])&gt;&gt;&gt; y = torch.zeros([2, 3, 3])&gt;&gt;&gt; z = torch.zeros([2, 2, 3])&gt;&gt;&gt; w = torch.Size([2, 6, 3])\n\nTensor–Data Type\nUsing different data types for model and data will cause errors\n\n\n\n\nData type\ndtype\ntensor\n\n\n\n32-bit floating point\ntorch.float\ntorch.Floattensor\n\n\n64-bit integer (signed)\ntorch.long\ntorch.LongTensor\n\n\n\nSimilar attributes &amp; same func\n\n\n\n\nPyTorch\nNumPy\n\n\n\nx.shape\nx.shape\n\n\nx.dtype\nx.dtype\n\n\nx.reshape/x.view\nx.reshape\n\n\nx.squeeze()\nx.squeeze()\n\n\nx.unsqueeze(1)\nnp.expand_dims(x,1)\n\n\nTensor–Device\nTensors &amp; modules will be computed with CPU by default\n  Use.to() to move tensors to appropriate devices.\n\nCPUx = x.to(‘cpu’)\n\nGPUx = x.to(‘cuda’)\n\n\nTensors–Device(GPU)\nCheck if your computer has NVIDIA GPI\ntorch.cuda.is_available()\nMultiple GPUs: specify ‘cuda:0’, ‘cuda:1’, ‘cuda:2’,…\n\nWhy use GPUs?\n\nParallel computing with more cores for arithmetic calculations\nsee What is a GPU and do you need one in Deep Learning?\n\n\n\nTensors–Gradient Calculation&gt;&gt;&gt; x = torch.tensor([[1. , 0.], [-1., 1.]], requires_grad = True)&gt;&gt;&gt; z = x.pow(2).sum()&gt;&gt;&gt; z.backward()        ## 非常重要，目标函数backward&gt;&gt;&gt; x.gradtensor([[2., 0.],        [-2., 2.]])\n\n\nStep2–Build NN\ntorch.nn–Network Layers\nLinear Layer(Fully-connected Layer)  nn.Linear(in_features, out_features)\n\n\n\n&gt;&gt;&gt; layer = torch.nn.Linear(32, 64)&gt;&gt;&gt; layer.weight.shapetorch.Size([64, 32])&gt;&gt;&gt; layer.bias.shapetorch.Size([64])\n\ntorch.nn – Non-Linear Activation Function\nSigmoid Activation  nn.Sigmoid\nReLU Activation  nn.ReLU\n\ntorch.nn – Build Neural Network\nStep3–Loss Func &amp; Optimtorch.nn–Loss Functions\nMean Squared Error(for regression tasks)criterion = nn.MSELoss()\nCross Entropy(for classification tasks)criterion = nn.CrossEntropyLoss()\nloss = criterion(model_out, expected_value)\n\ntorch.optim\nGradient based optimization algorithms that adjust networkparameters to reduce error.\n\nE.g. Stochastic Gradient Descent(SGD)torch.optim.SGD(model.parameters(), lr, momentum = 0)\n\n\noptimizer = torch.optim.SGD(model.parameters(), lr, momentum = 0)\n\nFor every batch of data:\nCall optimizer.zero_grad() to reset gradients of model parameters.\nCall loss.backward() to backpropagate gradients of prediction loss.\nCall optimizer.step() to adjust model parameters.\n\n\n\nStep4 Train&amp;Test&amp;PredTrain# Training setupdataset = MyDataset(file)       # read data via MyDatasettr_set = DataLoader(dataset, 16, shuffle=True) # put dataset into Dataloadermodel = MyModel().to(device)    # construct model and move to device (cpu/cuda)criterion = nn.MSELoss()        # set loss funcoptimizer = torch.optim.SGD(model.parameters(), 0.1)    # set optimizer# Training Loopfor epoch in range(n_epochs):   # iterate n_epochs    model.tarin()               # set model to train mode    for x, y in tr_set:         # iterate through the dataloader    optimizer.zero_grad()       # set gradient to zero    x, y = x.to(device), y.to(device)   # move data to device(cpu/cuda)    pred = model(x)             # forward pass(compute output)    loss = criterion(pred, y)   # compute loss    loss.backward()             # computegradient(bp)    optimizer.step()            # update model with optimizer\n\nTestmodel.eval()                    # set model to evaluation modetotal_loss = 0for x, y in dv_set:             # iterate through the dataloader    x, y = x.to(device), y.to(device)   # move data to device(cpu/cuda)    with torch.no_grad():       # disable gradient calculation        pred = model(x)         # forward pass(compute output)        loss = criterion(pred, y)       # compute loss    total_loss += loss.cpu().item() * len() # accumulate loss    avg_loss = total+loss / len(dv_set.dataset) #compute averaged loss\n\nPredictmodel.eval()                    # set model to evaluation modepreds = []:                     for x in tt_set:                # iterate through the dataloader     x = x.to(device)            # move data to device    with torch.no_grad():       # disable gradient calculation        pred = model(x)         # forward pass        preds.append(pred.cpu())    #collect\n\nNoticeeval &amp; no_grad\nload &amp; save\nTutorial2load data/PreprocessingLoad data: you can use pandas to load a csv file.\ntrain_data = pd.read_csv(\"./covid.train.csv\").drop(columns=['date']).values\nPreprocessing: Get model inputs and labels.\nx_train, y_train = train_data[:,:-1], train_data[:,-1]  # python的索引来说，右边】不包含\n\nDataset\ninit: Read data and preprocess\ngetitem: Return one sample at a time. In this case, one sample includes a 117 dimensional feature and a label\nlen: Return the size of the dataset. In this case, it is 2699\n\nDataloadertrain_loader = DataLoader(train_dataset, ba)\n\nGroup data into batches\nIf you set shuffle = True, dataloader will permutes the indices of all samples automatically.\nWe often set shuffle = True during training\nYou can check this page Advantage to shuffle a dataset if u are curious about why\n\nModel Buildingclass My_model(nn.Module):    def __init__(self, input_dim):        # TODO: modify model's structure, be aware of dimension.        self.layers = nn.Sequential(            nn.Linear(input_dim, 64),            nn.ReLU(),            nn.Linear(64, 32),            nn.ReLU(),            nn.Linear(32, 1)        )def forward(self, x):    x = self.layers(x)    x = x.squeeze(1) #(B, 1) -&gt; B\n\nCriterion# 调用 Loss Funccriterion = torch.nn.MSELoss(reduction = \"mean\")\n\nOptimizeroptimizer = torch.optim.SGD(model.parameters(), lr = 1e-5, momentum=0.9)\n\nTraining loopfor epoch in range(3000):    model.train()   # Set your model to train mode.    # tqdm is a package to visualize your training progress.    train_pbar = tqdm(train_loader, position=0, leave = True)    for x, y in train_pbar:        x, y = x.to('cuda'), y.to('cuda')   # Move your data to device.        pred = model(x)        loss = criterion(pred, y)        loss.backward()                     # Compute gradient        optimizer.step()                    # Update parameters.        optimizer.zero_grad()               # Set gradient to 0","categories":["Tutorial"],"tags":["Hung-yi Lee","PyTorch"]},{"title":"CS229 complete","url":"/broadcast/cs229-complete/","content":"\n\n\n","categories":["broadcast"],"tags":["CS229 (Andrew Ng)"]},{"title":"vim_tutorial","url":"/Tutorial/vim-tutorial/","content":"写在前面今天是 2022-09-09，开始学习vim，感觉vim很神奇也很硬核，慢慢学习掌握吧\nvim及vim-like插件vim平时都是用 macbook 写的博客，macos 环境会自带 vim，linux 也一样。\nvimiumvimium 是 chrome 的一个插件，使用类vim 的指令来控制 chrome 页面，进行一个浪的冲。\n但是 vimium 感觉还有很多需要补足的地方（可能是我还用的不熟），或者是自己diy的配置没做好。\n页面操作\n\n\n?       显示帮助对话框以获取所有可用键的列表h       向左滚动一点j       向下滚动一点k       向上滚动一点l       向右滚动一点gg      滚动到页面顶部G       滚动到页面底部d       向下滚动半页u       向上滚动半页f       打开元素定位器，是在当前标签页打开F       打开元素定位器，是在新标签页打开r       刷新gs      查看源码i       进入插入模式 - 在您按Esc退出之前，将忽略所有命令yy      将当前网址复制到剪贴板yf      将链接URL复制到剪贴板gf      循环到下一帧(尤其在选择网页内置视频的时候很管用)gF      聚焦主&#x2F;顶框架\n\n\n\n新浏览器操作\n\n\no   从URL、书签、历史记录中搜索地址，回车打开O   从URL、书签、历史记录中搜索地址，回车在新标签页中打开b   仅从书签搜索地址，回车打开B   仅从书签搜索地址，回车新标签页中打开T   搜索当前浏览器的所有标签\n\n\n\n使用搜索\n\n\n&#x2F;       进入查找模式 - 输入您的搜索查询并按Enter键进行搜索，或按Esc键取消n       查找下一个匹配项N       查找上一个匹配项\n\n\n\n浏览历史记录\n\n\nH       回到历史，也就是回到前一页L       在历史上前进，也就是回到后一页\n\n\n\n标签操作\n\n\nJ, gT   跳到左标签K, gt   跳到右标签g0      跳转到第一个标签g$      跳转到最后一个标签^       回到上一个访问的标签t       创建一个新的标签yt      复制当前页面，在新标签页打开x       关闭当前标签X       恢复关闭的上一个标签p       在当前标签页打开剪切板中的URL，如不是URL则默认引擎搜索P       在新标签页打开剪切板中的URL，如不是URL则默认引擎搜索T       在当前打开的标签中搜索W       将当前标签移动到新窗口&lt;a-p&gt;   pin&#x2F;unpin current tab\n\n\n\n标记（锚点）\n\n\nma      设置本地标记 amA      设置全局标记 Aa      跳转到本地标记 a A      跳转到q全局标记 a&#96;&#96;      跳回到跳转之前的位置 (也就是说，在执行gg，G，n，N，或&#x2F; a 之前的位置）\n\n\n\n其他高级浏览命令\n\n\n&lt;&lt;      当前标签页向左移动&gt;&gt;      当前标签页向右移动&lt;a-f&gt;   在新标签中打开多个链接gi      聚焦页面上的第一个（或第n个）文本输入框gu      跳转到URL层次的父类(xxx.com&#x2F;yyy&#x2F;zzz 跳转到 xxx.com&#x2F;yyy)gU      转到URL层次结构的根目录(也就是 xxx.com)ge      编辑当前URLgE      编辑当前URL并在新选项卡中打开zH      向左滚动zL      向右滚动v       进入预览模式;使用p &#x2F; P粘贴，然后使用y来拷贝V       enter visual line mode&lt;a-m&gt;   开&#x2F;关静音&lt;a-p&gt;   固定标签栏 \n\n\n\n预览模式 visual mode先用 / 定位，找到想要选择的字符    再按 v ,进入模式    然后使用        j：向下一行        k：向上一行        h：向左一个字符或标点（数字+h，可以移动多个字符）        l：向右一个字符或标点（数字+l，可以移动多个字符）        w：下一个标点符号后位置，包括看不见的换行符        e：下一个标点符号前位置        b：取消选中上一个字符，字符和标点算一个字符\n\n网页链接跳转这个vimium从需求出发设计的指令\n输入指令 f 会生成各个链接键的对应指令（指令不分大小写如果是 F 会打开新tab 但是不会切换到新页面；如果是 f 会打开新tab 并且切换\nvim&#x2F;vimium 学习资料\nvim-adventure: 前3章是免费的，hjkl web为止，但是后面10个地牢需要25美刀的订阅，有点小贵\nThePrimeagen 的油管视频，也有阿b搬运\nvimium的知乎文章\n安装完vim后，在环境中type vimtutor有官方的教程（做得挺好的\n\nvimLesson 1这一节主要设计vim的基本移动操作，以及个别模式切换。\nvim 的多模态vim 是一个 多模态(multi-modal) 编辑器\n\nnormal mode: 正常情况下的模式\ninsert mode: 输入i normal mode-&gt;insert mode, esc 退出\nvisual mode: v&#x2F;V &lt; ctrl-v &gt; 进入可视化模式，分别进入 一般，行可视化，块可视化（其实就是选取文本\ncommand-line mode: 输入“:“ 之后接命令\n\nnormal 下的基础指令hjklnormal模式下主要处于光标移动操作：\nh: 光标左移一格j: 光标下移一行k: 光标上移一行l: 光标右移一格支持在hjkl输入数字来重复输入指令 4h: 左移4个字符\n\n\n\n** Typing a number before a motion repeats it that many times. **\n\n\n\n复制&#x2F;粘贴&#x2F;撤回分别对应 ctrl+c, ctrl+v, ctrl+z 但是略有不同\ny: yank 将可视化模式选中的文本，导入寄存器p: paste 将粘贴存储在寄存器中的文本u: undo 撤销前一命令的效果U: UNDO to fix a whole line.Crtl-R: undo the undos特别注意： d相关的删除操作，会把删除的文本存入寄存器\n\nWEB 单词级跳跃w&#x2F;e&#x2F;b和W&#x2F;E&#x2F;B 能够实现在normal mode 下 word 和 WORD 级别的跳跃\nw: 跳至下一个word的词头e: 跳至下一个word的词尾b: 跳至前一个word的词头\n\nword or WORDword: 在英语文本下，空格隔开的单词。中文环境下是，由字母符号隔开的短句。WORD: 在英语文本下，单词和符号连起来的由空格隔开的文本段。中文一样。\n一些其他关键字i: (insert)进入insert mode，它有俩常用相关关键字o: 在所在行下插入空白行，并且进入insert mode.O: shift+o 可以在所在行上行插入空白行，并且进入insert mode.A: 跳转至所在行最后，并且进入insert mode.\nLesson 2normal mode 进阶指令这个节主要是，进入insert mode 的几种\na&#x2F;A i&#x2F;I 0A：shift+a 跳至行末并且进入 insert modea：在光标后，进入 insert modeI：shift+i 跳至行首并且进入 insert modei: 在光标前，进入 insert mode0: 跳转到本行开头.\nd motiondw: delete the word.dd: delete the line.d$: delete the sentence until the end of the line.d [motion]: \nwhere:  d     - is the delete operation  motion- is what the operation will operate on (list below)A short list of motions:  w     - until the start of the next word, EXCLUDING its first character.  e     - to the end of the current word, INCLUDING the last character.  $     - to the end of the line, INCLUDING the last character.\n\n\n\n\n** Typing a number with an operator repeats it that many times. **\n\n\n\nIn the combination of the delete operator and a motion mentioned above you insert a count before the motion to delete more:\nd    number motion\n\n\n额外补充，D: shift+d 删除本行 cursor 到最后的全部内容\n\nc command** To change until the end of a word, type ce . **\n\n\n\nNotice thatce deletes the word and places you in Insert mode.cc does the same for the whole line.\n\n\n\n\n\n\nce 删除并在 插入模式 中替代该词汇cc 在同一行中做一样的事情\n\n\n\nchange command 和 delete command 的唯一不同在于，change command会在删除操作后直接进入 insert mode。\n\n而外补充，C: shift+c 删除本行 cursor 到最后的全部内容，并且进入insert mode。\n\nr commandTHE REPLACE COMMAND** Type rx to replace the caracter at the cursor with x. **\n在光标所在位置输入用字符 x 替代原字符。\ns command同样的有 s command 删除一个字符（同 x）然后进入 insert modeS: shift+s 删除整行，然后进入 insert mode。\np command** Type p to put previously deleted text after the cursor. **\nthe words or line deleted by d motion will be stored in a Vim register. And then, p command can paste the content below the current line or behind the cursor.\nP: shift+p paste the content in the line ABOVE cursor.\nLesson 3这一节主要涉及一些vim的行间操作，如何在行间进行快速准确的光标跳转。毕竟wwwww，eeeeee挺蠢的。\nvim的行间跳转f&#x2F;tf: find, fx 在本行中（cursor后），找到第一个字符是 “x”的位置，并且实现光标跳转。F: FIND, Fx 在本行中（cursor前），找到第一个字符是 “x”的位置，并且实现光标跳转。t: to, tx 在本行中（cursor后），找到第一个字符是 ”x“的位置，并跳转到前一个位置。T: TO, Tx 在本行中（cursor前），找到第一个字符是 “x“的位置，并跳转到后一个位置。\n\n\n\n前后查找到的字符切换：；跳转到后一个查找到的字符。，跳转到前一个查找到的字符。\n\n\n\nNotice:因为 f&#x2F;t 扩展了motion的灵活性，所以 d&#x2F;c command 可以结合 f&#x2F;t 来实现精准操作。\ne.g.  command: dt) 能够删除 cursor到）前的所有内容。\n&#x2F; 查找等同于各种ide常用的 CTRL+f content &lt;enter&gt;\n在normal mode下， 输入command： &#x2F;content &lt;enter&gt;\n这里的检索查找设计与 vimium 是一样的\n\nn 查找下一个关键词\nN 查找上一个关键词\n\nCtrl+D&#x2F;U 翻页这个设计跟 vimium 大差不差，只是会加上 CTRL。CTRL+D: DOWN half page.CTRL+U: Up half page.\nzb: 当前cursor所在处，移动到页面末行。zt: 当前cursor所在处，移动到页面顶行。zz: 当前cursor所在处，移动到页面中央。\nlesson 4这节主要是阅读情景下的文页跳转\nvim 文本级跳转G 与 ggG: 跳转到文本的最后一行行首。gg: 跳转到文本的第一行行首。ctrl+g: 会打印光标所在行信息。\n光标循迹: 跳转到之前光标所在的位置。（在文本编辑时，非常有用。\n指定行跳转如果跳转到，xxx行的两种方法：\n\nxxxG\n:xxx** 两种方法的输入一样多，神不神奇 **\n\n当页首尾跳转H: 跳转到当前页面最上行。M: 跳转到当前页面的中间行。L: 跳转到当前页面最末行。\n匹配括号跳转%: 实现了已匹配的括号间跳转。{ or &lt;shift+[&gt;: 实现了光标跳转到上一个空行。} or &lt;shift+]&gt;: 实现了光标跳转到下一个空行。\nsubstitute command\n:s&#x2F;old&#x2F;new       在光标所在行首个old用new替代。\n:s&#x2F;old&#x2F;new&#x2F;g     在光标所在行所有old用new替代。\n:#,#s&#x2F;old&#x2F;new&#x2F;g  在两#间用的所有old用new替代,#是一种数字。\n:%s&#x2F;old&#x2F;new&#x2F;g    在全文件范围内进行模式的匹配和替换。\n:%s&#x2F;old&#x2F;new&#x2F;gc   在全文件范围内进行模式匹配与替换，并显示信息提醒。\n\nLesson 5vim 的文件操作执行拓展指令** Type :! followed by an external command to execute that command. **\n大概就是 :! 后可以接很多命令行，e.g. :! ls 新窗口显示 ls 的打印信息，再次输入 〈Enter〉 恢复\n\n\n\nAll : commands must be finished by hitting &lt;Enter&gt; \n\n\n\n写入文件操作** To save the changes made to the text, type :w FILENAME **\n\n会保存更改的内容，如果输入 FILENAME，会生成一个叫做 FILENAME 的文件，保存了当前file的内容。\n\n** To save part of the file, type v motion :w FILENAME **\n\n进入 visual mode 后，再执行 :w FILENAME\n在选择文段后，输入 :(colon) 会给提示符 ‘&lt;,&gt;’\n最后的输入现实为 :’&lt;,&gt;’w TEST\n\n\n\n\n\n\nNote: Pressing v starts Visual selection. You can move the cursor around to make the selection bigger or smaller. Then you can use an operator to do something with the text. For example, d deletes the text.\n\n\n\n文件的模块式导入RETRIEVING: 索回，挽回\n** To insert the contents of a file, type :r FILENAME **\n\n能将 FILENAME 里面的所有内容直接插入到 cursor 所在处。\n\n\n\n\nNote: we can also read the output of an external command. For example, :r !ls reads the output of the ls command and puts it below the cursor.\n\n\n\nSUMMARY\n\nLesson 6这一节主要是 各种插入 操作，其实在之前的小节已经写过，此处快速浏览 \nvim 的行插入** Type o to open a line below the cursor and place you in Insert mode. **o: 所在行下新增一列，并进入 Insert mode。O: 所在行上新增一列，并进入 Insert mode。\n** Type a to insert text AFTER the cursor. **a: 所在光标后进入 Insert mode(i是光标前)。A: 在光标所在行后进入 Insert mode。\n** Typa a capital R to replace more than one character. **R: 输入R后，输入的字符都会往后替代。r: 紧接着的字符会替代原字符。\nSET OPTION这个设定感觉还挺有意思的。** Set an option so a search or substitute ignores case **\n:set ic, 可以设置大小写识别，取消，：set noic:set hls, 对于搜索到的 pattern 的进行高亮:set is, 在输入 pattern 的时候就进行 search\n\n\n\nif you want to ignore case for just one searchcommand, use \\c in the phrase: .ignore\\n &lt;Enter&gt;\n\n\n\nSUMMARY\nLesson 7vim 拓展指令:help** Use the on-line help system **\nVim has a comprehensive on-line help system.\n\npress the  key (if you have one)\npress the  key (if you have one)type  :help &lt;Enter&gt;\n\nRead the text in the help window to find out how the help works.\n\nType CTRL-W CTRL-W to jump from one window to another.\nCTRL-W + hjkl: 进行一个各个小窗的跳转。\n\n\nType :q &lt;Enter&gt;  to close the help window.\n\nVim script这个主要是关于 Vim环境配置文件 脚本的编写\n** Enable Vim features **\n\nStart editing the “vimrc” file. This depends on your system: :e ~&#x2F;.vimrc     for Unix :e ~&#x2F;_vimrc     for Windows\nNow read the example “vimrc” file contents: :R $VIMRUNTIME&#x2F;vimrc_example.vim\nWrite the file with: :w\n\n‘’’ 这里给出一个配置语法高亮的例子 ‘’’\nCompletion** Command line completion with CTRL-D and &lt;TAB&gt; **\n在 command mode 下，键入 &lt;CTRL-D&gt; 会打印出 能够补全的所有items键入&lt;TAB&gt; 会按顺序便利补全内容\n写在最后** 这里主要是 vimtutor 作者的一些留言 **\nvimtutor 只是为了让方便入门的一个小文档，如果想要更近一步 “:help user-manual”\n好书推荐Vim - Vi Improved - by Steve OuallinePublisher: New Riders\n\n\n\n介绍了许多细节，对于初学者相当受益。更多的案例与图片 见 https://iccf-holland.org/click5.html\n\n\n\nLearning the Vi Editor - by Linda LambPublisher: O’Reilly &amp;Associates Inc.\n\n\n\n这本书比上本书更古早，并且更接近于 Vi 的书，但是仍然推荐。\n\n\n\n","categories":["Tutorial"],"tags":["vim","vimium"]},{"title":"计组01","url":"/uncategorized/%E8%AE%A1%E7%BB%8401/","content":"计算机组成原理————实现计算机体系结构所体现的属性，对程序员来说 “透明”-&gt;看不见\n\n如何用硬件实现所定义的接口\n如何实现指令\n\n计算机体系结构————机器语言程序员所见到的计算机系统的属性概念性的结构与功能特性（指令系统、数据类型、寻址技术、I&#x2F;O机理）\n\n如何设计硬件与软件间的接口\n有无乘法指令\n\n这门课很神奇，好像是国内特色的课程。在外面的课程设计应该是，体系结构的前半段。\n我是照着王道考研的网课看的，学习的目的是，准备夏令营（已经在摆烂了）学点概念，希望能在9月份的预推免上有点作用。感觉在考研的路上，越来越远了。。。\n\n计算机高&#x2F;低电平第一次听说 高&#x2F;低电平 的概念，在数电课上。本科的时候从来没有老师。。。好好说过 高&#x2F;低电平 的定义。\n看了Wiki和百度百科，这里是百科的简单定义：\n\n\n\n低电平（Vil）指的是保证逻辑门的输入为低电平时所允许的最大输入低电平，当输入电平低于Vil时，则认为输入电平为低电平。是与高电平相对的低电压，是电子工程上的一种说法。高电平，指的是与低电平相对的高电压，是电工程上的一种说法。在逻辑电平中，保证逻辑门的输入为高电平时所允许的最小输入高电平，当输入电平高于输入高电压（Vih）时，则认为输入电平为高电平。\n\n\n\n计算机系统计算机系统 &#x3D; 软件 + 硬件\n软件计算机软件分为 系统软件 和 应用软件 两部分。\n\n系统软件： 用于管理整个计算机系统\n操作系统\n数据库管理系统DBMS\n标准程序库\n网络软件\n语言处理软件\n服务程序\n\n\n应用软件： 按任务需求编制的各种程序\n\n硬件发展电子管-&gt;晶体管(肖克莱）-&gt;中小规模集成电路(仙童）-&gt;大规模、超大规模集成电路\n\n硬件的基本组成冯诺依曼机“存储程序”： 将指令以二进制代码的形式事先输入计算机的主存储器。\n\n计算机系统中，软硬件逻辑等效。 Eg.一般专门设计的硬件电路实现的运算更高效。\n冯诺依曼机特点：\n\n由五大部件组成\n指令与数据以同等地位存储于存储器\n指令和数据都以二进制表示\n指令：操作码+地址码\n存储程序\n以运算器为核心\n\n现代计算机结构\n现代计算机：以存储器为核心。CPU&#x3D;运算器+控制器\n\nCPU 和 主存储器（内存）合在一起叫做主机。\n\n主存储器主存储器：二进制数据的主要存储MAR： Memory Address Register 地址寄存器MDR： Memory Data Register 数据寄存器\n\n字word：每个存储单元的数据量\n运算器\nALU 是运算器的核心部件\n控制器\n指令工作\n\n计算机系统的层次结构\n\n\n计算机性能指标主存储器指标总容量 &#x3D; 存储单元个数 * 存储字长 bit &#x3D; 存储单元个数 * 存储字长&#x2F;8 Byte\nCPU指标\n\n系统整体的性能指标\n\n","tags":["计算机组成原理"]}]